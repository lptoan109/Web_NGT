{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13506368,"sourceType":"datasetVersion","datasetId":8575381}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":16337.907779,"end_time":"2025-10-16T15:46:33.160586","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-16T11:14:15.252807","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"3b80d5e5","cell_type":"code","source":"# 0.1. Cài đặt các thư viện cần thiết\n!pip install -q fpdf2 noisereduce librosa tensorflow scikit-learn matplotlib seaborn pytz PyDrive2","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":10.165551,"end_time":"2025-10-16T11:14:29.210898","exception":false,"start_time":"2025-10-16T11:14:19.045347","status":"completed"},"tags":[],"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"id":"aa0f9892","cell_type":"code","source":"# CÀI ĐẶT & CẤU HÌNH \n# 0.2. Import thư viện\n\n# --- Thư viện chuẩn của Python ---\\\nimport os\nimport glob \nimport random\nimport datetime\nimport pytz\nimport shutil\nfrom itertools import cycle\nfrom tqdm import tqdm\nimport gc\n\n# --- Thư viện xử lý dữ liệu và tính toán ---\\\nimport numpy as np\nimport pandas as pd\n\n# --- Thư viện trực quan hóa ---\\\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# --- Thư viện xử lý âm thanh ---\\\nimport librosa\nimport noisereduce as nr\n\n# --- Thư viện Machine Learning (Scikit-learn) ---\\\nfrom sklearn.preprocessing import LabelEncoder, label_binarize, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc as sklearn_auc\nfrom sklearn.utils import class_weight\n\n# --- TensorFlow & Keras ---\\\nimport tensorflow as tf\nfrom tensorflow.keras import mixed_precision\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.applications import EfficientNetV2B2\nfrom tensorflow.keras.applications.efficientnet_v2 import preprocess_input\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.utils import to_categorical, register_keras_serializable\n\n# Import các lớp (Layers) cần thiết cho CNN\nfrom tensorflow.keras.layers import (\n    Input, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, Activation\n)\n\nprint(f\"TensorFlow Version: {tf.__version__}\")\nprint(\"Tất cả các thư viện đã được import thành công.\")\n\n# --- Định nghĩa AUTOTUNE ---\nAUTOTUNE = tf.data.AUTOTUNE\n\n# --- Kết nối Accelerator (Khuyến nghị dùng 1 GPU để ổn định) ---\ntry:\n    gpus = tf.config.list_physical_devices('GPU')\n    if gpus:\n        # Sử dụng 1 GPU duy nhất để tránh các vấn đề đồng bộ hóa\n        tf.config.set_visible_devices(gpus[0], 'GPU')\n        strategy = tf.distribute.get_strategy()\n        policy = 'mixed_float16'\n        print(f\" KẾT NỐI 1-GPU THÀNH CÔNG! (Sử dụng {gpus[0].name})\")\n    else:\n        raise Exception(\"Không tìm thấy GPU\")\nexcept Exception:\n    print(\" Không tìm thấy GPU. Sử dụng CPU.\")\n    strategy = tf.distribute.get_strategy()\n    policy = 'float32'\n\nmixed_precision.set_global_policy(policy)\nprint(f\"   - Số lượng nhân (replicas): {strategy.num_replicas_in_sync}\")\nprint(f\"   - Kiểu dữ liệu (DType Policy): {policy}\")\ntf.config.optimizer.set_experimental_options({'layout_optimizer': False})","metadata":{"papermill":{"duration":21.881854,"end_time":"2025-10-16T11:14:51.096894","exception":false,"start_time":"2025-10-16T11:14:29.215040","status":"completed"},"tags":[],"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"id":"3b4cc0d2","cell_type":"code","source":"# THIẾT LẬP CẤU HÌNH (PHIÊN BẢN .NPY)\n\n# --- Cấu hình chung ---\\\nSEED = 42\ndef set_seed(seed_value):\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    tf.random.set_seed(seed_value)\nset_seed(SEED)\n\n# ĐỊNH NGHĨA TÊN CÁC LỚP (KHỚP VỚI prep.ipynb)\ntarget_names_str = ['asthma', 'covid', 'healthy', 'tuberculosis']\n\n# --- ĐƯỜNG DẪN DỮ LIỆU .NPY ---\\\n# !!! THAY ĐỔI ĐƯỜNG DẪN NÀY ĐẾN THƯ MỤC processed_data CỦA BẠN !!!\nDATA_DIR = \"/kaggle/input/coughdatangt/\" # Ví dụ: /kaggle/input/processed-data/\nX_TRAIN_PATH = os.path.join(DATA_DIR, \"X_train.npy\")\nY_TRAIN_PATH = os.path.join(DATA_DIR, \"y_train.npy\")\nX_TEST_PATH = os.path.join(DATA_DIR, \"X_test.npy\")\nY_TEST_PATH = os.path.join(DATA_DIR, \"y_test.npy\")\nCLASS_WEIGHTS_PATH = os.path.join(DATA_DIR, \"class_weights.npy\")\n\n# --- Cấu hình Lưu trữ ---\\\nKAGGLE_OUTPUT_PATH = \"/kaggle/working/output_results\"\nCHECKPOINT_PATH = \"/kaggle/working/checkpoints\"\nos.makedirs(CHECKPOINT_PATH, exist_ok=True)\nos.makedirs(KAGGLE_OUTPUT_PATH, exist_ok=True)\nMODEL_ID = f'CNN_from_NPY' # Đổi tên model\n\nN_SPLITS = 5\n\n# --- Cấu hình Kiến trúc CNN ---\n# Kích thước này phải khớp với đầu ra của prep.ipynb (ví dụ: 256x126) + 3 kênh màu\nINPUT_SHAPE = (256, 126, 3) \nNUM_CLASSES = len(target_names_str) # = 4\n\n# --- Cấu hình Regularization ---\\\nAUG_FREQ_MASKS = 2\nAUG_TIME_MASKS = 2\nAUG_FREQ_MASK_SIZE = 30\nAUG_TIME_MASK_SIZE = 40\nDROPOUT_RATE_1 = 0.3\nDROPOUT_RATE_2 = 0.2\nWEIGHT_DECAY = 1e-5\nLABEL_SMOOTHING_VALUE = 0.1\n\n# --- Các công tắc Bật/Tắt ---\\\nUSE_DATA_AUGMENTATION = True # Sẽ áp dụng augmentation trong pipeline tf.data\nUSE_CLASS_WEIGHTS = True # Bật để sử dụng file class_weights.npy\nUSE_FOCAL_LOSS = False    # Tắt Focal Loss, ưu tiên dùng Class Weights\nUSE_COSINE_DECAY_RESTARTS = True \nUSE_XLA_COMPILATION = False\n\n# --- Cấu hình pipeline dữ liệu ---\\\nBATCH_SIZE = 64 \nGLOBAL_BATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync\nSHUFFLE_BUFFER_SIZE = 2048\n\n# --- Cấu hình Huấn luyện ---\\\nSTAGE1_HEAD_EPOCHS = 20\nSTAGE1_HEAD_LR = 1e-4 \nSTAGE1_HEAD_PATIENCE = 5\n\nSTAGE1_FINETUNE_TOTAL_EPOCHS = 100 \nSTAGE1_FINETUNE_LR_INITIAL = 1e-5 \nSTAGE1_FINETUNE_PATIENCE = 15 \nRESTART_CYCLE_1_EPOCHS = 20\n\n# --- Các tham số khác ---\\\nGAMMA = 2.0 \nMIN_DELTA = 1e-4\n\nprint(f\"Input Shape: {INPUT_SHAPE}\")\nprint(f\"Số lớp: {NUM_CLASSES}\")\nprint(f\"Global Batch Size: {GLOBAL_BATCH_SIZE}\")","metadata":{"papermill":{"duration":0.014805,"end_time":"2025-10-16T11:14:51.115721","exception":false,"start_time":"2025-10-16T11:14:51.100916","status":"completed"},"tags":[],"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"id":"a4a0e7bd","cell_type":"code","source":"# KHỞI TẠO CÁC HÀM CẦN THIẾT \n\n# --- Hàm Tiền xử lý & Augmentation cho tf.data ---\\\n\ndef preprocess_npy_image(image_np):\n    \"\"\"\n    Tiền xử lý một ảnh spectrogram NumPy.\n    Giả định ảnh đầu vào là (H, W) hoặc (H, W, 1).\n    Kích thước (H, W) phải khớp với INPUT_SHAPE (ví dụ: 256, 126).\n    \"\"\"\n    image_np = tf.cast(image_np, tf.float32)\n    \n    # 1. Thêm 3 kênh màu (nếu cần)\n    if image_np.ndim == 2:\n        image_3d = tf.stack([image_np]*3, axis=-1)\n    elif image_np.ndim == 3 and image_np.shape[-1] == 1:\n         image_3d = tf.concat([image_np]*3, axis=-1)\n    else:\n        image_3d = image_np # Đã là (H, W, 3)\n\n    # 2. KHÔNG RESIZE - Giả định prep.ipynb đã chuẩn hóa kích thước\n    # image_resized = tf.image.resize(image_3d, [INPUT_SHAPE[0], INPUT_SHAPE[1]])\n    \n    # 3. Làm sạch NaN/Inf\n    image_sanitized = tf.where(tf.math.is_finite(image_3d), image_3d, 0.0)\n    \n    # 4. Scale về [0, 255]\n    min_val = tf.reduce_min(image_sanitized)\n    max_val = tf.reduce_max(image_sanitized)\n    denominator = tf.maximum(max_val - min_val, 1e-7)\n    image_scaled_01 = (image_sanitized - min_val) / denominator\n    image_scaled_255 = image_scaled_01 * 255.0\n\n    # 5. Áp dụng preprocess_input của EfficientNet\n    image_preprocessed = preprocess_input(image_scaled_255)\n    return image_preprocessed\n\n@tf.function\ndef tf_preprocess_map(image, label):\n    image_processed = preprocess_npy_image(image)\n    # Đảm bảo set_shape khớp với INPUT_SHAPE mới\n    image_processed.set_shape(INPUT_SHAPE) \n    return image_processed, label\n\ndef spec_augment(spectrogram):\n    \"\"\"\n    Sử dụng các tham số từ ô cấu hình để áp dụng SpecAugment.\n    \"\"\"\n    spectrogram_aug = spectrogram\n    freq_bins = tf.shape(spectrogram)[0]\n    time_steps = tf.shape(spectrogram)[1]\n    \n    # Mask tần số\n    for _ in range(AUG_FREQ_MASKS):\n        f = tf.random.uniform(shape=(), minval=0, maxval=AUG_FREQ_MASK_SIZE, dtype=tf.int32)\n        f0 = tf.random.uniform(shape=(), minval=0, maxval=freq_bins - f, dtype=tf.int32)\n        freq_mask_1d = tf.concat([tf.ones((f0,), dtype=spectrogram.dtype), tf.zeros((f,), dtype=spectrogram.dtype), tf.ones((freq_bins - f0 - f,), dtype=spectrogram.dtype)], axis=0)\n        freq_mask_3d = tf.reshape(freq_mask_1d, (freq_bins, 1, 1))\n        spectrogram_aug *= freq_mask_3d\n        \n    # Mask thời gian\n    for _ in range(AUG_TIME_MASKS):\n        t = tf.random.uniform(shape=(), minval=0, maxval=AUG_TIME_MASK_SIZE, dtype=tf.int32)\n        t0 = tf.random.uniform(shape=(), minval=0, maxval=time_steps - t, dtype=tf.int32)\n        time_mask_1d = tf.concat([tf.ones((t0,), dtype=spectrogram.dtype), tf.zeros((t,), dtype=spectrogram.dtype), tf.ones((time_steps - t0 - t,), dtype=spectrogram.dtype)], axis=0)\n        time_mask_3d = tf.reshape(time_mask_1d, (1, time_steps, 1))\n        spectrogram_aug *= time_mask_3d\n        \n    return spectrogram_aug\n\n@tf.function\ndef augment_map(image, label):\n    image = spec_augment(image)\n    return image, label\n\n# --- Định nghĩa Model CNN (Thay thế CRNN) ---\n@register_keras_serializable()\nclass FinalModelCNN(tf.keras.Model):\n    def __init__(self, input_shape_config, num_classes_config, **kwargs):\n        super(FinalModelCNN, self).__init__(**kwargs)\n        self.input_shape_config = input_shape_config\n        self.num_classes_config = num_classes_config\n\n        # 1. Base Model (CNN)\n        self.base_model = EfficientNetV2B2(\n            weights='imagenet', \n            include_top=False, \n            input_shape=self.input_shape_config\n        )\n        \n        # 2. Lớp Pooling để giảm chiều dữ liệu\n        self.gap = GlobalAveragePooling2D(name=\"global_avg_pool\")\n        \n        # 3. Các lớp Dense (Head)\n        self.dense1 = Dense(512, use_bias=False, kernel_regularizer=l2(WEIGHT_DECAY), name=\"dense_layer_1\")\n        self.bn1 = BatchNormalization(name=\"batch_norm_1\")\n        self.act1 = Activation('relu', name=\"activation_1\")\n        self.dropout1 = Dropout(DROPOUT_RATE_1, name=\"dropout_layer_1\")\n        \n        self.dense2 = Dense(256, use_bias=False, kernel_regularizer=l2(WEIGHT_DECAY), name=\"dense_layer_2\")\n        self.bn2 = BatchNormalization(name=\"batch_norm_2\")\n        self.act2 = Activation('relu', name=\"activation_2\")\n        self.dropout2 = Dropout(DROPOUT_RATE_2, name=\"dropout_layer_2\")\n        \n        # 4. Lớp Output\n        self.dense_output = Dense(self.num_classes_config, activation='linear', dtype='float32', name=\"output_layer\")\n\n    def call(self, inputs, training=None):\n        x = self.base_model(inputs, training=training)\n        # Bỏ các lớp RNN, thay bằng GAP\n        x = self.gap(x, training=training) \n        \n        # Các lớp Dense Head\n        x = self.dense1(x)\n        x = self.bn1(x, training=training); x = self.act1(x); x = self.dropout1(x, training=training)\n        x = self.dense2(x)\n        x = self.bn2(x, training=training); x = self.act2(x); x = self.dropout2(x, training=training)\n        outputs = self.dense_output(x)\n        return outputs\n\n    def get_config(self):\n        config = super(FinalModelCNN, self).get_config()\n        # Bỏ các tham số RNN\n        config.update({\n            'input_shape_config': self.input_shape_config,\n            'num_classes_config': self.num_classes_config,\n        })\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n\n# --- Hàm tính Macro F1 (giữ nguyên) ---\n@register_keras_serializable()\nclass MacroF1Score(tf.keras.metrics.Metric):\n    def __init__(self, num_classes, name='f1_macro', **kwargs):\n        super(MacroF1Score, self).__init__(name=name, **kwargs)\n        self.num_classes = num_classes\n        self.true_positives = self.add_weight(name='tp', shape=(num_classes,), initializer='zeros')\n        self.false_positives = self.add_weight(name='fp', shape=(num_classes,), initializer='zeros')\n        self.false_negatives = self.add_weight(name='fn', shape=(num_classes,), initializer='zeros')\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_pred_labels = tf.argmax(tf.nn.softmax(y_pred), axis=1)\n        y_true_labels = tf.argmax(y_true, axis=1)\n        cm = tf.math.confusion_matrix(y_true_labels, y_pred_labels, num_classes=self.num_classes, dtype=tf.float32)\n        tp = tf.linalg.diag_part(cm)\n        fp = tf.reduce_sum(cm, axis=0) - tp\n        fn = tf.reduce_sum(cm, axis=1) - tp\n        self.true_positives.assign_add(tp)\n        self.false_positives.assign_add(fp)\n        self.false_negatives.assign_add(fn)\n\n    def result(self):\n        precision = self.true_positives / (self.true_positives + self.false_positives + tf.keras.backend.epsilon())\n        recall = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n        macro_f1 = tf.reduce_mean(f1)\n        return macro_f1\n\n    def reset_state(self):\n        self.true_positives.assign(tf.zeros(self.num_classes))\n        self.false_positives.assign(tf.zeros(self.num_classes))\n        self.false_negatives.assign(tf.zeros(self.num_classes))\n\n    def get_config(self):\n        config = super(MacroF1Score, self).get_config()\n        config.update({'num_classes': self.num_classes})\n        return config\n\n# --- Callback Log Learning Rate (giữ nguyên) ---\nclass LearningRateLogger(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs=None):\n        current_lr = tf.keras.backend.get_value(self.model.optimizer.learning_rate)\n        print(f\"\\nEpoch {epoch+1}: Learning Rate is {current_lr:.2e}\")\n\n# --- Hàm vẽ biểu đồ (giữ nguyên) ---\ndef plot_training_history(history, title_prefix, fold_number, output_dir):\n    plt.figure(figsize=(18, 6))\n    full_title = f\"{title_prefix} - Fold {fold_number}\"\n    plt.suptitle(full_title, fontsize=16)\n    plt.subplot(1, 2, 1)\n    metrics_to_plot = ['accuracy', 'auc', 'f1_macro']\n    colors = {'accuracy': 'blue', 'auc': 'green', 'f1_macro': 'red'}\n    for metric in metrics_to_plot:\n        if history.history.get(metric):\n            plt.plot(history.history[metric], label=f'Training {metric.capitalize()}', color=colors[metric], linestyle='-')\n        val_metric = f'val_{metric}'\n        if history.history.get(val_metric):\n            plt.plot(history.history[val_metric], label=f'Validation {metric.capitalize()}', color=colors[metric], linestyle='--')\n    plt.title('Biểu đồ các chỉ số'); plt.xlabel('Epoch'); plt.ylabel('Giá trị')\n    if any(metric in history.history for metric in metrics_to_plot):\n        plt.legend(loc='lower left')\n    plt.grid(True)\n    plt.subplot(1, 2, 2)\n    if history.history.get('loss'):\n        plt.plot(history.history['loss'], label='Training Loss', color='orange')\n    if history.history.get('val_loss'):\n        plt.plot(history.history['val_loss'], label='Validation Loss', color='purple')\n    plt.title('Biểu đồ Loss'); plt.xlabel('Epoch'); plt.ylabel('Loss')\n    if history.history.get('loss') or history.history.get('val_loss'):\n        plt.legend(loc='upper right')\n    plt.grid(True)\n    filename_prefix = title_prefix.replace(' ', '_').replace('-', '_').replace(':', '')\n    filename = f\"fold_{fold_number}_{filename_prefix}_metrics.png\"\n    filepath = os.path.join(output_dir, filename)\n    plt.savefig(filepath)\n    plt.close()\n    print(f\"Đã lưu biểu đồ: {os.path.basename(filepath)}\")\n\nprint(\"Tất cả các hàm và model CNN đã được định nghĩa.\")","metadata":{"papermill":{"duration":0.035354,"end_time":"2025-10-16T11:14:51.154545","exception":false,"start_time":"2025-10-16T11:14:51.119191","status":"completed"},"tags":[],"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"id":"c42fcedb","cell_type":"code","source":"# CHUẨN BỊ DỮ LIỆU TỪ CÁC FILE .NPY\n\nprint(\"--- 3. Nạp Dữ liệu từ .npy ---\")\ntry:\n    X_train_all = np.load(X_TRAIN_PATH)\n    y_train_all = np.load(Y_TRAIN_PATH)\n    \n    if os.path.exists(X_TEST_PATH) and os.path.exists(Y_TEST_PATH):\n        X_test = np.load(X_TEST_PATH)\n        y_test = np.load(Y_TEST_PATH)\n        print(\"Đã tải thành công tập Train, Val và Test.\")\n    else:\n        print(\"Cảnh báo: Không tìm thấy file X_test.npy hoặc y_test.npy.\")\n        X_test, y_test = None, None\n        \n    print(f\"Shape X_train_all: {X_train_all.shape}\")\n    print(f\"Shape y_train_all: {y_train_all.shape}\")\n\n    # Kiểm tra số lớp\n    unique_labels = np.unique(y_train_all)\n    if len(unique_labels) != NUM_CLASSES:\n        print(f\"Cảnh báo: Số lớp tìm thấy ({len(unique_labels)}) không khớp với NUM_CLASSES ({NUM_CLASSES}).\")\n        NUM_CLASSES = len(unique_labels)\n        print(f\"Đã cập nhật NUM_CLASSES thành {NUM_CLASSES}.\")\n    \n    # Đảm bảo target_names_str khớp\n    if len(target_names_str) != NUM_CLASSES:\n        raise ValueError(f\"Lỗi: 'target_names_str' có {len(target_names_str)} tên, nhưng NUM_CLASSES là {NUM_CLASSES}.\")\n\n    # Chuyển nhãn thành one-hot encoding\n    y_train_all_ohe = to_categorical(y_train_all, num_classes=NUM_CLASSES)\n    if y_test is not None:\n        y_test_ohe = to_categorical(y_test, num_classes=NUM_CLASSES)\n\n    # Nạp class weights nếu cần\n    class_weights_dict = None\n    if USE_CLASS_WEIGHTS:\n        if os.path.exists(CLASS_WEIGHTS_PATH):\n            class_weights_array = np.load(CLASS_WEIGHTS_PATH)\n            class_weights_dict = dict(enumerate(class_weights_array))\n            print(\"Đã tải và sẽ sử dụng class weights:\")\n            print(class_weights_dict)\n        else:\n            print(\"Cảnh báo: Đã bật USE_CLASS_WEIGHTS nhưng không tìm thấy file class_weights.npy.\")\n            USE_CLASS_WEIGHTS = False\n\nexcept FileNotFoundError as e:\n    print(f\"Lỗi nghiêm trọng: Không tìm thấy file dữ liệu .npy! Vui lòng kiểm tra đường dẫn DATA_DIR.\")\n    print(e)\n    # Dừng notebook ở đây nếu không có dữ liệu\n    raise SystemExit(\"Dừng do thiếu dữ liệu.\")\nexcept Exception as e:\n    print(f\"Lỗi không xác định khi tải dữ liệu: {e}\")\n    raise SystemExit(\"Dừng do lỗi tải dữ liệu.\")","metadata":{"papermill":{"duration":45.776843,"end_time":"2025-10-16T11:15:36.934878","exception":false,"start_time":"2025-10-16T11:14:51.158035","status":"completed"},"tags":[],"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"id":"9a49800e","cell_type":"code","source":"# HUẤN LUYỆN CROSS-VALIDATION (TỪ DỮ LIỆU .NPY)\n\nprint(\"\\n--- 5. Bắt đầu Huấn luyện Cross-Validation ---\")\n\nfold_accuracies, fold_losses, fold_aucs, fold_f1s = [], [], [], []\n# Sử dụng StratifiedKFold vì chúng ta đang làm việc với mảng NumPy\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X_train_all, y_train_all)):\n    fold_number = fold + 1\n    print(\"-\" * 60 + f\"\\nBắt đầu Fold {fold_number}/{N_SPLITS}\\n\" + \"-\" * 60)\n\n    # --- 5A: Chuẩn bị dữ liệu cho Fold ---\n    print(\"   - Chuẩn bị dữ liệu...\")\n    X_train_fold, X_val_fold = X_train_all[train_idx], X_train_all[val_idx]\n    y_train_fold, y_val_fold = y_train_all_ohe[train_idx], y_train_all_ohe[val_idx]\n\n    train_ds = tf.data.Dataset.from_tensor_slices((X_train_fold, y_train_fold))\n    val_ds = tf.data.Dataset.from_tensor_slices((X_val_fold, y_val_fold))\n\n    # Xóa các mảng NumPy lớn để tiết kiệm RAM\n    del X_train_fold, X_val_fold, y_train_fold, y_val_fold\n    gc.collect()\n\n    # Áp dụng tiền xử lý, shuffle, batch, prefetch\n    train_ds = train_ds.map(tf_preprocess_map, num_parallel_calls=AUTOTUNE)\n    if USE_DATA_AUGMENTATION:\n        train_ds = train_ds.map(augment_map, num_parallel_calls=AUTOTUNE) # Áp dụng Augmentation\n        \n    train_ds = train_ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE).prefetch(AUTOTUNE)\n    val_ds = val_ds.map(tf_preprocess_map, num_parallel_calls=AUTOTUNE).batch(GLOBAL_BATCH_SIZE).prefetch(AUTOTUNE)\n\n    steps_per_epoch = len(train_idx) // GLOBAL_BATCH_SIZE\n    validation_steps = len(val_idx) // GLOBAL_BATCH_SIZE\n    print(f\"   - Steps per epoch: {steps_per_epoch}, Validation steps: {validation_steps}\")\n\n    # --- 5B: Huấn luyện 2 Giai đoạn ---\n    with strategy.scope():\n        # SỬA ĐỔI: Sử dụng FinalModelCNN\n        model = FinalModelCNN(\n            input_shape_config=INPUT_SHAPE, \n            num_classes_config=NUM_CLASSES\n        )\n        \n        if USE_FOCAL_LOSS:\n            loss_function = tf.keras.losses.CategoricalFocalCrossentropy(from_logits=True, gamma=GAMMA, label_smoothing=LABEL_SMOOTHING_VALUE)\n        else:\n            loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=LABEL_SMOOTHING_VALUE)\n            \n        # Giai đoạn 1: Huấn luyện Head (chỉ các lớp Dense)\n        print(\"\\n   --- Giai đoạn 1: Huấn luyện Head (Dense layers) ---\")\n        model.base_model.trainable = False\n        optimizer_head = tf.keras.optimizers.AdamW(learning_rate=STAGE1_HEAD_LR, weight_decay=WEIGHT_DECAY, epsilon=1e-7)\n        model.compile(optimizer=optimizer_head, loss=loss_function, metrics=['accuracy'], jit_compile=USE_XLA_COMPILATION)\n        history_1a = model.fit(train_ds, validation_data=val_ds, epochs=STAGE1_HEAD_EPOCHS, \n                               steps_per_epoch=steps_per_epoch, validation_steps=validation_steps,\n                               callbacks=[EarlyStopping(monitor='val_loss', patience=STAGE1_HEAD_PATIENCE, restore_best_weights=True)],\n                               class_weight=class_weights_dict if USE_CLASS_WEIGHTS else None,\n                               verbose=1)\n        \n        # Giai đoạn 2: Fine-tuning (toàn bộ model)\n        print(\"\\n   --- Giai đoạn 2: Fine-tuning ---\")\n        model.base_model.trainable = True\n        \n        if USE_COSINE_DECAY_RESTARTS:\n            first_decay_steps = RESTART_CYCLE_1_EPOCHS * steps_per_epoch\n            lr_scheduler = tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=STAGE1_FINETUNE_LR_INITIAL, first_decay_steps=first_decay_steps, t_mul=2.0, m_mul=0.9, alpha=0.1)\n            optimizer_finetune = tf.keras.optimizers.AdamW(learning_rate=lr_scheduler, weight_decay=WEIGHT_DECAY, epsilon=1e-7)\n            callbacks = [EarlyStopping(monitor='val_f1_macro', mode='max', patience=STAGE1_FINETUNE_PATIENCE, restore_best_weights=True, min_delta=MIN_DELTA, verbose=1), LearningRateLogger()]\n        else:\n            optimizer_finetune = tf.keras.optimizers.AdamW(learning_rate=STAGE1_FINETUNE_LR_INITIAL, weight_decay=WEIGHT_DECAY, epsilon=1e-7)\n            callbacks = [EarlyStopping(monitor='val_f1_macro', mode='max', patience=STAGE1_FINETUNE_PATIENCE, restore_best_weights=True, verbose=1)]\n        \n        f1_macro = MacroF1Score(num_classes=NUM_CLASSES)\n        model.compile(optimizer=optimizer_finetune, loss=loss_function, metrics=['accuracy', tf.keras.metrics.AUC(name='auc'), f1_macro], jit_compile=USE_XLA_COMPILATION)\n        \n        history_1b = model.fit(train_ds, validation_data=val_ds, epochs=STAGE1_FINETUNE_TOTAL_EPOCHS, \n                               steps_per_epoch=steps_per_epoch, validation_steps=validation_steps,\n                               callbacks=callbacks,\n                               class_weight=class_weights_dict if USE_CLASS_WEIGHTS else None,\n                               verbose=1)\n    \n    # --- 5C: Lưu model, Vẽ biểu đồ, Đánh giá ---\n    model_save_path = os.path.join(KAGGLE_OUTPUT_PATH, f'{MODEL_ID}_fold_{fold_number}.keras')\n    model.save(model_save_path)\n    print(f\"\\n   Đã lưu model cho Fold {fold_number} tại: {model_save_path}\")\n\n    print(\"\\n   --- Vẽ biểu đồ huấn luyện ---\")\n    plot_training_history(history_1a, \"Giai doan 1A - Head Training\", fold_number, KAGGLE_OUTPUT_PATH)\n    plot_training_history(history_1b, \"Giai doan 1B - Fine-tuning\", fold_number, KAGGLE_OUTPUT_PATH)\n\n    print(\"\\n   --- Đánh giá trên tập Validation ---\")\n    val_results = model.evaluate(val_ds, verbose=0, return_dict=True)\n    loss, accuracy, auc, f1 = val_results.get('loss', 0), val_results.get('accuracy', 0), val_results.get('auc', 0), val_results.get('f1_macro', 0)\n    print(f\"   Fold {fold_number} - Validation Loss: {loss:.4f}, Accuracy: {accuracy:.4f}, AUC: {auc:.4f}, F1-Macro: {f1:.4f}\")\n    \n    fold_accuracies.append(accuracy); fold_losses.append(loss); fold_aucs.append(auc); fold_f1s.append(f1)\n    \n    print(\"\\n   \" + \"=\" * 50 + \"\\n   Kết quả Cross-Validation Tạm thời:\\n\" \n          + f\"     - Accuracy trung bình: {np.mean(fold_accuracies):.4f} +/- {np.std(fold_accuracies):.4f}\\n\"\n          + f\"     - Loss trung bình: {np.mean(fold_losses):.4f} +/- {np.std(fold_losses):.4f}\\n\"\n          + f\"     - AUC trung bình: {np.mean(fold_aucs):.4f} +/- {np.std(fold_aucs):.4f}\\n\"\n          + f\"     - F1-Macro trung bình: {np.mean(fold_f1s):.4f} +/- {np.std(fold_f1s):.4f}\\n\"\n          + \"   \" + \"=\" * 50)\n          \n    # --- 5D: Dọn dẹp bộ nhớ ---\n    print(\"\\n   --- Dọn dẹp bộ nhớ ---\")\n    try:\n        del model, train_ds, val_ds\n        tf.keras.backend.clear_session()\n        gc.collect()\n        print(\"   Đã dọn dẹp bộ nhớ thành công.\")\n    except NameError as e:\n        print(f\"   Một số biến có thể đã được dọn dẹp, bỏ qua lỗi: {e}\")\n\n# --- IN KẾT QUẢ TỔNG KẾT CUỐI CÙNG ---\\\nprint(\"\\n\\n\" + \"=\" * 60 + \"\\nKẾT QUẢ CROSS-VALIDATION CUỐI CÙNG:\\n\" \n      + f\"  - Validation Accuracy trung bình: {np.mean(fold_accuracies):.4f} +/- {np.std(fold_accuracies):.4f}\\n\"\n      + f\"  - Validation Loss trung bình: {np.mean(fold_losses):.4f} +/- {np.std(fold_losses):.4f}\\n\"\n      + f\"  - Validation AUC trung bình: {np.mean(fold_aucs):.4f} +/- {np.std(fold_aucs):.4f}\\n\"\n      + f\"  - Validation F1-Macro trung bình: {np.mean(fold_f1s):.4f} +/- {np.std(fold_f1s):.4f}\\n\"\n      + \"=\" * 60)","metadata":{"papermill":{"duration":16254.012317,"end_time":"2025-10-16T15:46:30.968864","exception":false,"start_time":"2025-10-16T11:15:36.956547","status":"completed"},"tags":[],"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"id":"c80c4844","cell_type":"code","source":"# ĐÁNH GIÁ CUỐI CÙNG TRÊN TẬP TEST (HOLD-OUT)\n\nif X_test is not None and y_test is not None:\n    print(\"\\n--- 6. Bắt đầu Đánh giá cuối cùng trên tập Test (Hold-out) ---\")\n    \n    # Biến target_names_str đã được định nghĩa ở Cell 3\n    if len(target_names_str) != NUM_CLASSES:\n        raise ValueError(\"Lỗi: 'target_names_str' và 'NUM_CLASSES' không khớp.\")\n    \n    # --- 6A: Tạo Test Dataset ---\n    print(\"   - Chuẩn bị Test Dataset...\")\n    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test_ohe))\n    test_ds = test_ds.map(tf_preprocess_map, num_parallel_calls=AUTOTUNE) \\\n                       .batch(GLOBAL_BATCH_SIZE) \\\n                       .prefetch(AUTOTUNE)\n\n    # --- 6B: Lấy dự đoán từ 5 model ---\n    all_fold_preds = []\n    print(f\"   - Lấy dự đoán từ {N_SPLITS} models...\")\n    for fold_number in range(1, N_SPLITS + 1):\n        model_path = os.path.join(KAGGLE_OUTPUT_PATH, f'{MODEL_ID}_fold_{fold_number}.keras')\n        if os.path.exists(model_path):\n            print(f\"     - Đang tải model Fold {fold_number}...\")\n            with strategy.scope():\n                # SỬA LỖI: Đổi 'FinalModelCRNN' thành 'FinalModelCNN'\n                model = tf.keras.models.load_model(model_path, custom_objects={'FinalModelCNN': FinalModelCNN, 'MacroF1Score': MacroF1Score})\n            \n            print(f\"     - Đang dự đoán với model Fold {fold_number}...\")\n            fold_preds = model.predict(test_ds, verbose=1)\n            all_fold_preds.append(fold_preds)\n            \n            del model\n            tf.keras.backend.clear_session()\n            gc.collect()\n        else:\n            print(f\"     - Cảnh báo: Không tìm thấy model Fold {fold_number} tại '{model_path}'.\")\n\n    # --- 6C: Tính trung bình dự đoán (Ensemble) ---\n    if all_fold_preds:\n        print(\"\\n   - Tính trung bình dự đoán...\")\n        avg_preds = np.mean(all_fold_preds, axis=0)\n        y_pred_probs_final = tf.nn.softmax(avg_preds).numpy() # Áp dụng softmax cho logits trung bình\n        y_pred_final = np.argmax(y_pred_probs_final, axis=1)\n        y_true_final = y_test # Sử dụng nhãn gốc (integer)\n        \n        # --- 6D: Tính toán và In các chỉ số cuối cùng ---\n        print(\"\\n   - Kết quả đánh giá cuối cùng trên tập Test:\")\n        \n        # Classification Report\n        print(\"\\nClassification Report:\\n\", classification_report(y_true_final, y_pred_final, target_names=target_names_str))\n        \n        # Confusion Matrix\n        cm = confusion_matrix(y_true_final, y_pred_final)\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names_str, yticklabels=target_names_str)\n        plt.xlabel('Predicted Label')\n        plt.ylabel('True Label')\n        plt.title('Confusion Matrix on Test Set')\n        cm_path = os.path.join(KAGGLE_OUTPUT_PATH, f'{MODEL_ID}_confusion_matrix.png')\n        plt.savefig(cm_path)\n        print(f\"   Đã lưu Confusion Matrix tại: {cm_path}\")\n        plt.show()\n\n        # Tính AUC đa lớp (One-vs-Rest)\n        y_test_ohe_for_auc = y_test_ohe\n        y_pred_probs_for_auc = y_pred_probs_final\n        \n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n        for i in range(NUM_CLASSES):\n            fpr[i], tpr[i], _ = roc_curve(y_test_ohe_for_auc[:, i], y_pred_probs_for_auc[:, i])\n            roc_auc[i] = sklearn_auc(fpr[i], tpr[i])\n\n        # (Phần tính Micro và Macro AUC giữ nguyên)\n        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_ohe_for_auc.ravel(), y_pred_probs_for_auc.ravel())\n        roc_auc[\"micro\"] = sklearn_auc(fpr[\"micro\"], tpr[\"micro\"])\n        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(NUM_CLASSES)]))\n        mean_tpr = np.zeros_like(all_fpr)\n        for i in range(NUM_CLASSES):\n            mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n        mean_tpr /= NUM_CLASSES\n        fpr[\"macro\"] = all_fpr\n        tpr[\"macro\"] = mean_tpr\n        roc_auc[\"macro\"] = sklearn_auc(fpr[\"macro\"], tpr[\"macro\"])\n\n        # Vẽ ROC Curves\n        plt.figure(figsize=(10, 8))\n        plt.plot(fpr[\"micro\"], tpr[\"micro\"], label=f'Micro-average ROC (AUC = {roc_auc[\"micro\"]:.2f})', color='deeppink', linestyle=':', linewidth=4)\n        plt.plot(fpr[\"macro\"], tpr[\"macro\"], label=f'Macro-average ROC (AUC = {roc_auc[\"macro\"]:.2f})', color='navy', linestyle=':', linewidth=4)\n        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green'])\n        for i, color in zip(range(NUM_CLASSES), colors):\n            plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve of class {target_names_str[i]} (AUC = {roc_auc[i]:.2f})')\n\n        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n        plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n        plt.title('Receiver Operating Characteristic (ROC) on Test Set')\n        plt.legend(loc=\"lower right\")\n        roc_path = os.path.join(KAGGLE_OUTPUT_PATH, f'{MODEL_ID}_roc_curve.png')\n        plt.savefig(roc_path)\n        print(f\"   Đã lưu ROC Curve tại: {roc_path}\")\n        plt.show()\n    else:\n        print(\"\\n   Không thể thực hiện đánh giá cuối cùng do thiếu kết quả từ các fold.\")\nelse:\n     print(\"\\n--- Bỏ qua Đánh giá cuối cùng trên tập Test do thiếu dữ liệu Test (X_test hoặc y_test là None) ---\")\n\nprint(\"\\n=== QUY TRÌNH HOÀN TẤT ====\")","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"completed"},"tags":[],"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"id":"424666a9","cell_type":"code","source":"# PHÂN TÍCH GRAD-CAM TRÊN TẬP TEST\n# Logic Grad-CAM này vẫn hoạt động cho CNN vì nó tìm 'top_conv' bên trong base_model\n\nprint(\"--- Chuẩn bị dữ liệu Test cho việc phân tích Grad-CAM ---\\n\")\nif 'test_ds' in locals(): # Kiểm tra xem test_ds đã được tạo ở ô trước chưa\n    \n    @tf.function\n    def get_grad_cam_batched(model, img_batch):\n        \"\"\"\n        Phiên bản Grad-CAM linh hoạt, hoạt động với các model lồng nhau.\n        \"\"\"\n        # Tìm lớp Conv cuối cùng trong base_model\n        last_conv_layer = model.base_model.get_layer(\"top_conv\")\n        \n        # Tạo grad_model\n        grad_model = tf.keras.models.Model(\n            [model.inputs], [last_conv_layer.output, model.output]\n        )\n        \n        with tf.GradientTape() as tape:\n            last_conv_layer_output_value, preds = grad_model(img_batch)\n            pred_indices = tf.argmax(preds, axis=1)\n            class_channels = tf.gather(preds, pred_indices, axis=1, batch_dims=1)\n\n        grads = tape.gradient(class_channels, last_conv_layer_output_value)\n        pooled_grads = tf.reduce_mean(grads, axis=(1, 2))\n        \n        heatmap_batch = tf.einsum('bhwc,bc->bhw', last_conv_layer_output_value, pooled_grads)\n        heatmap_batch = tf.maximum(heatmap_batch, 0)\n        \n        max_vals = tf.reduce_max(heatmap_batch, axis=(1, 2), keepdims=True)\n        heatmap_batch = heatmap_batch / (max_vals + tf.keras.backend.epsilon())\n        \n        return heatmap_batch, preds\n\n    # Hàm run_grad_cam_analysis_final\n    def run_grad_cam_analysis_final(model, model_name, output_base_path, test_dataset):\n        print(f\"\\n--- Bắt đầu phân tích cho mô hình: {model_name} ---\")\n        \n        # SỬA LỖI: Sử dụng 'target_names_str' thay vì 'target_names'\n        results_by_class = { name: {'correct_heatmaps': [], 'correct_confidences': [], 'correct_images': [],\n                                    'incorrect_heatmaps': [], 'incorrect_confidences': [], 'incorrect_images': []}\n                            for name in target_names_str }\n\n        print(\"  - Xử lý các batch trên dataset...\")\n        for images_batch, labels_batch in tqdm(test_dataset, desc=f\"Analyzing {model_name}\"):\n            heatmap_batch, preds_batch = get_grad_cam_batched(model, images_batch)\n            y_pred_probs_batch = tf.nn.softmax(preds_batch).numpy()\n            y_pred_batch = np.argmax(y_pred_probs_batch, axis=1)\n            y_true_batch = np.argmax(labels_batch.numpy(), axis=1)\n\n            for i in range(images_batch.shape[0]):\n                y_pred, y_true = y_pred_batch[i], y_true_batch[i]\n                # SỬA LỖI: Sử dụng 'target_names_str'\n                true_class_name = target_names_str[y_true] \n                \n                if y_pred == y_true:\n                    results_by_class[true_class_name]['correct_heatmaps'].append(heatmap_batch[i].numpy())\n                    results_by_class[true_class_name]['correct_confidences'].append(y_pred_probs_batch[i, y_pred])\n                    results_by_class[true_class_name]['correct_images'].append(images_batch[i].numpy())\n                else:\n                    results_by_class[true_class_name]['incorrect_heatmaps'].append(heatmap_batch[i].numpy())\n                    results_by_class[true_class_name]['incorrect_confidences'].append(y_pred_probs_batch[i, y_pred])\n                    results_by_class[true_class_name]['incorrect_images'].append(images_batch[i].numpy())\n        \n        # (Bạn có thể thêm code ở đây để lưu các heatmap đã thu thập được)\n        print(f\"  - Phân tích cho {model_name} hoàn tất.\")\n\n    \n    # --- VÒNG LẶP CHÍNH ĐỂ PHÂN TÍCH 5 FOLDS ---\n    grad_cam_main_path = os.path.join(KAGGLE_OUTPUT_PATH, \"grad_cam_detailed_analysis\")\n    os.makedirs(grad_cam_main_path, exist_ok=True)\n    \n    for fold_number in range(1, N_SPLITS + 1):\n        print(f\"\\n---> Bắt đầu phân tích Grad-CAM cho Fold {fold_number}/{N_SPLITS}...\")\n        model_path = os.path.join(KAGGLE_OUTPUT_PATH, f'{MODEL_ID}_fold_{fold_number}.keras')\n        if not os.path.exists(model_path):\n            print(f\"Bỏ qua Fold {fold_number}, không tìm thấy file.\")\n            continue\n        \n        try:\n            with strategy.scope():\n                # SỬA LỖI: Đổi 'FinalModel' thành 'FinalModelCNN'\n                model = tf.keras.models.load_model(model_path, custom_objects={'FinalModelCNN': FinalModelCNN, 'MacroF1Score': MacroF1Score})\n            \n            model_name = f\"fold_{fold_number}\"\n            fold_output_path = os.path.join(grad_cam_main_path, model_name)\n            os.makedirs(fold_output_path, exist_ok=True)\n            \n            # Gọi hàm phân tích với test_ds (đã được batch)\n            run_grad_cam_analysis_final(model, model_name, fold_output_path, test_ds)\n        \n        except Exception as e:\n            print(f\"!!! Lỗi khi phân tích Grad-CAM cho Fold {fold_number}: {e}\")\n            \n    print(\"\\n--- Toàn bộ quá trình phân tích Grad-CAM đã hoàn tất ---\")\nelse:\n    print(\"Lỗi: Không tìm thấy 'test_ds'. Vui lòng chạy ô đánh giá (Cell 7) trước.\")","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"completed"},"tags":[],"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"id":"4fe40d01","cell_type":"code","source":"# CHUYỂN ĐỔI SANG TFLITE\n\nprint(\"--- Bắt đầu quy trình chuyển đổi 5-Fold sang TFLite ---\")\nif 'fold_f1s' in locals() and len(fold_f1s) == N_SPLITS:\n    \n    # Dùng X_train_all để tạo representative dataset\n    \n    for fold_number in range(1, N_SPLITS + 1):\n        print(\"=\" * 60)\n        print(f\"--- Bắt đầu chuyển đổi cho Fold {fold_number} ---\")\n        \n        MODEL_PATH = os.path.join(KAGGLE_OUTPUT_PATH, f'{MODEL_ID}_fold_{fold_number}.keras')\n        TFLITE_MODEL_PATH = os.path.join(KAGGLE_OUTPUT_PATH, f'model_fold_{fold_number}_quantized.tflite')\n\n        if not os.path.exists(MODEL_PATH):\n            print(f\"Lỗi: Không tìm thấy file model tại '{MODEL_PATH}'. Bỏ qua fold này.\")\n            continue\n\n        print(\"Đang tải lại model .keras...\")\n        with strategy.scope():\n            # SỬA LỖI: Đổi 'FinalModel' thành 'FinalModelCNN'\n            model_to_convert = tf.keras.models.load_model(MODEL_PATH, custom_objects={'FinalModelCNN': FinalModelCNN, 'MacroF1Score': MacroF1Score})\n\n        print(f\"Đang tạo representative dataset...\")\n        def representative_data_gen():\n            # Lấy 150 mẫu ngẫu nhiên từ TẬP TRAIN GỐC để hiệu chỉnh\n            for i in np.random.choice(len(X_train_all), 150, replace=False):\n                img = X_train_all[i]\n                # Sử dụng hàm tiền xử lý đã được cập nhật\n                img_processed = preprocess_npy_image(img) \n                yield [tf.expand_dims(img_processed, axis=0)]\n\n        print(f\"Đang chuyển đổi mô hình của Fold {fold_number}...\")\n        converter = tf.lite.TFLiteConverter.from_keras_model(model_to_convert)\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n        converter.representative_dataset = representative_data_gen\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n        converter.inference_input_type = tf.float32\n        converter.inference_output_type = tf.float32\n\n        tflite_quant_model = converter.convert()\n\n        with open(TFLITE_MODEL_PATH, 'wb') as f:\n            f.write(tflite_quant_model)\n        \n        print(f\"Đã lưu thành công model TFLite cho Fold {fold_number} tại: {TFLITE_MODEL_PATH}\")\n        print(f\"Kích thước file: {len(tflite_quant_model) / (1024 * 1024):.2f} MB\")\n        \n        del model_to_convert\n        gc.collect()\n        tf.keras.backend.clear_session()\n\n    print(\"=\" * 60)\n    print(\"\\nHoàn tất chuyển đổi cho cả 5 mô hình!\")\nelse:\n    print(\"Lỗi: Không tìm thấy kết quả của 5 fold. Vui lòng chạy ô huấn luyện (Cell 6) trước.\")","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"completed"},"tags":[],"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}