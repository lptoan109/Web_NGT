{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":12870874,"sourceType":"datasetVersion","datasetId":8141736}],"dockerImageVersionId":31091,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# CÀI ĐẶT & CẤU HÌNH (SETUP & CONFIGURATION)\n\n# 0.1. Cài đặt các thư viện cần thiết\n!pip install -q fpdf2 noisereduce librosa tensorflow scikit-learn matplotlib seaborn pytz PyDrive2 \n\n# 0.2. Import thư viện\nimport os\nimport glob\nimport random\nimport datetime\nimport pytz\nimport shutil\nimport joblib\nimport zipfile\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nfrom fpdf import FPDF\nfrom tqdm import tqdm\nimport librosa\nimport noisereduce as nr\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D, AveragePooling2D\nfrom tensorflow.keras.applications import ResNet50V2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.utils import class_weight\nfrom pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\nfrom kaggle_secrets import UserSecretsClient\nfrom oauth2client.service_account import ServiceAccountCredentials\nfrom tensorflow.keras.regularizers import l2\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.metrics import roc_curve, auc as sklearn_auc\nfrom sklearn.preprocessing import label_binarize\nfrom itertools import cycle\n\ntry:\n    # Cố gắng kết nối với TPU bằng cách chỉ định tpu='local'\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\n    print('Đã tìm thấy TPU Resolver.')\n    \n    # Kết nối và khởi tạo hệ thống TPU\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    print('Đã khởi tạo hệ thống TPU.')\n\n    # Tạo strategy\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('THÀNH CÔNG: Đã tạo TPUStrategy!')\n    print(f'Số lượng nhân (replicas): {strategy.num_replicas_in_sync}')\n    \nexcept (ValueError, RuntimeError) as e:\n    # Nếu vẫn không tìm thấy TPU, tự động chuyển về chiến lược mặc định\n    print(f'Lỗi kết nối TPU: {e}')\n    print('Không tìm thấy TPU. Sử dụng chiến lược mặc định cho GPU/CPU.')\n    strategy = tf.distribute.get_strategy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T03:11:45.909308Z","iopub.execute_input":"2025-08-29T03:11:45.909527Z","iopub.status.idle":"2025-08-29T03:12:49.843115Z","shell.execute_reply.started":"2025-08-29T03:11:45.909505Z","shell.execute_reply":"2025-08-29T03:12:49.836856Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1756437156.355597      10 common_lib.cc:612] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:230\n/usr/local/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n/usr/local/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n The versions of TensorFlow you are currently using is 2.18.0 and is not supported. \nSome things might work, some things might not.\nIf you were to encounter a bug, do not file an issue.\nIf you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \nYou can find the compatibility matrix in TensorFlow Addon's readme:\nhttps://github.com/tensorflow/addons\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m label_binarize\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cycle\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_addons\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtfa\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Cố gắng kết nối với TPU bằng cách chỉ định tpu='local'\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     tpu \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mcluster_resolver\u001b[38;5;241m.\u001b[39mTPUClusterResolver(tpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow_addons/__init__.py:23\u001b[0m\n\u001b[1;32m     20\u001b[0m _check_tf_version()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Local project imports\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_addons\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_addons\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callbacks\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_addons\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m image\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow_addons/activations/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Additional activation functions.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_addons\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgelu\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gelu\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_addons\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhardshrink\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hardshrink\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_addons\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlisht\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lisht\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow_addons/activations/gelu.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_addons\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorLike\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mregister_keras_serializable(package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAddons\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgelu\u001b[39m(x: TensorLike, approximate: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tf\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gaussian Error Linear Unit.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    Computes gaussian error linear:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m        A `Tensor`. Has the same type as `x`.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow_addons/utils/types.py:29\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# TODO: Remove once https://github.com/tensorflow/tensorflow/issues/44613 is resolved\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Version(tf\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mrelease \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.13\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrelease:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# New versions of Keras require importing from `keras.src` when\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# importing internal symbols.\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_tensor\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m Version(tf\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mrelease \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrelease:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_tensor\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.src.engine'"],"ename":"ModuleNotFoundError","evalue":"No module named 'keras.src.engine'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# THIẾT LẬP CẤU HÌNH \n\n# tf.keras.mixed_precision.set_global_policy('mixed_bfloat16')\n\n# --- Các cấu hình cơ bản ---\nSEED = 42\ndef set_seed(seed_value):\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    #os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    tf.random.set_seed(seed_value)\nset_seed(SEED)\n\nKAGGLE_PROCESSED_DATA_PATH = \"/kaggle/input/ngt-spectrogram-id/\"\nKAGGLE_OUTPUT_PATH = \"/kaggle/working/output_results\"\nos.makedirs(KAGGLE_OUTPUT_PATH, exist_ok=True)\n\nCLASSES_TO_TRAIN = ['covid', 'asthma', 'healthy', 'tuberculosis']\nALL_CLASSES = ['healthy', 'asthma', 'covid', 'tuberculosis']\nN_SPLITS = 5\nTEST_SPLIT_RATIO = 0.15\nUSE_DATA_AUGMENTATION = True # Bật/tắt augmentation ở đây\nUSE_FOCAL_LOSS = True\n\nMODEL_ID = f'ResNet50V2_CV_TPU'\nEPOCHS = 150\nEPOCHSFINAL = 50\nEARLY_STOPPING_PATIENCE = 7\nMIN_DELTA = 1e-4\nSHUFFLE_BUFFER_SIZE = 2048 \nWEIGHT_DECAY = 1e-4\nLEARNING_RATE = 3e-5\nGAMMA = 2.0 \n# --- ĐỊNH NGHĨA BATCH SIZE ---\n# BATCH_SIZE này là batch size cho mỗi nhân TPU (per-replica)\nBATCH_SIZE = 16\n# Tính toán GLOBAL_BATCH_SIZE để dùng trong pipeline\n# Biến 'strategy' được lấy từ ô code đầu tiên\nGLOBAL_BATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync\nprint(f\"Batch size mỗi nhân: {BATCH_SIZE}\")\nprint(f\"Global batch size (tổng cộng): {GLOBAL_BATCH_SIZE}\")\n\n# INPUT_SHAPE sẽ được cập nhật lại ở ô chuẩn bị dữ liệu\nINPUT_SHAPE = (256, 126, 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T03:12:49.844840Z","iopub.status.idle":"2025-08-29T03:12:49.845492Z","shell.execute_reply.started":"2025-08-29T03:12:49.845024Z","shell.execute_reply":"2025-08-29T03:12:49.845038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# KHỞI TẠO CÁC HÀM CẦN THIẾT (PHIÊN BẢN ĐÚNG)\n\ndef get_patient_id(filepath, class_name):\n    filename = os.path.basename(filepath)\n    if class_name.lower() in ['asthma', 'covid', 'healthy']:\n        return filename.split('_')[0]\n    elif class_name.lower() == 'tuberculosis':\n        return '_'.join(filename.split('_')[:-1]).replace('.npy', '')\n    else:\n        return filename.split('_')[0]\n\nclass MacroF1Score(tf.keras.metrics.Metric):\n    \"\"\"\n    Lớp metric để tính toán Macro F1-Score một cách chính xác trên toàn bộ epoch.\n    \"\"\"\n    def __init__(self, num_classes, name='f1_macro', **kwargs):\n        super(MacroF1Score, self).__init__(name=name, **kwargs)\n        self.num_classes = num_classes\n        # Tạo các biến để lưu trữ các giá trị TP, FP, FN qua các batch\n        self.true_positives = self.add_weight(name='tp', shape=(num_classes,), initializer='zeros')\n        self.false_positives = self.add_weight(name='fp', shape=(num_classes,), initializer='zeros')\n        self.false_negatives = self.add_weight(name='fn', shape=(num_classes,), initializer='zeros')\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        # Chuyển đổi đầu ra của model (logits) và nhãn thật\n        y_pred_labels = tf.argmax(tf.nn.softmax(y_pred), axis=1)\n        y_true_labels = tf.argmax(y_true, axis=1)\n        \n        # Tính ma trận nhầm lẫn cho batch hiện tại\n        cm = tf.math.confusion_matrix(y_true_labels, y_pred_labels, num_classes=self.num_classes, dtype=tf.float32)\n        \n        # Từ ma trận nhầm lẫn, tính TP, FP, FN cho mỗi lớp\n        tp = tf.linalg.diag_part(cm)\n        fp = tf.reduce_sum(cm, axis=0) - tp\n        fn = tf.reduce_sum(cm, axis=1) - tp\n        \n        # Cập nhật các biến trạng thái\n        self.true_positives.assign_add(tp)\n        self.false_positives.assign_add(fp)\n        self.false_negatives.assign_add(fn)\n\n    def result(self):\n        # Tính toán Precision, Recall từ các giá trị đã tích lũy\n        precision = self.true_positives / (self.true_positives + self.false_positives + tf.keras.backend.epsilon())\n        recall = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n        \n        # Tính F1-Score cho mỗi lớp\n        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n        \n        # Lấy trung bình cộng (macro)\n        macro_f1 = tf.reduce_mean(f1)\n        return macro_f1\n\n    def reset_state(self):\n        # Reset các biến trạng thái về 0 ở đầu mỗi epoch\n        self.true_positives.assign(tf.zeros(self.num_classes))\n        self.false_positives.assign(tf.zeros(self.num_classes))\n        self.false_negatives.assign(tf.zeros(self.num_classes))\n        \ndef parse_tfrecord_fn(example):\n    \"\"\"Hàm đọc và xử lý một mẫu từ file TFRecord.\"\"\"\n    feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([], tf.int64),\n    }\n    example = tf.io.parse_single_example(example, feature_description)\n    image = tf.io.parse_tensor(example['image'], out_type=tf.float32)\n    image = tf.reshape(image, (256, 126))\n    image = tf.stack([image, image, image], axis=-1)\n    image.set_shape(INPUT_SHAPE)\n    image_shape = tf.shape(image)\n    image_flat = tf.reshape(image, (1, -1))\n    def _scale(data):\n        scaled_data = scaler.transform(data)\n        return np.nan_to_num(scaled_data).astype(np.float32)\n    scaled_flat = tf.numpy_function(_scale, [image_flat], tf.float32)\n    image_scaled = tf.reshape(scaled_flat, image_shape)\n    label_encoded = tf.cast(example['label'], tf.int32)\n    label_onehot = tf.one_hot(label_encoded, depth=len(ALL_CLASSES))\n    return image_scaled, label_onehot\n\ndef augment(spectrogram, label):\n    spectrogram = spec_augment(spectrogram)\n    return spectrogram, label\n\ndef focal_loss_from_logits_optimized(alpha, gamma=2.0):\n    \"\"\"\n    Tạo ra hàm Focal Loss phiên bản đầy đủ và sạch sẽ.\n    \n    Args:\n        alpha: Một list hoặc array chứa trọng số cho mỗi lớp.\n        gamma: Hệ số tập trung, mặc định là 2.0.\n    \"\"\"\n    # Chuyển alpha sang dạng tensor để tính toán\n    alpha = tf.constant(alpha, dtype=tf.float32)\n\n    def focal_loss_fixed(y_true, y_pred):\n        y_true = tf.cast(y_true, 'float32')\n        y_pred = tf.cast(y_pred, 'float32')\n        \n        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n        probs = tf.nn.softmax(y_pred)\n        pt = tf.reduce_sum(y_true * probs, axis=-1)\n        focal_term = (1.0 - pt) ** gamma\n        alpha_t = tf.reduce_sum(y_true * alpha, axis=-1)\n        loss = alpha_t * focal_term * cross_entropy\n        \n        return tf.reduce_mean(loss)\n        \n    return focal_loss_fixed\n\ndef spec_augment(spectrogram, time_masking_para=40, frequency_masking_para=30, num_time_masks=1, num_freq_masks=1):\n    spectrogram_aug = spectrogram\n    freq_bins = tf.shape(spectrogram)[1] # Sửa: Lấy chiều tần số từ shape 4D\n    time_steps = tf.shape(spectrogram)[2] # Sửa: Lấy chiều thời gian từ shape 4D\n    \n    for _ in range(num_freq_masks):\n        f = tf.random.uniform(shape=(), minval=0, maxval=frequency_masking_para, dtype=tf.int32)\n        f0 = tf.random.uniform(shape=(), minval=0, maxval=freq_bins - f, dtype=tf.int32)\n        freq_mask_1d = tf.concat([tf.ones((f0,), dtype=spectrogram.dtype), tf.zeros((f,), dtype=spectrogram.dtype), tf.ones((freq_bins - f0 - f,), dtype=spectrogram.dtype)], axis=0)\n        freq_mask_4d = tf.reshape(freq_mask_1d, (1, freq_bins, 1, 1)) # Sửa: Reshape thành 4D để broadcast\n        spectrogram_aug *= freq_mask_4d\n        \n    for _ in range(num_time_masks):\n        t = tf.random.uniform(shape=(), minval=0, maxval=time_masking_para, dtype=tf.int32)\n        t0 = tf.random.uniform(shape=(), minval=0, maxval=time_steps - t, dtype=tf.int32)\n        time_mask_1d = tf.concat([tf.ones((t0,), dtype=spectrogram.dtype), tf.zeros((t,), dtype=spectrogram.dtype), tf.ones((time_steps - t0 - t,), dtype=spectrogram.dtype)], axis=0)\n        time_mask_4d = tf.reshape(time_mask_1d, (1, 1, time_steps, 1)) # Sửa: Reshape thành 4D để broadcast\n        spectrogram_aug *= time_mask_4d\n        \n    return spectrogram_aug\n\n# HÀM CREATE_MODEL PHIÊN BẢN ĐÚNG VÀ ĐƠN GIẢN\nclass FinalModel(tf.keras.Model):\n    def __init__(self, input_shape, num_classes):\n        super(FinalModel, self).__init__()\n        # 1. Khởi tạo các lớp bên trong\n        self.base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=input_shape)\n        # Đóng băng các lớp ban đầu\n        for layer in self.base_model.layers[:100]:\n            layer.trainable = False\n        \n        self.pooling = GlobalAveragePooling2D()\n        self.dropout = Dropout(0.5)\n        self.dense_output = Dense(num_classes,\n                                  activation='linear',\n                                  kernel_regularizer=l2(0.001),\n                                  dtype='float32')\n\n    def call(self, inputs, training=None):\n        # 2. Định nghĩa luồng đi của dữ liệu (forward pass)\n        # Tham số 'training' sẽ được Keras tự động truyền vào (True khi fit, False khi evaluate/predict)\n        x = self.base_model(inputs, training=training)\n        x = self.pooling(x)\n        x = self.dropout(x, training=training)\n        outputs = self.dense_output(x)\n        return outputs\n\ndef load_data_from_df(df):\n    X, y = [], []\n    for _, row in df.iterrows():\n        X.append(np.load(row['filepath']))\n        y.append(row['label'])\n    return np.array(X), np.array(y)\n\ndef get_grad_cam_final(model, img_array, last_conv_layer_name, pred_index=None):\n    global strategy\n    \n    with strategy.scope():\n        grad_model = Model(\n            [model.inputs[0]], \n            [model.get_layer(last_conv_layer_name).output, model.output]\n        )\n\n    with tf.GradientTape() as tape:\n        # --- THAY ĐỔI QUAN TRỌNG ---\n        # Thêm training=False để chỉ định rõ đây là chế độ inference\n        last_conv_layer_output, preds = grad_model(tf.cast(img_array, tf.float32), training=False)\n        \n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    \n    heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + tf.keras.backend.epsilon())\n    \n    return heatmap.numpy()\n\ndef overlay_grad_cam(spec, heatmap, alpha=0.6):\n    heatmap_resized = tf.image.resize(heatmap[..., np.newaxis], (spec.shape[0], spec.shape[1]))\n    heatmap_resized = np.uint8(255 * heatmap_resized)\n    jet = plt.cm.get_cmap(\"jet\")\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap_resized.squeeze()]\n    spec_display = np.stack([spec]*3, axis=-1)\n    spec_display = (spec_display - spec_display.min()) / (spec_display.max() - spec_display.min())\n    superimposed_img = jet_heatmap * alpha + spec_display\n    superimposed_img = np.clip(superimposed_img, 0, 1)\n    return superimposed_img\n\nclass PDFReport(FPDF):\n    def header(self):\n        self.set_font('Arial', 'B', 12)\n        self.cell(0, 10, 'BAO CAO KET QUA HUAN LUYEN MO HINH AI', 0, 1, 'C')\n        self.ln(10)\n    def footer(self):\n        self.set_y(-15)\n        self.set_font('Arial', 'I', 8)\n        self.cell(0, 10, f'Trang {self.page_no()}', 0, 0, 'C')\n    def chapter_title(self, title):\n        self.set_font('Arial', 'B', 12)\n        self.cell(0, 10, title, 0, 1, 'L')\n        self.ln(5)\n    def chapter_body(self, content):\n        self.set_font('Arial', '', 10)\n        safe_content = content.encode('latin-1', 'replace').decode('latin-1')\n        self.multi_cell(0, 5, safe_content)\n        self.ln()\n    def add_image_section(self, title, img_path):\n        self.chapter_title(title)\n        if os.path.exists(img_path):\n            self.image(img_path, x=None, y=None, w=180)\n            self.ln(5)\n        else:\n            self.chapter_body(f\"Khong tim thay hinh anh: {img_path}\")\n\ndef authenticate_gdrive():\n    user_secrets = UserSecretsClient()\n    secret_value = user_secrets.get_secret(\"google_service_account_key\")\n    with open(\"service_account.json\", \"w\") as f:\n        f.write(secret_value)\n    scope = [\"https://www.googleapis.com/auth/drive\"]\n    gauth = GoogleAuth()\n    gauth.credentials = ServiceAccountCredentials.from_json_keyfile_name(\"service_account.json\", scope)\n    drive = GoogleDrive(gauth)\n    return drive\n\ndef upload_folder_to_drive(drive, folder_path, parent_folder_id):\n    folder_name = os.path.basename(folder_path)\n    print(f\"Đang tạo thư mục '{folder_name}' trên Google Drive...\")\n    folder_metadata = {'title': folder_name, 'mimeType': 'application/vnd.google-apps.folder', 'parents': [{'id': parent_folder_id}]}\n    folder = drive.CreateFile(folder_metadata)\n    folder.Upload()\n    \n    print(f\"Bắt đầu tải nội dung của '{folder_name}'...\")\n    for item in tqdm(os.listdir(folder_path), desc=f\"Uploading {folder_name}\"):\n        item_path = os.path.join(folder_path, item)\n        if os.path.isfile(item_path):\n            gfile = drive.CreateFile({'title': item, 'parents': [{'id': folder['id']}]})\n            gfile.SetContentFile(item_path)\n            gfile.Upload(param={'supportsTeamDrives': True})\n        elif os.path.isdir(item_path):\n            upload_folder_to_drive(drive, item_path, folder['id'])\n\ndef _bytes_feature(value):\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))):\n        value = value.numpy()\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _int64_feature(value):\n    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef serialize_example(image, label):\n    \"\"\"Creates a tf.train.Example message ready to be written to a file.\"\"\"\n    feature = {\n        'image': _bytes_feature(tf.io.serialize_tensor(image)),\n        'label': _int64_feature(label)\n    }\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n    return example_proto.SerializeToString()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T03:12:49.847977Z","iopub.status.idle":"2025-08-29T03:12:49.848446Z","shell.execute_reply.started":"2025-08-29T03:12:49.848136Z","shell.execute_reply":"2025-08-29T03:12:49.848149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CHUẨN BỊ DỮ LIỆU VÀ TẠO TFRECORD\nsuspicious_files_to_remove = [\n    '/kaggle/input/ngt-spectrogram-id/healthy/P0030101_123370_Dkwg3F7jMGaR7kbc-seg2.npy',\n    '/kaggle/input/ngt-spectrogram-id/covid/P0032202_15897_PcbyJQWemBfghUYp-seg2.npy',\n    '/kaggle/input/ngt-spectrogram-id/covid/P0027142_5701_hupBI5CxKMNCfe8b-seg1.npy',\n    '/kaggle/input/ngt-spectrogram-id/covid/P0056214_89533_0WsmNRSKuQFGodg1-seg1.npy',\n]\n# --- BƯỚC 1: TẢI VÀ PHÂN CHIA DỮ LIỆU BAN ĐẦU ---\nprint(\"Bắt đầu chuẩn bị và phân chia dữ liệu...\")\nall_files_to_split = []\nfor class_name in ALL_CLASSES:\n    source_dir = os.path.join(KAGGLE_PROCESSED_DATA_PATH, class_name)\n    if os.path.exists(source_dir):\n        files = glob.glob(os.path.join(source_dir, '*.npy'))\n        for f in files:\n            all_files_to_split.append({'filepath': f, 'label': class_name})\n\nall_data_df = pd.DataFrame(all_files_to_split)\nall_data_df['patient_id'] = all_data_df.apply(lambda row: get_patient_id(row['filepath'], row['label']), axis=1)\n\nprint(f\"Số lượng mẫu ban đầu: {len(all_data_df)}\")\nall_data_df = all_data_df[~all_data_df['filepath'].isin(suspicious_files_to_remove)].reset_index(drop=True)\nprint(f\"Số lượng mẫu sau khi lọc bỏ file 'im lặng': {len(all_data_df)}\")\n\nprint(\"Tách tập Test cuối cùng (Hold-out set)...\")\npatient_ids = all_data_df['patient_id'].unique()\nnp.random.shuffle(patient_ids)\ntest_patient_count = int(len(patient_ids) * TEST_SPLIT_RATIO)\ntest_patients = patient_ids[:test_patient_count]\ntrain_val_patients = patient_ids[test_patient_count:]\n\ntest_df = all_data_df[all_data_df['patient_id'].isin(test_patients)].reset_index(drop=True)\ntrain_val_df = all_data_df[all_data_df['patient_id'].isin(train_val_patients)].reset_index(drop=True)\n\nprint(f\"Đã tách: {len(train_val_df)} mẫu cho Train/Validation (CV) và {len(test_df)} mẫu cho Test cuối cùng.\")\n\n# --- BƯỚC 2: KHỞI TẠO LABEL ENCODER VÀ STANDARD SCALER ---\nle = LabelEncoder().fit(ALL_CLASSES)\n# Cập nhật INPUT_SHAPE từ một file mẫu\nsample_spec = np.load(train_val_df['filepath'][0])\nINPUT_SHAPE = (sample_spec.shape[0], sample_spec.shape[1], 3)\nprint(f\"Kích thước input được cập nhật: {INPUT_SHAPE}\")\n\nprint(\"Đang fit StandardScaler...\")\nscaler_fit_sample_df = train_val_df.sample(n=min(len(train_val_df), 500), random_state=SEED)\nscaler_fit_data = []\nfor filepath in scaler_fit_sample_df['filepath']:\n    spec = np.load(filepath)\n    # QUAN TRỌNG: Stack 3 kênh TRƯỚC KHI flatten, để khớp với pipeline\n    spec_3_channels = np.stack([spec, spec, spec], axis=-1)\n    scaler_fit_data.append(spec_3_channels.flatten())\nscaler = StandardScaler().fit(scaler_fit_data)\nprint(\"Fit StandardScaler hoàn tất.\")\n\n# --- BƯỚC 3: CHUYỂN ĐỔI DỮ LIỆU SANG ĐỊNH DẠNG TFRECORD ---\nTFRECORD_OUTPUT_PATH = \"/kaggle/working/tfrecords\"\nos.makedirs(TFRECORD_OUTPUT_PATH, exist_ok=True)\nprint(f\"Bắt đầu chuyển đổi dữ liệu sang TFRecord tại: {TFRECORD_OUTPUT_PATH}\")\n\nall_dfs = {'train_val': train_val_df, 'test': test_df}\n\nfor df_name, df in all_dfs.items():\n    print(f\"--- Đang xử lý tập {df_name} ---\")\n    tfrecord_path = os.path.join(TFRECORD_OUTPUT_PATH, f\"{df_name}.tfrec\")\n    \n    with tf.io.TFRecordWriter(tfrecord_path) as writer:\n        # Sử dụng tqdm tiêu chuẩn để tránh lỗi ImportError\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Creating {df_name}.tfrec\"):\n            spectrogram = np.load(row['filepath']).astype(np.float32)\n            label_encoded = le.transform([row['label']])[0]\n            \n            example = serialize_example(spectrogram, label_encoded)\n            writer.write(example)\n            \nprint(\"\\nChuyển đổi sang TFRecord hoàn tất!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T03:12:49.850027Z","iopub.status.idle":"2025-08-29T03:12:49.851006Z","shell.execute_reply.started":"2025-08-29T03:12:49.850192Z","shell.execute_reply":"2025-08-29T03:12:49.850206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# HUẤN LUYỆN MÔ HÌNH Cross-Validation\n\n# --- BƯỚC 1: KHỞI TẠO CÁC BIẾN CẦN THIẾT CHO VIỆC CHIA FOLD ---\nprint(\"Đang khởi tạo các biến cho Cross-Validation...\")\nAUTOTUNE = tf.data.AUTOTUNE\n\nskf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\ncv_data_to_split = train_val_df[train_val_df['label'].isin(CLASSES_TO_TRAIN)]\nX_cv_paths = cv_data_to_split['filepath'].values\ny_cv_labels = le.transform(cv_data_to_split['label'])\ngroups_cv = cv_data_to_split['patient_id'].values\n\n# THÊM 2: Chuẩn bị list để lưu F1-score\nfold_accuracies, fold_losses, fold_aucs, fold_f1s = [], [], [], []\n\nprint(\"Đang tính toán trọng số alpha cho Focal Loss...\")\nclass_weights_array = class_weight.compute_class_weight('balanced', classes=np.unique(y_cv_labels), y=y_cv_labels)\nalpha_weights_list = class_weights_array.tolist()\nprint(\"Trọng số Alpha được tính toán:\")\nfor i, w in enumerate(alpha_weights_list):\n    class_name = le.inverse_transform([i])[0]\n    print(f\"- Lớp '{class_name}': {w:.2f}\")\n    \n# --- BƯỚC 2: BẮT ĐẦU VÒNG LẶP CROSS-VALIDATION ---\nLOCAL_TFRECORD_PATH = TFRECORD_OUTPUT_PATH\nTRAIN_VAL_TFREC = os.path.join(LOCAL_TFRECORD_PATH, 'train_val.tfrec')\nprint(f\"Sẵn sàng đọc dữ liệu TFRecord từ: {TRAIN_VAL_TFREC}\")\n\nfor fold, (train_indices, val_indices) in enumerate(skf.split(X_cv_paths, y_cv_labels, groups_cv)):\n    fold_number = fold + 1\n    print(\"-\" * 50 + f\"\\nBắt đầu Fold {fold_number}/{N_SPLITS}\\n\" + \"-\" * 50)\n    \n    # Tạo pipeline dữ liệu (giữ nguyên)\n    train_indices_tf = tf.constant(train_indices, dtype=tf.int64)\n    val_indices_tf = tf.constant(val_indices, dtype=tf.int64)\n    full_ds = tf.data.TFRecordDataset(TRAIN_VAL_TFREC).enumerate()\n    train_ds = full_ds.filter(lambda i, data: tf.reduce_any(i == train_indices_tf)).map(lambda i, data: data)\n    val_ds = full_ds.filter(lambda i, data: tf.reduce_any(i == val_indices_tf)).map(lambda i, data: data)\n    train_ds = train_ds.map(parse_tfrecord_fn, num_parallel_calls=AUTOTUNE)\n    val_ds = val_ds.map(parse_tfrecord_fn, num_parallel_calls=AUTOTUNE)\n    train_ds = train_ds.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE).repeat().batch(GLOBAL_BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n    val_ds = val_ds.cache().batch(GLOBAL_BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n\n    # --- TẠO VÀ BIÊN DỊCH MODEL TRONG STRATEGY.SCOPE ---\n    with strategy.scope():\n        model = FinalModel(input_shape=INPUT_SHAPE, num_classes=len(ALL_CLASSES))\n        \n        f1_macro = MacroF1Score(num_classes=len(ALL_CLASSES), name='f1_macro')\n        optimizer = tf.keras.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY, clipvalue=1.0)\n        \n        if USE_FOCAL_LOSS:\n            loss_function = focal_loss_from_logits_optimized(alpha=alpha_weights_list, gamma=GAMMA)\n        else:\n            loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n            \n        model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy', tf.keras.metrics.AUC(name='auc'), f1_macro])\n\n    # --- TÍNH TOÁN STEPS VÀ BẮT ĐẦU HUẤN LUYỆN ---\n    steps_per_epoch = len(train_indices) // GLOBAL_BATCH_SIZE\n    validation_steps = len(val_indices) // GLOBAL_BATCH_SIZE\n    \n    print(f\"Số bước mỗi epoch: {steps_per_epoch} | Số bước validation: {validation_steps}\")\n\n    # THÊM 3: Sửa lại Callbacks cho nhất quán\n    callbacks = [\n        EarlyStopping(monitor='val_f1_macro', mode='max', patience=EARLY_STOPPING_PATIENCE, min_delta=MIN_DELTA, restore_best_weights=True),\n        ReduceLROnPlateau(monitor='val_f1_macro', mode='max', patience=5, min_lr=1e-8, verbose=1)\n    ]\n    \n    history = model.fit(\n        train_ds, \n        validation_data=val_ds, \n        epochs=EPOCHS, \n        steps_per_epoch=steps_per_epoch,\n        validation_steps=validation_steps,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # --- VẼ BIỂU ĐỒ VÀ ĐÁNH GIÁ ---\n    print(\"Đang tạo và lưu biểu đồ huấn luyện...\")\n    plt.figure(figsize=(18, 7))\n    plt.suptitle(f'Training Metrics for Fold {fold_number}', fontsize=16)\n    \n    # --- Biểu đồ cho các chỉ số (Accuracy, AUC, F1-Macro) ---\n    plt.subplot(1, 2, 1)\n    # Vẽ các chỉ số của tập Train\n    plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue', linestyle='-')\n    plt.plot(history.history['auc'], label='Training AUC', color='green', linestyle='-')\n    plt.plot(history.history['f1_macro'], label='Training F1-Macro', color='red', linestyle='-')\n    \n    # Vẽ các chỉ số của tập Validation\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='blue', linestyle='--')\n    plt.plot(history.history['val_auc'], label='Validation AUC', color='green', linestyle='--')\n    plt.plot(history.history['val_f1_macro'], label='Validation F1-Macro', color='red', linestyle='--')\n    \n    # Cập nhật lại tiêu đề và nhãn\n    plt.title('Biểu đồ các chỉ số (Accuracy, AUC, F1-Macro)')\n    plt.xlabel('Epoch')\n    plt.ylabel('Giá trị')\n    plt.legend(loc='lower right')\n    plt.grid(True)\n    \n    # --- Biểu đồ cho Loss ---\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Training Loss', color='orange')\n    plt.plot(history.history['val_loss'], label='Validation Loss', color='purple')\n    plt.title('Biểu đồ Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='upper right')\n    plt.grid(True)\n    \n    # Lưu và đóng hình ảnh\n    plot_filename = f'fold_{fold_number}_metrics.png'\n    plot_filepath = os.path.join(KAGGLE_OUTPUT_PATH, plot_filename)\n    plt.savefig(plot_filepath)\n    plt.close()\n    \n    print(f\"Đã lưu biểu đồ cho Fold {fold_number} tại: {plot_filepath}\")\n    \n    # THÊM 4: Sửa lại model.evaluate để nhận đủ 4 giá trị\n    loss, accuracy, auc, f1 = model.evaluate(val_ds, verbose=0)\n    print(f\"Fold {fold_number} - Validation Loss: {loss:.4f}, Validation Accuracy: {accuracy:.4f}, Validation AUC: {auc:.4f}, Validation F1-Macro: {f1:.4f}\")\n    \n    fold_aucs.append(auc)\n    fold_losses.append(loss)\n    fold_accuracies.append(accuracy)\n    fold_f1s.append(f1) # Thêm lưu trữ F1\n\n    # THÊM 5: Sửa lại câu lệnh print cuối cùng\n    print(\"=\" * 50 + \"\\nKết quả Cross-Validation:\\n\" \n      + f\"Validation Accuracy trung bình: {np.mean(fold_accuracies):.4f} +/- {np.std(fold_accuracies):.4f}\\n\"\n      + f\"Validation Loss trung bình: {np.mean(fold_losses):.4f} +/- {np.std(fold_losses):.4f}\\n\"\n      + f\"Validation AUC trung bình: {np.mean(fold_aucs):.4f} +/- {np.std(fold_aucs):.4f}\\n\"\n      + f\"Validation F1-Macro trung bình: {np.mean(fold_f1s):.4f} +/- {np.std(fold_f1s):.4f}\\n\" # Thêm dòng này\n      + \"=\" * 50)","metadata":{"execution":{"iopub.status.busy":"2025-08-29T03:12:49.852665Z","iopub.status.idle":"2025-08-29T03:12:49.853410Z","shell.execute_reply.started":"2025-08-29T03:12:49.852863Z","shell.execute_reply":"2025-08-29T03:12:49.852877Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# HUẤN LUYỆN MÔ HÌNH CUỐI CÙNG (PHIÊN BẢN HOÀN CHỈNH)\n\nprint(\"Bắt đầu huấn luyện lại mô hình cuối cùng trên toàn bộ dữ liệu Train+Validation...\")\n\n# --- BƯỚC 1: TẠO PIPELINE DỮ LIỆU TỪ TFRECORD ---\nprint(f\"Sử dụng dữ liệu từ: {TRAIN_VAL_TFREC}\")\n\nfinal_train_ds = tf.data.TFRecordDataset(TRAIN_VAL_TFREC)\nfinal_train_ds = final_train_ds.map(parse_tfrecord_fn, num_parallel_calls=AUTOTUNE)\n\n# Pipeline theo đúng chuẩn\nfinal_train_ds = final_train_ds.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE).repeat().batch(GLOBAL_BATCH_SIZE)\nif USE_DATA_AUGMENTATION:\n    final_train_ds = final_train_ds.map(augment, num_parallel_calls=AUTOTUNE)\nfinal_train_ds = final_train_ds.prefetch(buffer_size=AUTOTUNE)\n\n\n# --- BƯỚC 2: TẠO VÀ BIÊN DỊCH MODEL TRONG STRATEGY.SCOPE ---\n\n# Tính lại alpha cho toàn bộ tập train+val\nprint(\"Đang tính toán lại trọng số alpha cho toàn bộ dữ liệu...\")\nfinal_train_labels = le.transform(train_val_df['label'][train_val_df['label'].isin(CLASSES_TO_TRAIN)])\nfinal_class_weights_array = class_weight.compute_class_weight(\n    'balanced',\n    classes=np.unique(final_train_labels),\n    y=final_train_labels\n)\nfinal_alpha_weights_list = final_class_weights_array.tolist()\nprint(\"Trọng số Alpha cuối cùng được tính toán:\")\nfor i, w in enumerate(final_alpha_weights_list):\n    class_name = le.inverse_transform([i])[0]\n    print(f\"- Lớp '{class_name}': {w:.2f}\")\n\n\nwith strategy.scope():\n    # THAY ĐỔI 1: SỬ DỤNG LỚP FinalModel THAY VÌ HÀM create_model\n    final_model = FinalModel(input_shape=INPUT_SHAPE, num_classes=len(ALL_CLASSES))\n    f1_macro = MacroF1Score(num_classes=len(ALL_CLASSES), name='f1_macro')\n    final_optimizer = tf.keras.optimizers.AdamW(\n        learning_rate=LEARNING_RATE, \n        weight_decay=WEIGHT_DECAY,\n        clipvalue=1.0\n    )\n    \n    if USE_FOCAL_LOSS:\n        loss_function = focal_loss_from_logits_optimized(alpha=final_alpha_weights_list, gamma=GAMMA)\n    else:\n        loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n    \n    final_model.compile(optimizer=final_optimizer, \n                        loss=loss_function, \n                        metrics=['accuracy', tf.keras.metrics.AUC(name='auc'), f1_macro])\n\n# --- BƯỚC 3: HUẤN LUYỆN MODEL ---\nrun_timestamp = datetime.datetime.now(pytz.timezone('Asia/Ho_Chi_Minh')).strftime(\"%Y-%m-%d_%H-%M-%S\")\nmodel_checkpoint_path = os.path.join(KAGGLE_OUTPUT_PATH, f'{MODEL_ID}_final_model_{run_timestamp}.keras')\n\nfinal_steps_per_epoch = len(train_val_df[train_val_df['label'].isin(CLASSES_TO_TRAIN)]) // GLOBAL_BATCH_SIZE\nprint(f\"Số bước mỗi epoch cho huấn luyện cuối cùng: {final_steps_per_epoch}\")\n\n# Lưu ý: monitor='loss' là đúng vì không có tập validation ở đây\nfinal_history = final_model.fit(\n    final_train_ds, \n    epochs=EPOCHSFINAL,\n    steps_per_epoch=final_steps_per_epoch, \n    callbacks=[ModelCheckpoint(filepath=model_checkpoint_path, \n                               save_best_only=True, \n                               monitor='loss',\n                               mode='min',\n                               verbose=1)], \n    verbose=1\n)\n\nprint(\"\\nHuấn luyện mô hình cuối cùng hoàn tất.\")\nprint(f\"Mô hình cuối cùng đã được lưu tại: {model_checkpoint_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T03:12:49.855095Z","iopub.status.idle":"2025-08-29T03:12:49.855765Z","shell.execute_reply.started":"2025-08-29T03:12:49.855478Z","shell.execute_reply":"2025-08-29T03:12:49.855493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ĐÁNH GIÁ MÔ HÌNH VÀ VẼ CÁC SƠ ĐỒ (PHIÊN BẢN HOÀN CHỈNH)\nprint(\"\\nĐang đánh giá mô hình cuối cùng trên tập Test (Hold-out)...\")\n\n# --- BƯỚC 1: TẠO KIẾN TRÚC, TẢI TRỌNG SỐ VÀ BIÊN DỊCH LẠI MODEL ---\n\nprint(\"Đang tạo lại kiến trúc model và tải trọng số đã lưu...\")\nwith strategy.scope():\n    # Tạo một kiến trúc model mới y hệt lúc huấn luyện\n    final_model = create_model(INPUT_SHAPE, len(ALL_CLASSES))\n    \n    # Tải các trọng số đã được huấn luyện vào kiến trúc mới này\n    final_model.load_weights(model_checkpoint_path)\n\n    # --- THÊM VÀO ĐÂY: BIÊN DỊCH LẠI MODEL ---\n    # Model cần được compile với đúng các thiết lập như khi huấn luyện\n    # để có thể tính toán loss và metrics.\n    print(\"Đang biên dịch lại model...\")\n    if USE_FOCAL_LOSS:\n        loss_function = focal_loss_from_logits_optimized(alpha=final_alpha_weights_list, gamma=GAMMA)\n    else:\n        loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n        \n    final_optimizer = tf.keras.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    \n    final_model.compile(optimizer=final_optimizer, \n                        loss=loss_function, \n                        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n\nprint(\"Tải và biên dịch model cuối cùng hoàn tất.\")\n\n\n# --- BƯỚC 2: TẠO PIPELINE DỮ LIỆU CHO TẬP TEST ---\nTEST_TFREC = os.path.join(LOCAL_TFRECORD_PATH, 'test.tfrec')\nprint(f\"Sử dụng dữ liệu test từ: {TEST_TFREC}\")\n\ntest_ds = tf.data.TFRecordDataset(TEST_TFREC)\ntest_ds = test_ds.map(parse_tfrecord_fn, num_parallel_calls=AUTOTUNE)\ntest_ds = test_ds.batch(GLOBAL_BATCH_SIZE, drop_remainder=True)\ntest_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n\n\n# --- BƯỚC 3: ĐÁNH GIÁ VÀ DỰ ĐOÁN TRÊN PIPELINE ---\nprint(\"Đang đánh giá trên tập test...\")\nloss, accuracy, auc = final_model.evaluate(test_ds, verbose=1)\nprint(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}, Test AUC: {auc:.4f}\")\n\nprint(\"Đang dự đoán trên tập test...\")\ny_pred_logits = final_model.predict(test_ds, verbose=1) # Đổi tên thành logits để rõ ràng hơn\ny_pred_encoded = np.argmax(y_pred_logits, axis=1)\n\n# Lấy nhãn thật từ dataframe\nfinal_test_df = test_df[test_df['label'].isin(CLASSES_TO_TRAIN)]\ny_test_labels = final_test_df['label'].values\ny_test_encoded = le.transform(y_test_labels)\ny_test_encoded = y_test_encoded[:len(y_pred_encoded)]\n\ntrained_class_indices = np.unique(y_test_encoded)\ntarget_names_trained = le.inverse_transform(trained_class_indices)\n\n\n# --- BƯỚC 4: TÍNH TOÁN VÀ TRÍCH XUẤT CÁC CHỈ SỐ CHI TIẾT ---\n# Sử dụng output_dict=True để lấy kết quả dưới dạng dictionary\nreport_dict = classification_report(y_test_encoded, y_pred_encoded, target_names=target_names_trained, labels=trained_class_indices, output_dict=True)\nreport_text = classification_report(y_test_encoded, y_pred_encoded, target_names=target_names_trained, labels=trained_class_indices)\n\nprint(\"\\nClassification Report:\\n\", report_text)\n\n# Ví dụ cách trích xuất các chỉ số bạn cần cho báo cáo:\nmacro_f1 = report_dict['macro avg']['f1-score']\nweighted_f1 = report_dict['weighted avg']['f1-score']\nasthma_recall = report_dict['asthma']['recall']\n\nprint(f\"\\\\nCác chỉ số riêng lẻ:\")\nprint(f\"- Macro F1-Score: {macro_f1:.4f}\")\nprint(f\"- Weighted F1-Score: {weighted_f1:.4f}\")\nprint(f\"- Recall của lớp 'asthma': {asthma_recall:.4f}\")\n\n\n# --- BƯỚC 5: VẼ CÁC BIỂU ĐỒ ---\nreport_figs_path = os.path.join(KAGGLE_OUTPUT_PATH, \"report_figures\")\nos.makedirs(report_figs_path, exist_ok=True)\n\n# Biểu đồ training history (giữ nguyên)\nplt.figure(figsize=(8, 6))\nplt.plot(final_history.history['accuracy'], label='Training Accuracy')\nplt.plot(final_history.history['auc'], label='Training AUC') # Thêm AUC vào biểu đồ\nplt.title('Biểu đồ Accuracy & AUC của mô hình cuối cùng')\nplt.xlabel('Epoch')\nplt.ylabel('Value')\nplt.legend(loc='lower right')\naccuracy_plot_path = os.path.join(report_figs_path, f'final_accuracy_plot_{run_timestamp}.png')\nplt.savefig(accuracy_plot_path)\nplt.close()\n\nplt.figure(figsize=(8, 6))\nplt.plot(final_history.history['loss'], label='Training Loss')\nplt.title('Biểu đồ Loss của mô hình cuối cùng')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='upper right')\nloss_plot_path = os.path.join(report_figs_path, f'final_loss_plot_{run_timestamp}.png')\nplt.savefig(loss_plot_path)\nplt.close()\n\n# Ma trận nhầm lẫn (giữ nguyên)\ncm = confusion_matrix(y_test_encoded, y_pred_encoded, labels=trained_class_indices)\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names_trained, yticklabels=target_names_trained)\nplt.title('Ma trận nhầm lẫn trên tập Test cuối cùng')\nplt.ylabel('Nhãn thật')\nplt.xlabel('Nhãn dự đoán')\ncm_plot_path = os.path.join(report_figs_path, f'confusion_matrix_{run_timestamp}.png')\nplt.savefig(cm_plot_path)\nplt.close()\n\n\n# --- BƯỚC 6: TÍNH TOÁN VÀ VẼ ĐƯỜNG CONG ROC CHO TỪNG LỚP ---\n# Chuyển logits thành xác suất\ny_pred_probs = tf.nn.softmax(y_pred_logits).numpy()\n\n# Chuyển nhãn thật sang dạng one-hot để tính ROC cho từng lớp\ny_test_binarized = label_binarize(y_test_encoded, classes=trained_class_indices)\nn_classes = y_test_binarized.shape[1]\n\n# Tính toán ROC và AUC cho từng lớp\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_probs[:, i])\n    roc_auc[i] = sklearn_auc(fpr[i], tpr[i])\n\n# Vẽ đường cong ROC\nplt.figure(figsize=(10, 8))\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'deeppink'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n             label='Đường cong ROC của lớp {0} (AUC = {1:0.2f})'\n             ''.format(target_names_trained[i], roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Tỷ lệ Dương tính Giả (False Positive Rate)')\nplt.ylabel('Tỷ lệ Dương tính Thật (True Positive Rate)')\nplt.title('Đường cong ROC cho từng lớp')\nplt.legend(loc=\"lower right\")\nroc_plot_path = os.path.join(report_figs_path, f'roc_curves_{run_timestamp}.png')\nplt.savefig(roc_plot_path)\nplt.show()\nplt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T03:12:49.856690Z","iopub.status.idle":"2025-08-29T03:12:49.857160Z","shell.execute_reply.started":"2025-08-29T03:12:49.856892Z","shell.execute_reply":"2025-08-29T03:12:49.856906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# VẼ GRAD-CAM (PHIÊN BẢN HOÀN CHỈNH)\n\nprint(\"Đang tạo lại kiến trúc model và tải trọng số đã lưu...\")\nwith strategy.scope():\n    # Tạo một kiến trúc model mới y hệt lúc huấn luyện\n    final_model = create_model(INPUT_SHAPE, len(ALL_CLASSES))\n    \n    # Tải các trọng số đã được huấn luyện vào kiến trúc mới này\n    final_model.load_weights(model_checkpoint_path)\n\nprint(\"Tải trọng số cho model cuối cùng hoàn tất.\")\n\n# --- BƯỚC 1: LẤY DỮ LIỆU TEST TỪ PIPELINE `test_ds` ---\nprint(\"Đang trích xuất dữ liệu từ test_ds để vẽ Grad-CAM...\")\nX_test_list = []\ny_test_onehot_list = []\n\n# Sử dụng tqdm để theo dõi tiến trình\nfor images, labels in tqdm(test_ds, desc=\"Extracting data from test_ds\"):\n    X_test_list.append(images.numpy())\n    y_test_onehot_list.append(labels.numpy())\n\n# Nối các batch lại thành các mảng NumPy lớn\nX_test = np.concatenate(X_test_list, axis=0)\ny_test_onehot = np.concatenate(y_test_onehot_list, axis=0)\ny_test_encoded_from_ds = np.argmax(y_test_onehot, axis=1)\n\nprint(f\"Đã trích xuất thành công {len(X_test)} mẫu.\")\n\n\n# --- BƯỚC 2: TÌM LỚP CONVOLUTION CUỐI CÙNG (giữ nguyên) ---\nlast_conv_layer_name = None\nfor layer in reversed(final_model.layers):\n    if isinstance(layer, tf.keras.layers.GlobalAveragePooling2D):\n        pooling_index = final_model.layers.index(layer)\n        last_conv_layer_name = final_model.layers[pooling_index - 1].name\n        break\nif last_conv_layer_name is None:\n    raise ValueError(\"Không thể tự động tìm thấy lớp phù hợp cho Grad-CAM.\")\nprint(f\"Đã tự động xác định lớp Grad-CAM: {last_conv_layer_name}\")\n\n\n# --- BƯỚC 3: TẠO HÌNH ẢNH GRAD-CAM CHO CÁC MẪU TIÊU BIỂU ---\ngradcam_path = os.path.join(report_figs_path, \"grad_cam\")\nos.makedirs(gradcam_path, exist_ok=True)\nprint(\"Tạo hình ảnh Grad-CAM cho các mẫu tiêu biểu...\")\n\nresults_list = []\nfor i in range(len(y_pred_encoded)):\n    true_label_encoded = y_test_encoded_from_ds[i]\n    pred_label_encoded = y_pred_encoded[i]\n    confidence = y_pred_probs[i][pred_label_encoded]\n    is_correct = (true_label_encoded == pred_label_encoded)\n    results_list.append({'index': i, 'true_label': true_label_encoded, 'pred_label': pred_label_encoded, 'confidence': confidence, 'is_correct': is_correct})\nresults_df = pd.DataFrame(results_list)\n\nfor class_index, class_name in zip(trained_class_indices, target_names_trained):\n    correct_samples = results_df[(results_df['is_correct'] == True) & (results_df['true_label'] == class_index)].nlargest(3, 'confidence')\n    incorrect_samples = results_df[(results_df['is_correct'] == False) & (results_df['true_label'] == class_index)].nlargest(3, 'confidence')\n    \n    for _, row in correct_samples.iterrows():\n        idx = int(row['index'])\n        img_array, spec = X_test[idx][np.newaxis, ...], X_test[idx, :, :, 0]\n        # Đổi tên hàm thành get_grad_cam_final\n        heatmap = get_grad_cam_final(final_model, img_array, last_conv_layer_name, pred_index=class_index)\n        overlay = overlay_grad_cam(spec, heatmap)\n        plt.imshow(overlay)\n        plt.title(f\"Đúng: {class_name}, Tin cậy: {row['confidence']:.2f}\")\n        plt.axis('off')\n        plt.savefig(os.path.join(gradcam_path, f\"correct_{class_name}_{idx}_{run_timestamp}.png\"))\n        plt.close()\n        \n    for _, row in incorrect_samples.iterrows():\n        idx = int(row['index'])\n        pred_class_name = le.inverse_transform([int(row['pred_label'])])[0]\n        img_array, spec = X_test[idx][np.newaxis, ...], X_test[idx, :, :, 0]\n        # Đổi tên hàm thành get_grad_cam_final\n        heatmap = get_grad_cam_final(final_model, img_array, last_conv_layer_name, pred_index=class_index)\n        overlay = overlay_grad_cam(spec, heatmap)\n        plt.imshow(overlay)\n        plt.title(f\"Thật: {class_name}, Sai -> {pred_class_name}, Tin cậy: {row['confidence']:.2f}\")\n        plt.axis('off')\n        plt.savefig(os.path.join(gradcam_path, f\"incorrect_{class_name}_as_{pred_class_name}_{idx}_{run_timestamp}.png\"))\n        plt.close()\n\n# --- BƯỚC 4: TÍNH TOÁN VÀ VẼ GRAD-CAM TRUNG BÌNH ---\nprint(\"Đang tính toán Grad-CAM trung bình cho tất cả các lớp...\")\n\n# Tạo các dictionary để lưu trữ heatmaps\ncorrect_heatmaps = {class_name: [] for class_name in target_names_trained}\nincorrect_heatmaps = {class_name: [] for class_name in target_names_trained}\n\n# Lặp qua tất cả các kết quả để tính heatmap\nfor _, row in tqdm(results_df.iterrows(), total=len(results_df), desc=\"Calculating all heatmaps\"):\n    idx = int(row['index'])\n    true_label_index = int(row['true_label'])\n    true_class_name = le.inverse_transform([true_label_index])[0]\n    \n    img_array = X_test[idx][np.newaxis, ...]\n    # Luôn tính heatmap theo nhãn thật để xem model tập trung vào đâu\n    heatmap = get_grad_cam_final(final_model, img_array, last_conv_layer_name, pred_index=true_label_index)\n    \n    if row['is_correct']:\n        correct_heatmaps[true_class_name].append(heatmap)\n    else:\n        incorrect_heatmaps[true_class_name].append(heatmap)\n\n# Vẽ Grad-CAM trung bình\nfor class_name in target_names_trained:\n    # Xử lý các ca đoán đúng\n    if correct_heatmaps[class_name]:\n        avg_heatmap_correct = np.mean(correct_heatmaps[class_name], axis=0)\n        overlay = overlay_grad_cam(np.zeros(INPUT_SHAPE[:2]), avg_heatmap_correct)\n        plt.imshow(overlay)\n        plt.title(f\"Grad-CAM TB - Đúng cho lớp {class_name}\")\n        plt.axis('off')\n        plt.savefig(os.path.join(gradcam_path, f\"avg_correct_{class_name}_{run_timestamp}.png\"))\n        plt.close()\n        \n    # Xử lý các ca đoán sai\n    if incorrect_heatmaps[class_name]:\n        avg_heatmap_incorrect = np.mean(incorrect_heatmaps[class_name], axis=0)\n        overlay = overlay_grad_cam(np.zeros(INPUT_SHAPE[:2]), avg_heatmap_incorrect)\n        plt.imshow(overlay)\n        plt.title(f\"Grad-CAM TB - Sai cho lớp {class_name}\")\n        plt.axis('off')\n        plt.savefig(os.path.join(gradcam_path, f\"avg_incorrect_{class_name}_{run_timestamp}.png\"))\n        plt.close()\n\nprint(\"Hoàn tất việc tạo Grad-CAM.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T03:12:49.858002Z","iopub.status.idle":"2025-08-29T03:12:49.858431Z","shell.execute_reply.started":"2025-08-29T03:12:49.858138Z","shell.execute_reply":"2025-08-29T03:12:49.858149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TẠO BÁO CÁO PDF\nprint(\"Tạo báo cáo PDF...\")\npdf = PDFReport()\npdf.add_page()\npdf.chapter_title(\"1. Tom tat cau hinh va Ket qua\")\nconfig_summary = f\"\"\"\n- Model ID: {MODEL_ID}\n- Thoi gian chay: {datetime.datetime.now(pytz.timezone('Asia/Ho_Chi_Minh')).strftime(\"%Y-%m-%d %H:%M:%S\")}\n- Cac lop huan luyen: {', '.join(CLASSES_TO_TRAIN)}\n- K-Fold Cross-Validation: {N_SPLITS} folds\n\n--- KET QUA CROSS-VALIDATION ---\n- Validation Accuracy trung binh: {np.mean(fold_accuracies):.4f} +/- {np.std(fold_accuracies):.4f}\n- Validation Loss trung binh: {np.mean(fold_losses):.4f} +/- {np.std(fold_losses):.4f}\n\n--- KET QUA TREN TAP TEST CUOI CUNG ---\n- Test Loss: {loss:.4f}\n- Test Accuracy: {accuracy:.4f}\n\n--- CAU HINH CHI TIET ---\n- SEED: {SEED}\n- Epochs: {EPOCHS} (Patience: {EARLY_STOPPING_PATIENCE})\n- Batch Size: {BATCH_SIZE}\n- Learning Rate: {LEARNING_RATE}\n- Ham Loss: {'Focal Loss' if USE_FOCAL_LOSS else 'Categorical Crossentropy'}\n- Tang cuong du lieu: {'Co (SpecAugment)' if USE_DATA_AUGMENTATION else 'Khong'}\n- Kich thuoc Input: {INPUT_SHAPE}\n\"\"\"\npdf.chapter_body(config_summary)\npdf.add_image_section(\"2. Bieu do Huan luyen cua Mo hinh Cuoi cung\", accuracy_plot_path)\npdf.add_image_section(\"\", loss_plot_path)\npdf.chapter_title(\"3. Danh gia chi tiet tren tap Test\")\npdf.chapter_body(\"Bao cao phan loai chi tiet:\")\npdf.set_font('Courier', '', 8)\npdf.chapter_body(report)\npdf.add_image_section(\"Ma tran nham lan:\", cm_plot_path)\n\npdf.add_page()\npdf.chapter_title(\"4. Phan tich Grad-CAM\")\nfor class_name in target_names_trained:\n    pdf.chapter_body(f\"Lop: {class_name}\")\n    correct_imgs = sorted(glob.glob(os.path.join(gradcam_path, f\"correct_{class_name}_*_{run_timestamp}.png\")))\n    incorrect_imgs = sorted(glob.glob(os.path.join(gradcam_path, f\"incorrect_{class_name}_*_{run_timestamp}.png\")))\n    \n    x_pos, y_pos = pdf.get_x(), pdf.get_y()\n    for i, img_path in enumerate(correct_imgs[:3]):\n        if os.path.exists(img_path): pdf.image(img_path, x=x_pos + i * 60, y=y_pos, w=55)\n    if correct_imgs: y_pos += 45\n    for i, img_path in enumerate(incorrect_imgs[:3]):\n        if os.path.exists(img_path): pdf.image(img_path, x=x_pos + i * 60, y=y_pos, w=55)\n    if incorrect_imgs: y_pos += 45\n    pdf.set_y(y_pos)\n    \n    avg_correct_path = os.path.join(gradcam_path, f\"avg_correct_{class_name}_{run_timestamp}.png\")\n    avg_incorrect_path = os.path.join(gradcam_path, f\"avg_incorrect_{class_name}_{run_timestamp}.png\")\n    if os.path.exists(avg_correct_path):\n        pdf.image(avg_correct_path, w=80)\n    if os.path.exists(avg_incorrect_path):\n        pdf.image(avg_incorrect_path, w=80)\n    pdf.ln(10)\n\nreport_filename = f\"report_{MODEL_ID}_{run_timestamp}.pdf\"\nreport_filepath = os.path.join(KAGGLE_OUTPUT_PATH, report_filename)\npdf.output(report_filepath)\nprint(f\"Đã tạo báo cáo PDF tại: {report_filepath}\")\n\nprint(\"\\nBắt đầu quá trình tải kết quả lên Google Drive...\")\ndrive = authenticate_gdrive()\nupload_folder_to_drive(drive, KAGGLE_OUTPUT_PATH, DRIVE_RESULTS_FOLDER_ID)\nprint(\"Hoàn tất! Toàn bộ kết quả đã được lưu về Google Drive.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T03:12:49.859531Z","iopub.status.idle":"2025-08-29T03:12:49.860044Z","shell.execute_reply.started":"2025-08-29T03:12:49.859695Z","shell.execute_reply":"2025-08-29T03:12:49.859710Z"}},"outputs":[],"execution_count":null}]}