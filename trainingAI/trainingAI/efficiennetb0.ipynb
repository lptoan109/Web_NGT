{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":12870874,"sourceType":"datasetVersion","datasetId":8141736}],"dockerImageVersionId":31091,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 0.1. Cài đặt các thư viện cần thiết\n!pip install -q fpdf2 noisereduce librosa tensorflow scikit-learn matplotlib seaborn pytz PyDrive2 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T15:49:58.354816Z","iopub.execute_input":"2025-09-03T15:49:58.355223Z","iopub.status.idle":"2025-09-03T15:50:15.699504Z","shell.execute_reply.started":"2025-09-03T15:49:58.355197Z","shell.execute_reply":"2025-09-03T15:50:15.694659Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# CÀI ĐẶT & CẤU HÌNH (SETUP & CONFIGURATION)\n# 0.2. Import thư viện\nimport os\nimport glob \nimport random\nimport datetime\nimport pytz\nimport shutil\nimport joblib\nimport zipfile\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nfrom fpdf import FPDF\nfrom tqdm import tqdm\nimport librosa\nimport noisereduce as nr\nimport tensorflow as tf\nfrom tensorflow.keras import mixed_precision\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D, AveragePooling2D\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.utils import class_weight\nfrom pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\nfrom kaggle_secrets import UserSecretsClient\nfrom oauth2client.service_account import ServiceAccountCredentials\nfrom tensorflow.keras.regularizers import l2\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.metrics import roc_curve, auc as sklearn_auc\nfrom sklearn.preprocessing import label_binarize\nfrom itertools import cycle\nmixed_precision.set_global_policy('mixed_bfloat16')\n\ntry:\n    # Cố gắng kết nối với TPU bằng cách chỉ định tpu='local'\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\n    print('Đã tìm thấy TPU Resolver.')\n    \n    # Kết nối và khởi tạo hệ thống TPU\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    print('Đã khởi tạo hệ thống TPU.')\n\n    # Tạo strategy\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('THÀNH CÔNG: Đã tạo TPUStrategy!')\n    print(f'Số lượng nhân (replicas): {strategy.num_replicas_in_sync}')\n    \nexcept (ValueError, RuntimeError) as e:\n    # Nếu vẫn không tìm thấy TPU, tự động chuyển về chiến lược mặc định\n    print(f'Lỗi kết nối TPU: {e}')\n    print('Không tìm thấy TPU. Sử dụng chiến lược mặc định cho GPU/CPU.')\n    strategy = tf.distribute.get_strategy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T15:50:15.701815Z","iopub.execute_input":"2025-09-03T15:50:15.702059Z","iopub.status.idle":"2025-09-03T15:51:12.180401Z","shell.execute_reply.started":"2025-09-03T15:50:15.702036Z","shell.execute_reply":"2025-09-03T15:51:12.176188Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1756914650.644036      10 common_lib.cc:612] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:230\n","output_type":"stream"},{"name":"stdout","text":"Đã tìm thấy TPU Resolver.\nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1756914667.876107      10 service.cc:148] XLA service 0x5d0b6608cbc0 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1756914667.876147      10 service.cc:156]   StreamExecutor device (0): TPU, 2a886c8\nI0000 00:00:1756914667.876151      10 service.cc:156]   StreamExecutor device (1): TPU, 2a886c8\nI0000 00:00:1756914667.876154      10 service.cc:156]   StreamExecutor device (2): TPU, 2a886c8\nI0000 00:00:1756914667.876156      10 service.cc:156]   StreamExecutor device (3): TPU, 2a886c8\nI0000 00:00:1756914667.876160      10 service.cc:156]   StreamExecutor device (4): TPU, 2a886c8\nI0000 00:00:1756914667.876162      10 service.cc:156]   StreamExecutor device (5): TPU, 2a886c8\nI0000 00:00:1756914667.876165      10 service.cc:156]   StreamExecutor device (6): TPU, 2a886c8\nI0000 00:00:1756914667.876167      10 service.cc:156]   StreamExecutor device (7): TPU, 2a886c8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Finished initializing TPU system.\nĐã khởi tạo hệ thống TPU.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\nTHÀNH CÔNG: Đã tạo TPUStrategy!\nSố lượng nhân (replicas): 8\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# THIẾT LẬP CẤU HÌNH \n# --- Các cấu hình cơ bản ---\nSEED = 42\ndef set_seed(seed_value):\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    #os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    tf.random.set_seed(seed_value)\nset_seed(SEED)\n\nKAGGLE_PROCESSED_DATA_PATH = \"/kaggle/input/ngt-spectrogram-id/\"\nKAGGLE_OUTPUT_PATH = \"/kaggle/working/output_results\"\nos.makedirs(KAGGLE_OUTPUT_PATH, exist_ok=True)\n\nCLASSES_TO_TRAIN = ['covid', 'asthma', 'healthy', 'tuberculosis']\nALL_CLASSES = ['healthy', 'asthma', 'covid', 'tuberculosis']\nN_SPLITS = 5\nTEST_SPLIT_RATIO = 0.15\nUSE_DATA_AUGMENTATION = True # Bật/tắt augmentation ở đây\nUSE_FOCAL_LOSS = True\nUSE_COSINE_DECAY_RESTARTS = True \n\nMODEL_ID = f'EfficienetB0_CV_TPU'\nMIN_DELTA = 1e-4\nSHUFFLE_BUFFER_SIZE = 2048 \nGAMMA = 2.0 # Giữ nguyên giá trị tiêu chuẩn\n\n# --- CÁC THAY ĐỔI CHÍNH ---\nLEARNING_RATE = 5e-6              # Thay đổi: Giảm LR để ổn định hơn\nWEIGHT_DECAY = 3e-3               # Thay đổi: Tăng để chống overfitting mạnh hơn\n\n# Cấu hình cho Cross-Validation\nTOTAL_EPOCHS = 200                # Giữ nguyên tổng số epochs tối đa\nWARMUP_EPOCHS = 3                 # Thay đổi: Rút ngắn Warmup cho hiệu quả\nRESTART_CYCLE_1_EPOCHS = 50       # Thay đổi: Chu kỳ ngắn hơn để khám phá nhiều hơn\nPATIENCE_EPOCHS = RESTART_CYCLE_1_EPOCHS + 25 # Thay đổi: Patience = 50\n\n# --- ĐỊNH NGHĨA BATCH SIZE ---\n# BATCH_SIZE này là batch size cho mỗi nhân TPU (per-replica)\nBATCH_SIZE = 32\n# Tính toán GLOBAL_BATCH_SIZE để dùng trong pipeline\n# Biến 'strategy' được lấy từ ô code đầu tiên\nGLOBAL_BATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync\nprint(f\"Batch size mỗi nhân: {BATCH_SIZE}\")\nprint(f\"Global batch size (tổng cộng): {GLOBAL_BATCH_SIZE}\")\n\n# INPUT_SHAPE sẽ được cập nhật lại ở ô chuẩn bị dữ liệu\nINPUT_SHAPE = (224, 224, 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T15:51:12.182486Z","iopub.execute_input":"2025-09-03T15:51:12.182708Z","iopub.status.idle":"2025-09-03T15:51:12.196083Z","shell.execute_reply.started":"2025-09-03T15:51:12.182685Z","shell.execute_reply":"2025-09-03T15:51:12.190589Z"}},"outputs":[{"name":"stdout","text":"Batch size mỗi nhân: 32\nGlobal batch size (tổng cộng): 256\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# KHỞI TẠO CÁC HÀM CẦN THIẾT (PHIÊN BẢN ĐÚNG)\n\ndef get_patient_id(filepath, class_name):\n    filename = os.path.basename(filepath)\n    if class_name.lower() in ['asthma', 'covid', 'healthy']:\n        return filename.split('_')[0]\n    elif class_name.lower() == 'tuberculosis':\n        return '_'.join(filename.split('_')[:-1]).replace('.npy', '')\n    else:\n        return filename.split('_')[0]\n\n@register_keras_serializable()\nclass MacroF1Score(tf.keras.metrics.Metric):\n    \"\"\"\n    Lớp metric để tính toán Macro F1-Score một cách chính xác trên toàn bộ epoch.\n    \"\"\"\n    def __init__(self, num_classes, name='f1_macro', **kwargs):\n        super(MacroF1Score, self).__init__(name=name, **kwargs)\n        self.num_classes = num_classes\n        self.true_positives = self.add_weight(name='tp', shape=(num_classes,), initializer='zeros')\n        self.false_positives = self.add_weight(name='fp', shape=(num_classes,), initializer='zeros')\n        self.false_negatives = self.add_weight(name='fn', shape=(num_classes,), initializer='zeros')\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_pred_labels = tf.argmax(tf.nn.softmax(y_pred), axis=1)\n        y_true_labels = tf.argmax(y_true, axis=1)\n        cm = tf.math.confusion_matrix(y_true_labels, y_pred_labels, num_classes=self.num_classes, dtype=tf.float32)\n        tp = tf.linalg.diag_part(cm)\n        fp = tf.reduce_sum(cm, axis=0) - tp\n        fn = tf.reduce_sum(cm, axis=1) - tp\n        self.true_positives.assign_add(tp)\n        self.false_positives.assign_add(fp)\n        self.false_negatives.assign_add(fn)\n\n    def result(self):\n        precision = self.true_positives / (self.true_positives + self.false_positives + tf.keras.backend.epsilon())\n        recall = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n        macro_f1 = tf.reduce_mean(f1)\n        return macro_f1\n\n    def reset_state(self):\n        self.true_positives.assign(tf.zeros(self.num_classes))\n        self.false_positives.assign(tf.zeros(self.num_classes))\n        self.false_negatives.assign(tf.zeros(self.num_classes))\n\n    # Thêm phương thức get_config\n    def get_config(self):\n        config = super(MacroF1Score, self).get_config()\n        config.update({'num_classes': self.num_classes})\n        return config\n\n    def reset_state(self):\n        # Reset các biến trạng thái về 0 ở đầu mỗi epoch\n        self.true_positives.assign(tf.zeros(self.num_classes))\n        self.false_positives.assign(tf.zeros(self.num_classes))\n        self.false_negatives.assign(tf.zeros(self.num_classes))\ndef parse_tfrecord_fn(example):\n    \"\"\"Hàm đọc và xử lý một mẫu từ file TFRecord cho EfficientNet.\"\"\"\n    feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([], tf.int64),\n    }\n    example = tf.io.parse_single_example(example, feature_description)\n    image = tf.io.parse_tensor(example['image'], out_type=tf.float32)\n    image = tf.reshape(image, (256, 126))\n    \n    # --- THAY ĐỔI QUAN TRỌNG ---\n    # 1. Stack 3 kênh TRƯỚC KHI resize để tạo thành ảnh 3D\n    image_3d = tf.stack([image, image, image], axis=-1)\n    \n    # 2. Bây giờ mới resize ảnh 3D này về kích thước mong muốn\n    image_resized = tf.image.resize(image_3d, [224, 224])\n    \n    # 3. Sử dụng hàm tiền xử lý chuyên dụng của EfficientNet\n    image_preprocessed = preprocess_input(image_resized)\n    \n    label_encoded = tf.cast(example['label'], tf.int32)\n    label_onehot = tf.one_hot(label_encoded, depth=len(ALL_CLASSES))\n    \n    return image_preprocessed, label_onehot\n\ndef augment(spectrogram, label):\n    spectrogram = spec_augment(spectrogram)\n    return spectrogram, label\n\ndef focal_loss_from_logits_optimized(alpha, gamma=2.0):\n    \"\"\"\n    Tạo ra hàm Focal Loss phiên bản đầy đủ và sạch sẽ.\n    \n    Args:\n        alpha: Một list hoặc array chứa trọng số cho mỗi lớp.\n        gamma: Hệ số tập trung, mặc định là 2.0.\n    \"\"\"\n    # Chuyển alpha sang dạng tensor để tính toán\n    alpha = tf.constant(alpha, dtype=tf.float32)\n\n    def focal_loss_fixed(y_true, y_pred):\n        y_true = tf.cast(y_true, 'float32')\n        y_pred = tf.cast(y_pred, 'float32')\n        \n        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n        probs = tf.nn.softmax(y_pred)\n        pt = tf.reduce_sum(y_true * probs, axis=-1)\n        focal_term = (1.0 - pt) ** gamma\n        alpha_t = tf.reduce_sum(y_true * alpha, axis=-1)\n        loss = alpha_t * focal_term * cross_entropy\n        \n        return tf.reduce_mean(loss)\n        \n    return focal_loss_fixed\n\ndef spec_augment(spectrogram, time_masking_para=50, frequency_masking_para=40, num_time_masks=1, num_freq_masks=1):\n    spectrogram_aug = spectrogram\n    freq_bins = tf.shape(spectrogram)[1] # Sửa: Lấy chiều tần số từ shape 4D\n    time_steps = tf.shape(spectrogram)[2] # Sửa: Lấy chiều thời gian từ shape 4D\n    \n    for _ in range(num_freq_masks):\n        f = tf.random.uniform(shape=(), minval=0, maxval=frequency_masking_para, dtype=tf.int32)\n        f0 = tf.random.uniform(shape=(), minval=0, maxval=freq_bins - f, dtype=tf.int32)\n        freq_mask_1d = tf.concat([tf.ones((f0,), dtype=spectrogram.dtype), tf.zeros((f,), dtype=spectrogram.dtype), tf.ones((freq_bins - f0 - f,), dtype=spectrogram.dtype)], axis=0)\n        freq_mask_4d = tf.reshape(freq_mask_1d, (1, freq_bins, 1, 1)) # Sửa: Reshape thành 4D để broadcast\n        spectrogram_aug *= freq_mask_4d\n        \n    for _ in range(num_time_masks):\n        t = tf.random.uniform(shape=(), minval=0, maxval=time_masking_para, dtype=tf.int32)\n        t0 = tf.random.uniform(shape=(), minval=0, maxval=time_steps - t, dtype=tf.int32)\n        time_mask_1d = tf.concat([tf.ones((t0,), dtype=spectrogram.dtype), tf.zeros((t,), dtype=spectrogram.dtype), tf.ones((time_steps - t0 - t,), dtype=spectrogram.dtype)], axis=0)\n        time_mask_4d = tf.reshape(time_mask_1d, (1, 1, time_steps, 1)) # Sửa: Reshape thành 4D để broadcast\n        spectrogram_aug *= time_mask_4d\n        \n    return spectrogram_aug\n\n# HÀM CREATE_MODEL PHIÊN BẢN ĐÚNG VÀ ĐƠN GIẢN\n@register_keras_serializable()\nclass FinalModel(tf.keras.Model):\n    def __init__(self, input_shape, num_classes, **kwargs):\n        super(FinalModel, self).__init__(**kwargs)\n        self.input_shape_config = input_shape\n        self.num_classes_config = num_classes\n        \n        self.base_model = EfficientNetB0(\n            weights='imagenet',\n            include_top=False,\n            input_shape=self.input_shape_config\n        )\n        \n        self.pooling = GlobalAveragePooling2D()\n        self.dense1 = Dense(256, activation='relu', kernel_regularizer=l2(0.001)) # Lớp mới\n        self.dropout = Dropout(0.5)\n        self.dense_output = Dense(num_classes, activation='linear', dtype='float32')\n\n    def call(self, inputs, training=None):\n        x = self.pooling(x)\n        x = self.dense1(x) # Thêm vào\n        x = self.dropout(x, training=training)\n        outputs = self.dense_output(x)\n        return outputs\n\n    # Thêm phương thức get_config\n    def get_config(self):\n        # Lấy config của lớp cha\n        config = super(FinalModel, self).get_config()\n        # Cập nhật với các tham số của lớp con\n        config.update({\n            'input_shape': self.input_shape_config,\n            'num_classes': self.num_classes_config\n        })\n        return config\n\ndef load_data_from_df(df):\n    X, y = [], []\n    for _, row in df.iterrows():\n        X.append(np.load(row['filepath']))\n        y.append(row['label'])\n    return np.array(X), np.array(y)\n\ndef get_grad_cam_final(model, img_array, last_conv_layer_name, pred_index=None):\n    \"\"\"\n    Tạo Grad-CAM cho một subclassed model.\n    Lưu ý: Model phải được build (chạy qua dữ liệu một lần) trước khi gọi hàm này.\n    \"\"\"\n    # Tạo một model trung gian với input là input của model chính,\n    # và output là lớp conv cuối và output cuối cùng của model chính.\n    grad_model = Model(\n        inputs=model.inputs,\n        outputs=[model.base_model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Tính toán gradient\n    with tf.GradientTape() as tape:\n        # Đưa ảnh vào grad_model để lấy 2 output đã định nghĩa ở trên\n        last_conv_layer_output, preds = grad_model(img_array)\n        \n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # Lấy gradient của lớp được dự đoán đối với feature map của lớp conv cuối\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # Tính trung bình gradient và tạo heatmap\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    \n    heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + tf.keras.backend.epsilon())\n    \n    return heatmap.numpy()\n\ndef overlay_grad_cam(spec, heatmap, alpha=0.6):\n    heatmap_resized = tf.image.resize(heatmap[..., np.newaxis], (spec.shape[0], spec.shape[1]))\n    heatmap_resized = np.uint8(255 * heatmap_resized)\n    jet = plt.cm.get_cmap(\"jet\")\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap_resized.squeeze()]\n    spec_display = np.stack([spec]*3, axis=-1)\n    spec_display = (spec_display - spec_display.min()) / (spec_display.max() - spec_display.min())\n    superimposed_img = jet_heatmap * alpha + spec_display\n    superimposed_img = np.clip(superimposed_img, 0, 1)\n    return superimposed_img\n\nclass PDFReport(FPDF):\n    def header(self):\n        self.set_font('Arial', 'B', 12)\n        self.cell(0, 10, 'BAO CAO KET QUA HUAN LUYEN MO HINH AI', 0, 1, 'C')\n        self.ln(10)\n    def footer(self):\n        self.set_y(-15)\n        self.set_font('Arial', 'I', 8)\n        self.cell(0, 10, f'Trang {self.page_no()}', 0, 0, 'C')\n    def chapter_title(self, title):\n        self.set_font('Arial', 'B', 12)\n        self.cell(0, 10, title, 0, 1, 'L')\n        self.ln(5)\n    def chapter_body(self, content):\n        self.set_font('Arial', '', 10)\n        safe_content = content.encode('latin-1', 'replace').decode('latin-1')\n        self.multi_cell(0, 5, safe_content)\n        self.ln()\n    def add_image_section(self, title, img_path):\n        self.chapter_title(title)\n        if os.path.exists(img_path):\n            self.image(img_path, x=None, y=None, w=180)\n            self.ln(5)\n        else:\n            self.chapter_body(f\"Khong tim thay hinh anh: {img_path}\")\n\ndef authenticate_gdrive():\n    user_secrets = UserSecretsClient()\n    secret_value = user_secrets.get_secret(\"google_service_account_key\")\n    with open(\"service_account.json\", \"w\") as f:\n        f.write(secret_value)\n    scope = [\"https://www.googleapis.com/auth/drive\"]\n    gauth = GoogleAuth()\n    gauth.credentials = ServiceAccountCredentials.from_json_keyfile_name(\"service_account.json\", scope)\n    drive = GoogleDrive(gauth)\n    return drive\n\ndef upload_folder_to_drive(drive, folder_path, parent_folder_id):\n    folder_name = os.path.basename(folder_path)\n    print(f\"Đang tạo thư mục '{folder_name}' trên Google Drive...\")\n    folder_metadata = {'title': folder_name, 'mimeType': 'application/vnd.google-apps.folder', 'parents': [{'id': parent_folder_id}]}\n    folder = drive.CreateFile(folder_metadata)\n    folder.Upload()\n    \n    print(f\"Bắt đầu tải nội dung của '{folder_name}'...\")\n    for item in tqdm(os.listdir(folder_path), desc=f\"Uploading {folder_name}\"):\n        item_path = os.path.join(folder_path, item)\n        if os.path.isfile(item_path):\n            gfile = drive.CreateFile({'title': item, 'parents': [{'id': folder['id']}]})\n            gfile.SetContentFile(item_path)\n            gfile.Upload(param={'supportsTeamDrives': True})\n        elif os.path.isdir(item_path):\n            upload_folder_to_drive(drive, item_path, folder['id'])\n\ndef _bytes_feature(value):\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))):\n        value = value.numpy()\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _int64_feature(value):\n    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef serialize_example(image, label):\n    \"\"\"Creates a tf.train.Example message ready to be written to a file.\"\"\"\n    feature = {\n        'image': _bytes_feature(tf.io.serialize_tensor(image)),\n        'label': _int64_feature(label)\n    }\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n    return example_proto.SerializeToString()\nclass LearningRateLogger(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs=None):\n        current_lr = tf.keras.backend.get_value(self.model.optimizer.learning_rate)\n        print(f\"\\nEpoch {epoch+1}: Learning Rate is {current_lr:.2e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T11:51:44.158782Z","iopub.execute_input":"2025-09-03T11:51:44.159031Z","iopub.status.idle":"2025-09-03T11:51:44.206982Z","shell.execute_reply.started":"2025-09-03T11:51:44.159007Z","shell.execute_reply":"2025-09-03T11:51:44.201381Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# CHUẨN BỊ DỮ LIỆU VÀ TẠO TFRECORD\nsuspicious_files_to_remove = [\n    '/kaggle/input/ngt-spectrogram-id/healthy/P0030101_123370_Dkwg3F7jMGaR7kbc-seg2.npy',\n    '/kaggle/input/ngt-spectrogram-id/covid/P0032202_15897_PcbyJQWemBfghUYp-seg2.npy',\n    '/kaggle/input/ngt-spectrogram-id/covid/P0027142_5701_hupBI5CxKMNCfe8b-seg1.npy',\n    '/kaggle/input/ngt-spectrogram-id/covid/P0056214_89533_0WsmNRSKuQFGodg1-seg1.npy',\n]\n# --- BƯỚC 1: TẢI VÀ PHÂN CHIA DỮ LIỆU BAN ĐẦU ---\nprint(\"Bắt đầu chuẩn bị và phân chia dữ liệu...\")\nall_files_to_split = []\nfor class_name in ALL_CLASSES:\n    source_dir = os.path.join(KAGGLE_PROCESSED_DATA_PATH, class_name)\n    if os.path.exists(source_dir):\n        files = glob.glob(os.path.join(source_dir, '*.npy'))\n        for f in files:\n            all_files_to_split.append({'filepath': f, 'label': class_name})\n\nall_data_df = pd.DataFrame(all_files_to_split)\nall_data_df['patient_id'] = all_data_df.apply(lambda row: get_patient_id(row['filepath'], row['label']), axis=1)\n\nprint(f\"Số lượng mẫu ban đầu: {len(all_data_df)}\")\nall_data_df = all_data_df[~all_data_df['filepath'].isin(suspicious_files_to_remove)].reset_index(drop=True)\nprint(f\"Số lượng mẫu sau khi lọc bỏ file 'im lặng': {len(all_data_df)}\")\n\nprint(\"Tách tập Test cuối cùng (Hold-out set)...\")\npatient_ids = all_data_df['patient_id'].unique()\nnp.random.shuffle(patient_ids)\ntest_patient_count = int(len(patient_ids) * TEST_SPLIT_RATIO)\ntest_patients = patient_ids[:test_patient_count]\ntrain_val_patients = patient_ids[test_patient_count:]\n\ntest_df = all_data_df[all_data_df['patient_id'].isin(test_patients)].reset_index(drop=True)\ntrain_val_df = all_data_df[all_data_df['patient_id'].isin(train_val_patients)].reset_index(drop=True)\n\nprint(f\"Đã tách: {len(train_val_df)} mẫu cho Train/Validation (CV) và {len(test_df)} mẫu cho Test cuối cùng.\")\n\n# --- BƯỚC 2: KHỞI TẠO LABEL ENCODER VÀ STANDARD SCALER ---\nle = LabelEncoder().fit(ALL_CLASSES)\n# Cập nhật INPUT_SHAPE từ một file mẫu\nsample_spec = np.load(train_val_df['filepath'][0])\nprint(f\"Kích thước input được cập nhật: {INPUT_SHAPE}\")\n\n\n# --- BƯỚC 3: CHUYỂN ĐỔI DỮ LIỆU SANG ĐỊNH DẠNG TFRECORD ---\nTFRECORD_OUTPUT_PATH = \"/kaggle/working/tfrecords\"\nos.makedirs(TFRECORD_OUTPUT_PATH, exist_ok=True)\nprint(f\"Bắt đầu chuyển đổi dữ liệu sang TFRecord tại: {TFRECORD_OUTPUT_PATH}\")\n\nall_dfs = {'train_val': train_val_df, 'test': test_df}\n\nfor df_name, df in all_dfs.items():\n    print(f\"--- Đang xử lý tập {df_name} ---\")\n    tfrecord_path = os.path.join(TFRECORD_OUTPUT_PATH, f\"{df_name}.tfrec\")\n    \n    with tf.io.TFRecordWriter(tfrecord_path) as writer:\n        # Sử dụng tqdm tiêu chuẩn để tránh lỗi ImportError\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Creating {df_name}.tfrec\"):\n            spectrogram = np.load(row['filepath']).astype(np.float32)\n            label_encoded = le.transform([row['label']])[0]\n            \n            example = serialize_example(spectrogram, label_encoded)\n            writer.write(example)\n            \nprint(\"\\nChuyển đổi sang TFRecord hoàn tất!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T11:51:44.208671Z","iopub.execute_input":"2025-09-03T11:51:44.208956Z","iopub.status.idle":"2025-09-03T11:57:43.844758Z","shell.execute_reply.started":"2025-09-03T11:51:44.208925Z","shell.execute_reply":"2025-09-03T11:57:43.838607Z"}},"outputs":[{"name":"stdout","text":"Bắt đầu chuẩn bị và phân chia dữ liệu...\nSố lượng mẫu ban đầu: 33084\nSố lượng mẫu sau khi lọc bỏ file 'im lặng': 33080\nTách tập Test cuối cùng (Hold-out set)...\nĐã tách: 28060 mẫu cho Train/Validation (CV) và 5020 mẫu cho Test cuối cùng.\nKích thước input được cập nhật: (224, 224, 3)\nBắt đầu chuyển đổi dữ liệu sang TFRecord tại: /kaggle/working/tfrecords\n--- Đang xử lý tập train_val ---\n","output_type":"stream"},{"name":"stderr","text":"Creating train_val.tfrec: 100%|██████████| 28060/28060 [04:58<00:00, 93.92it/s] \n","output_type":"stream"},{"name":"stdout","text":"--- Đang xử lý tập test ---\n","output_type":"stream"},{"name":"stderr","text":"Creating test.tfrec: 100%|██████████| 5020/5020 [00:57<00:00, 87.27it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nChuyển đổi sang TFRecord hoàn tất!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# HUẤN LUYỆN MÔ HÌNH Cross-Validation\n\n# --- BƯỚC 1: KHỞI TẠO CÁC BIẾN CẦN THIẾT ---\nprint(\"Đang khởi tạo các biến cho Cross-Validation...\")\nAUTOTUNE = tf.data.AUTOTUNE\nskf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\ncv_data_to_split = train_val_df[train_val_df['label'].isin(CLASSES_TO_TRAIN)]\nX_cv_paths = cv_data_to_split['filepath'].values\ny_cv_labels = le.transform(cv_data_to_split['label'])\ngroups_cv = cv_data_to_split['patient_id'].values\n\nfold_accuracies, fold_losses, fold_aucs, fold_f1s = [], [], [], []\n\nprint(\"Đang tính toán trọng số alpha cho Focal Loss...\")\nclass_weights_array = class_weight.compute_class_weight('balanced', classes=np.unique(y_cv_labels), y=y_cv_labels)\nalpha_weights_list = class_weights_array.tolist()\nprint(\"Trọng số Alpha được tính toán:\")\nfor i, w in enumerate(alpha_weights_list):\n    class_name = le.inverse_transform([i])[0]\n    print(f\"- Lớp '{class_name}': {w:.2f}\")\n\n# --- BƯỚC 2: BẮT ĐẦU VÒNG LẶP CROSS-VALIDATION ---\nLOCAL_TFRECORD_PATH = TFRECORD_OUTPUT_PATH\nTRAIN_VAL_TFREC = os.path.join(LOCAL_TFRECORD_PATH, 'train_val.tfrec')\nprint(f\"Sẵn sàng đọc dữ liệu TFRecord từ: {TRAIN_VAL_TFREC}\")\n\nfor fold, (train_indices, val_indices) in enumerate(skf.split(X_cv_paths, y_cv_labels, groups_cv)):\n    fold_number = fold + 1\n    print(\"-\" * 50 + f\"\\nBắt đầu Fold {fold_number}/{N_SPLITS}\\n\" + \"-\" * 50)\n    # TÍNH TOÁN CÁC BƯỚC CHO SCHEDULER\n    # 1. Tạo pipeline dữ liệu trước\n    train_indices_tf = tf.constant(train_indices, dtype=tf.int64)\n    val_indices_tf = tf.constant(val_indices, dtype=tf.int64)\n    full_ds = tf.data.TFRecordDataset(TRAIN_VAL_TFREC).enumerate()\n    train_ds = full_ds.filter(lambda i, data: tf.reduce_any(i == train_indices_tf)).map(lambda i, data: data)\n    val_ds = full_ds.filter(lambda i, data: tf.reduce_any(i == val_indices_tf)).map(lambda i, data: data)\n    train_ds = train_ds.map(parse_tfrecord_fn, num_parallel_calls=AUTOTUNE)\n    val_ds = val_ds.map(parse_tfrecord_fn, num_parallel_calls=AUTOTUNE)\n    train_ds = train_ds.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE).repeat().batch(GLOBAL_BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n    val_ds = val_ds.cache().batch(GLOBAL_BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n    \n    # 2. Bây giờ mới tính toán steps_per_epoch\n    steps_per_epoch = len(train_indices) // GLOBAL_BATCH_SIZE\n    validation_steps = len(val_indices) // GLOBAL_BATCH_SIZE\n    print(f\"Số bước mỗi epoch: {steps_per_epoch} | Số bước validation: {validation_steps}\")\n\n\n    # --- TẠO MODEL VÀ LOSS FUNCTION ---\n    with strategy.scope():\n        model = FinalModel(input_shape=INPUT_SHAPE, num_classes=len(ALL_CLASSES))\n        if USE_FOCAL_LOSS:\n            loss_function = focal_loss_from_logits_optimized(alpha=alpha_weights_list, gamma=GAMMA)\n        else:\n            loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n\n    # --- GIAI ĐOẠN 1: HUẤN LUYỆN CÁC LỚP CUỐI (HEAD TRAINING) ---\n    print(\"\\n--- Bắt đầu Giai đoạn 1: Huấn luyện các lớp cuối ---\")\n    with strategy.scope():\n        # Đóng băng toàn bộ mô hình nền\n        model.base_model.trainable = False\n        # Compile với learning rate lớn hơn (ví dụ 1e-3)\n        optimizer_head = tf.keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=WEIGHT_DECAY)\n        model.compile(optimizer=optimizer_head, loss=loss_function, metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n        # Huấn luyện trong 5 epochs\n        model.fit(train_ds, validation_data=val_ds, epochs=5,\n              steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, verbose=1)\n\n    # --- GIAI ĐOẠN 2: WARMUP VÀ HUẤN LUYỆN CHÍNH VỚI RESTART ---\n    print(\"\\n--- Bắt đầu Giai đoạn 2: Fine-tuning ---\")\n\n    # 1. THIẾT LẬP CÁC SIÊU THAM SỐ MỚI CHO LỊCH TRÌNH\n    with strategy.scope():\n        # Đếm tổng số lớp trong mô hình nền (EfficientNetB0)\n        num_base_layers = len(model.base_model.layers)\n        # Tính toán chỉ số của lớp bắt đầu mở băng (từ giữa mô hình trở đi)\n        fine_tune_at = int(num_base_layers * 0.5) # 50%\n        # Đầu tiên, đặt toàn bộ mô hình nền là không thể huấn luyện\n        model.base_model.trainable = False\n        # Bây giờ, mở băng các lớp từ chỉ số `fine_tune_at` đến cuối\n        for layer in model.base_model.layers[fine_tune_at:]:\n            layer.trainable = True\n    \n        print(f\"Tổng số lớp trong base model: {num_base_layers}\")\n        print(f\"Sẽ mở băng và huấn luyện {num_base_layers - fine_tune_at} lớp cuối cùng (bắt đầu từ lớp {fine_tune_at}).\")\n\n        f1_macro = MacroF1Score(num_classes=len(ALL_CLASSES), name='f1_macro')\n\n        # 2. GIAI ĐOẠN 2A: WARMUP\n        print(f\"\\n--- Giai đoạn 2A: Bắt đầu Warmup trong {WARMUP_EPOCHS} epochs ---\")\n        warmup_lr = LEARNING_RATE / 10 # Bắt đầu với LR rất thấp\n        optimizer_warmup = tf.keras.optimizers.AdamW(learning_rate=warmup_lr, weight_decay=WEIGHT_DECAY)\n        model.compile(optimizer=optimizer_warmup, loss=loss_function, metrics=['accuracy', f1_macro])\n\n        model.fit(\n            train_ds, validation_data=val_ds, epochs=WARMUP_EPOCHS,\n            steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, verbose=1\n        )\n\n        # 3. GIAI ĐOẠN 2B: HUẤN LUYỆN CHÍNH VỚI COSINE DECAY RESTARTS\n        print(f\"\\n--- Giai đoạn 2B: Bắt đầu huấn luyện chính ---\")\n\n        # Sử dụng công tắc để chọn optimizer và callbacks\n        if USE_COSINE_DECAY_RESTARTS:\n            print(\"Sử dụng scheduler: CosineDecayRestarts\")\n            first_decay_steps = RESTART_CYCLE_1_EPOCHS * steps_per_epoch\n            lr_scheduler = tf.keras.optimizers.schedules.CosineDecayRestarts(\n                initial_learning_rate=LEARNING_RATE,\n                first_decay_steps=first_decay_steps,\n                t_mul=2.0, \n                m_mul=0.9  # Giảm biên độ restart để ổn định hơn\n            )\n            optimizer_finetune = tf.keras.optimizers.AdamW(learning_rate=lr_scheduler, weight_decay=WEIGHT_DECAY)\n            \n            # Callbacks cho CosineDecayRestarts\n            callbacks = [\n                EarlyStopping(\n                    monitor='val_f1_macro', mode='max', patience=PATIENCE_EPOCHS,\n                    restore_best_weights=True, min_delta=MIN_DELTA, verbose=1\n                ),\n                LearningRateLogger()\n            ]\n        else:\n            print(\"Sử dụng scheduler: ReduceLROnPlateau\")\n            # Optimizer với learning rate ban đầu cố định\n            optimizer_finetune = tf.keras.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n            \n            # Callbacks cho ReduceLROnPlateau\n            callbacks = [\n                EarlyStopping(\n                    monitor='val_f1_macro', mode='max', patience=30, # Có thể cần patience dài hơn\n                    restore_best_weights=True, min_delta=MIN_DELTA, verbose=1\n                ),\n                tf.keras.callbacks.ReduceLROnPlateau(\n                    monitor='val_f1_macro', mode='max', factor=0.2,\n                    patience=10, # Giảm LR nếu F1 không cải thiện trong 10 epochs\n                    min_lr=1e-7,\n                    verbose=1\n                ),\n                LearningRateLogger()\n            ]\n\n        # Biên dịch lại mô hình với optimizer và metrics cuối cùng\n        with strategy.scope():\n            model.compile(optimizer=optimizer_finetune, loss=loss_function, metrics=['accuracy', tf.keras.metrics.AUC(name='auc'), f1_macro])\n        \n        # Huấn luyện mô hình\n        history = model.fit(\n            train_ds, validation_data=val_ds, epochs=TOTAL_EPOCHS,\n            initial_epoch=WARMUP_EPOCHS,\n            steps_per_epoch=steps_per_epoch, validation_steps=validation_steps,\n            callbacks=callbacks, verbose=1\n        )\n\n    # Tạo đường dẫn và tên file để lưu model\n    model_save_path = os.path.join(KAGGLE_OUTPUT_PATH, f'{MODEL_ID}_fold_{fold_number}.keras')\n    # Lưu lại toàn bộ mô hình (kiến trúc + trọng số)\n    model.save(model_save_path)\n    # In ra thông báo để xác nhận\n    print(f\"Đã lưu mô hình cho Fold {fold_number} tại: {model_save_path}\")\n\n    # --- VẼ BIỂU ĐỒ VÀ ĐÁNH GIÁ ---\n    print(\"Đang tạo và lưu biểu đồ huấn luyện...\")\n    plt.figure(figsize=(18, 7))\n    plt.suptitle(f'Training Metrics for Fold {fold_number}', fontsize=16)\n    \n    # --- Biểu đồ cho các chỉ số (Accuracy, AUC, F1-Macro) ---\n    plt.subplot(1, 2, 1)\n    # Vẽ các chỉ số của tập Train\n    plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue', linestyle='-')\n    plt.plot(history.history['auc'], label='Training AUC', color='green', linestyle='-')\n    plt.plot(history.history['f1_macro'], label='Training F1-Macro', color='red', linestyle='-')\n    \n    # Vẽ các chỉ số của tập Validation\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='blue', linestyle='--')\n    plt.plot(history.history['val_auc'], label='Validation AUC', color='green', linestyle='--')\n    plt.plot(history.history['val_f1_macro'], label='Validation F1-Macro', color='red', linestyle='--')\n    \n    # Cập nhật lại tiêu đề và nhãn\n    plt.title('Biểu đồ các chỉ số (Accuracy, AUC, F1-Macro)')\n    plt.xlabel('Epoch')\n    plt.ylabel('Giá trị')\n    plt.legend(loc='lower right')\n    plt.grid(True)\n    \n    # --- Biểu đồ cho Loss ---\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Training Loss', color='orange')\n    plt.plot(history.history['val_loss'], label='Validation Loss', color='purple')\n    plt.title('Biểu đồ Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='upper right')\n    plt.grid(True)\n    \n    # Lưu và đóng hình ảnh\n    plot_filename = f'fold_{fold_number}_metrics.png'\n    plot_filepath = os.path.join(KAGGLE_OUTPUT_PATH, plot_filename)\n    plt.savefig(plot_filepath)\n    plt.close()\n    \n    print(f\"Đã lưu biểu đồ cho Fold {fold_number} tại: {plot_filepath}\")\n    \n    # THÊM 4: Sửa lại model.evaluate để nhận đủ 4 giá trị\n    loss, accuracy, auc, f1 = model.evaluate(val_ds, verbose=0)\n    print(f\"Fold {fold_number} - Validation Loss: {loss:.4f}, Validation Accuracy: {accuracy:.4f}, Validation AUC: {auc:.4f}, Validation F1-Macro: {f1:.4f}\")\n    \n    fold_aucs.append(auc)\n    fold_losses.append(loss)\n    fold_accuracies.append(accuracy)\n    fold_f1s.append(f1) # Thêm lưu trữ F1\n\n    # THÊM 5: Sửa lại câu lệnh print cuối cùng\n    print(\"=\" * 50 + \"\\nKết quả Cross-Validation:\\n\" \n      + f\"Validation Accuracy trung bình: {np.mean(fold_accuracies):.4f} +/- {np.std(fold_accuracies):.4f}\\n\"\n      + f\"Validation Loss trung bình: {np.mean(fold_losses):.4f} +/- {np.std(fold_losses):.4f}\\n\"\n      + f\"Validation AUC trung bình: {np.mean(fold_aucs):.4f} +/- {np.std(fold_aucs):.4f}\\n\"\n      + f\"Validation F1-Macro trung bình: {np.mean(fold_f1s):.4f} +/- {np.std(fold_f1s):.4f}\\n\" # Thêm dòng này\n      + \"=\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T11:57:43.847134Z","iopub.execute_input":"2025-09-03T11:57:43.847419Z","iopub.status.idle":"2025-09-03T15:22:17.122321Z","shell.execute_reply.started":"2025-09-03T11:57:43.847391Z","shell.execute_reply":"2025-09-03T15:22:17.115693Z"}},"outputs":[{"name":"stdout","text":"Đang khởi tạo các biến cho Cross-Validation...\nĐang tính toán trọng số alpha cho Focal Loss...\nTrọng số Alpha được tính toán:\n- Lớp 'asthma': 1.82\n- Lớp 'covid': 0.80\n- Lớp 'healthy': 0.87\n- Lớp 'tuberculosis': 0.96\nSẵn sàng đọc dữ liệu TFRecord từ: /kaggle/working/tfrecords/train_val.tfrec\n--------------------------------------------------\nBắt đầu Fold 1/5\n--------------------------------------------------\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fd0d45a7760> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd0d45a7760>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function <lambda> at 0x7fd0d45a7760> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd0d45a7760>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fd82090b760> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd82090b760>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function <lambda> at 0x7fd82090b760> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd82090b760>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fd82090b640> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd82090b640>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function <lambda> at 0x7fd82090b640> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd82090b640>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fd0d45a6710> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd0d45a6710>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform <function <lambda> at 0x7fd0d45a6710> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd0d45a6710>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nSố bước mỗi epoch: 87 | Số bước validation: 21\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756900668.298453      10 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n\n--- Bắt đầu Giai đoạn 1: Huấn luyện các lớp cuối ---\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756900699.170761      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:10934389281517148678\nI0000 00:00:1756900701.834812     947 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(2486306964948227037), session_name()\nI0000 00:00:1756900725.458849     947 tpu_compile_op_common.cc:245] Compilation of 2486306964948227037 with session name  took 23.623338785s and succeeded\nI0000 00:00:1756900725.529632     947 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(2486306964948227037), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_10934389281517148678\", property.function_library_fingerprint = 7262044275807281730, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756900725.529690     947 tpu_compilation_cache_interface.cc:542] After adding entry for key 2486306964948227037 with session_name  cache is 1 entries (76305773 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.6322 - auc: 0.7673 - loss: 6.8659","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756900740.424078      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:11039352951898252573\nI0000 00:00:1756900741.664727     929 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(2364892514853699563), session_name()\nI0000 00:00:1756900753.567244     929 tpu_compile_op_common.cc:245] Compilation of 2364892514853699563 with session name  took 11.9024604s and succeeded\nI0000 00:00:1756900753.583729     929 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(2364892514853699563), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_11039352951898252573\", property.function_library_fingerprint = 11943736688161709818, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756900753.583767     929 tpu_compilation_cache_interface.cc:542] After adding entry for key 2364892514853699563 with session_name  cache is 2 entries (102945333 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 363ms/step - accuracy: 0.6316 - auc: 0.7669 - loss: 6.8873 - val_accuracy: 0.2794 - val_auc: 0.5327 - val_loss: 22.8379\nEpoch 2/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.3120 - auc: 0.5410 - loss: 15.7070 - val_accuracy: 0.3912 - val_auc: 0.5986 - val_loss: 18.6935\nEpoch 3/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.3691 - auc: 0.5906 - loss: 12.8615 - val_accuracy: 0.4061 - val_auc: 0.6086 - val_loss: 18.3514\nEpoch 4/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 156ms/step - accuracy: 0.3874 - auc: 0.6180 - loss: 12.1497 - val_accuracy: 0.4111 - val_auc: 0.6122 - val_loss: 17.6716\nEpoch 5/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 152ms/step - accuracy: 0.3884 - auc: 0.6111 - loss: 12.1739 - val_accuracy: 0.4226 - val_auc: 0.6189 - val_loss: 17.7311\n\n--- Bắt đầu Giai đoạn 2: Fine-tuning ---\nTổng số lớp trong base model: 238\nSẽ mở băng và huấn luyện 119 lớp cuối cùng (bắt đầu từ lớp 119).\n\n--- Giai đoạn 2A: Bắt đầu Warmup trong 3 epochs ---\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756900833.262130      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:13394302486796388458\nI0000 00:00:1756900837.985692     926 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(3608692472238479403), session_name()\nI0000 00:00:1756900870.242154     926 tpu_compile_op_common.cc:245] Compilation of 3608692472238479403 with session name  took 32.256404396s and succeeded\nI0000 00:00:1756900870.356301     926 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(3608692472238479403), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_13394302486796388458\", property.function_library_fingerprint = 10560744040983356684, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756900870.356352     926 tpu_compilation_cache_interface.cc:542] After adding entry for key 3608692472238479403 with session_name  cache is 3 entries (239795202 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.3565 - f1_macro: 0.1982 - loss: 6.6145","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756900885.859140      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:14856046441394204111\nI0000 00:00:1756900887.075007     931 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(17051414394516747451), session_name()\nI0000 00:00:1756900899.074047     931 tpu_compile_op_common.cc:245] Compilation of 17051414394516747451 with session name  took 11.998988771s and succeeded\nI0000 00:00:1756900899.091140     931 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(17051414394516747451), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_14856046441394204111\", property.function_library_fingerprint = 15938656239983341799, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756900899.091175     931 tpu_compilation_cache_interface.cc:542] After adding entry for key 17051414394516747451 with session_name  cache is 4 entries (265576025 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 355ms/step - accuracy: 0.3551 - f1_macro: 0.1983 - loss: 6.6397 - val_accuracy: 0.2524 - val_f1_macro: 0.1779 - val_loss: 7.8507\nEpoch 2/3\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 151ms/step - accuracy: 0.3536 - f1_macro: 0.2148 - loss: 6.8836 - val_accuracy: 0.2355 - val_f1_macro: 0.2113 - val_loss: 7.2640\nEpoch 3/3\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 134ms/step - accuracy: 0.3362 - f1_macro: 0.2126 - loss: 6.8015 - val_accuracy: 0.2375 - val_f1_macro: 0.2035 - val_loss: 7.2215\n\n--- Giai đoạn 2B: Bắt đầu huấn luyện chính ---\nSử dụng scheduler: CosineDecayRestarts\n\nEpoch 4: Learning Rate is 5.00e-06\nEpoch 4/200\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756900949.452676      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:15045101906473860996\nI0000 00:00:1756900955.328865     968 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(3412828142271044695), session_name()\nI0000 00:00:1756900989.086986     968 tpu_compile_op_common.cc:245] Compilation of 3412828142271044695 with session name  took 33.758049364s and succeeded\nI0000 00:00:1756900989.180098     968 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(3412828142271044695), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_15045101906473860996\", property.function_library_fingerprint = 7398845064470284712, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756900989.180160     968 tpu_compilation_cache_interface.cc:542] After adding entry for key 3412828142271044695 with session_name  cache is 5 entries (404396948 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.3771 - auc: 0.6155 - f1_macro: 0.2008 - loss: 6.3941","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756901004.953979      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:7394872944529074536\nI0000 00:00:1756901006.134451     895 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(8429148184976958832), session_name()\nI0000 00:00:1756901018.295256     895 tpu_compile_op_common.cc:245] Compilation of 8429148184976958832 with session name  took 12.160757039s and succeeded\nI0000 00:00:1756901018.310372     895 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(8429148184976958832), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_7394872944529074536\", property.function_library_fingerprint = 12769297493263875905, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756901018.310409     895 tpu_compilation_cache_interface.cc:542] After adding entry for key 8429148184976958832 with session_name  cache is 6 entries (431913251 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 368ms/step - accuracy: 0.3757 - auc: 0.6141 - f1_macro: 0.2010 - loss: 6.4158 - val_accuracy: 0.3493 - val_auc: 0.5895 - val_f1_macro: 0.3390 - val_loss: 5.9316\n\nEpoch 5: Learning Rate is 5.00e-06\nEpoch 5/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.3616 - auc: 0.6104 - f1_macro: 0.2294 - loss: 6.2557 - val_accuracy: 0.4051 - val_auc: 0.6292 - val_f1_macro: 0.3876 - val_loss: 5.5956\n\nEpoch 6: Learning Rate is 4.98e-06\nEpoch 6/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 146ms/step - accuracy: 0.3613 - auc: 0.6054 - f1_macro: 0.2605 - loss: 6.1773 - val_accuracy: 0.4245 - val_auc: 0.6471 - val_f1_macro: 0.3977 - val_loss: 5.4983\n\nEpoch 7: Learning Rate is 4.96e-06\nEpoch 7/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 145ms/step - accuracy: 0.3629 - auc: 0.6052 - f1_macro: 0.2821 - loss: 6.0782 - val_accuracy: 0.4369 - val_auc: 0.6592 - val_f1_macro: 0.4038 - val_loss: 5.4251\n\nEpoch 8: Learning Rate is 4.92e-06\nEpoch 8/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 150ms/step - accuracy: 0.3572 - auc: 0.5990 - f1_macro: 0.2814 - loss: 6.0259 - val_accuracy: 0.4520 - val_auc: 0.6686 - val_f1_macro: 0.4069 - val_loss: 5.3652\n\nEpoch 9: Learning Rate is 4.88e-06\nEpoch 9/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.3656 - auc: 0.6044 - f1_macro: 0.3087 - loss: 6.0463 - val_accuracy: 0.4557 - val_auc: 0.6718 - val_f1_macro: 0.4009 - val_loss: 5.3380\n\nEpoch 10: Learning Rate is 4.82e-06\nEpoch 10/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.3891 - auc: 0.6254 - f1_macro: 0.3214 - loss: 5.8233 - val_accuracy: 0.4650 - val_auc: 0.6788 - val_f1_macro: 0.4079 - val_loss: 5.2942\n\nEpoch 11: Learning Rate is 4.76e-06\nEpoch 11/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 149ms/step - accuracy: 0.4307 - auc: 0.6543 - f1_macro: 0.3552 - loss: 5.4426 - val_accuracy: 0.4691 - val_auc: 0.6804 - val_f1_macro: 0.4096 - val_loss: 5.2572\n\nEpoch 12: Learning Rate is 4.69e-06\nEpoch 12/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 153ms/step - accuracy: 0.4377 - auc: 0.6618 - f1_macro: 0.3627 - loss: 5.3017 - val_accuracy: 0.4715 - val_auc: 0.6807 - val_f1_macro: 0.4083 - val_loss: 5.2474\n\nEpoch 13: Learning Rate is 4.61e-06\nEpoch 13/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 152ms/step - accuracy: 0.4607 - auc: 0.6757 - f1_macro: 0.3759 - loss: 5.2435 - val_accuracy: 0.4760 - val_auc: 0.6848 - val_f1_macro: 0.4106 - val_loss: 5.2070\n\nEpoch 14: Learning Rate is 4.52e-06\nEpoch 14/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 151ms/step - accuracy: 0.4651 - auc: 0.6863 - f1_macro: 0.3722 - loss: 5.0102 - val_accuracy: 0.4788 - val_auc: 0.6900 - val_f1_macro: 0.4124 - val_loss: 5.1642\n\nEpoch 15: Learning Rate is 4.43e-06\nEpoch 15/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 169ms/step - accuracy: 0.4772 - auc: 0.6900 - f1_macro: 0.3749 - loss: 4.9447 - val_accuracy: 0.4836 - val_auc: 0.6929 - val_f1_macro: 0.4177 - val_loss: 5.1154\n\nEpoch 16: Learning Rate is 4.32e-06\nEpoch 16/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4869 - auc: 0.7072 - f1_macro: 0.3710 - loss: 4.7758 - val_accuracy: 0.4840 - val_auc: 0.6936 - val_f1_macro: 0.4246 - val_loss: 5.0965\n\nEpoch 17: Learning Rate is 4.21e-06\nEpoch 17/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 154ms/step - accuracy: 0.5235 - auc: 0.7378 - f1_macro: 0.4023 - loss: 4.4554 - val_accuracy: 0.4847 - val_auc: 0.6958 - val_f1_macro: 0.4279 - val_loss: 5.0674\n\nEpoch 18: Learning Rate is 4.09e-06\nEpoch 18/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 151ms/step - accuracy: 0.5209 - auc: 0.7286 - f1_macro: 0.3900 - loss: 4.4851 - val_accuracy: 0.4907 - val_auc: 0.6989 - val_f1_macro: 0.4337 - val_loss: 5.0184\n\nEpoch 19: Learning Rate is 3.97e-06\nEpoch 19/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.5536 - auc: 0.7507 - f1_macro: 0.4142 - loss: 4.2724 - val_accuracy: 0.4907 - val_auc: 0.7031 - val_f1_macro: 0.4354 - val_loss: 4.9931\n\nEpoch 20: Learning Rate is 3.84e-06\nEpoch 20/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 142ms/step - accuracy: 0.5696 - auc: 0.7687 - f1_macro: 0.4208 - loss: 4.0641 - val_accuracy: 0.4922 - val_auc: 0.7040 - val_f1_macro: 0.4377 - val_loss: 4.9702\n\nEpoch 21: Learning Rate is 3.70e-06\nEpoch 21/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 141ms/step - accuracy: 0.5609 - auc: 0.7647 - f1_macro: 0.4127 - loss: 4.0192 - val_accuracy: 0.4896 - val_auc: 0.7070 - val_f1_macro: 0.4368 - val_loss: 4.9290\n\nEpoch 22: Learning Rate is 3.56e-06\nEpoch 22/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.5635 - auc: 0.7680 - f1_macro: 0.4180 - loss: 3.9731 - val_accuracy: 0.4914 - val_auc: 0.7085 - val_f1_macro: 0.4429 - val_loss: 4.9052\n\nEpoch 23: Learning Rate is 3.42e-06\nEpoch 23/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 146ms/step - accuracy: 0.5846 - auc: 0.7802 - f1_macro: 0.4355 - loss: 3.8157 - val_accuracy: 0.4924 - val_auc: 0.7106 - val_f1_macro: 0.4473 - val_loss: 4.8770\n\nEpoch 24: Learning Rate is 3.27e-06\nEpoch 24/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 146ms/step - accuracy: 0.5967 - auc: 0.7979 - f1_macro: 0.4332 - loss: 3.5762 - val_accuracy: 0.4940 - val_auc: 0.7133 - val_f1_macro: 0.4482 - val_loss: 4.8514\n\nEpoch 25: Learning Rate is 3.12e-06\nEpoch 25/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 139ms/step - accuracy: 0.6239 - auc: 0.8132 - f1_macro: 0.4584 - loss: 3.4155 - val_accuracy: 0.4933 - val_auc: 0.7151 - val_f1_macro: 0.4482 - val_loss: 4.8374\n\nEpoch 26: Learning Rate is 2.97e-06\nEpoch 26/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 158ms/step - accuracy: 0.6238 - auc: 0.8124 - f1_macro: 0.4642 - loss: 3.3434 - val_accuracy: 0.4909 - val_auc: 0.7139 - val_f1_macro: 0.4495 - val_loss: 4.8370\n\nEpoch 27: Learning Rate is 2.81e-06\nEpoch 27/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 154ms/step - accuracy: 0.6290 - auc: 0.8239 - f1_macro: 0.4305 - loss: 3.2593 - val_accuracy: 0.4900 - val_auc: 0.7140 - val_f1_macro: 0.4509 - val_loss: 4.8259\n\nEpoch 28: Learning Rate is 2.66e-06\nEpoch 28/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 141ms/step - accuracy: 0.6429 - auc: 0.8205 - f1_macro: 0.4607 - loss: 3.3447 - val_accuracy: 0.4879 - val_auc: 0.7157 - val_f1_macro: 0.4502 - val_loss: 4.8126\n\nEpoch 29: Learning Rate is 2.50e-06\nEpoch 29/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 155ms/step - accuracy: 0.6528 - auc: 0.8270 - f1_macro: 0.4585 - loss: 3.2049 - val_accuracy: 0.4888 - val_auc: 0.7183 - val_f1_macro: 0.4488 - val_loss: 4.7911\n\nEpoch 30: Learning Rate is 2.34e-06\nEpoch 30/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.6606 - auc: 0.8328 - f1_macro: 0.4628 - loss: 3.1216 - val_accuracy: 0.4870 - val_auc: 0.7175 - val_f1_macro: 0.4482 - val_loss: 4.7866\n\nEpoch 31: Learning Rate is 2.19e-06\nEpoch 31/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 137ms/step - accuracy: 0.6669 - auc: 0.8404 - f1_macro: 0.4592 - loss: 3.0325 - val_accuracy: 0.4833 - val_auc: 0.7183 - val_f1_macro: 0.4493 - val_loss: 4.7666\n\nEpoch 32: Learning Rate is 2.03e-06\nEpoch 32/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 146ms/step - accuracy: 0.6654 - auc: 0.8416 - f1_macro: 0.4609 - loss: 3.0312 - val_accuracy: 0.4844 - val_auc: 0.7184 - val_f1_macro: 0.4525 - val_loss: 4.7576\n\nEpoch 33: Learning Rate is 1.88e-06\nEpoch 33/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.6789 - auc: 0.8452 - f1_macro: 0.4600 - loss: 2.9451 - val_accuracy: 0.4799 - val_auc: 0.7176 - val_f1_macro: 0.4498 - val_loss: 4.7583\n\nEpoch 34: Learning Rate is 1.73e-06\nEpoch 34/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 151ms/step - accuracy: 0.6959 - auc: 0.8579 - f1_macro: 0.4721 - loss: 2.7504 - val_accuracy: 0.4756 - val_auc: 0.7163 - val_f1_macro: 0.4498 - val_loss: 4.7440\n\nEpoch 35: Learning Rate is 1.58e-06\nEpoch 35/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 149ms/step - accuracy: 0.6850 - auc: 0.8472 - f1_macro: 0.4628 - loss: 2.9126 - val_accuracy: 0.4753 - val_auc: 0.7156 - val_f1_macro: 0.4490 - val_loss: 4.7447\n\nEpoch 36: Learning Rate is 1.44e-06\nEpoch 36/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.6984 - auc: 0.8525 - f1_macro: 0.4852 - loss: 2.8652 - val_accuracy: 0.4745 - val_auc: 0.7155 - val_f1_macro: 0.4487 - val_loss: 4.7448\n\nEpoch 37: Learning Rate is 1.30e-06\nEpoch 37/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 153ms/step - accuracy: 0.6632 - auc: 0.8383 - f1_macro: 0.4481 - loss: 3.0566 - val_accuracy: 0.4751 - val_auc: 0.7163 - val_f1_macro: 0.4504 - val_loss: 4.7294\n\nEpoch 38: Learning Rate is 1.16e-06\nEpoch 38/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.6562 - auc: 0.8295 - f1_macro: 0.4514 - loss: 3.1455 - val_accuracy: 0.4632 - val_auc: 0.7124 - val_f1_macro: 0.4404 - val_loss: 4.7587\n\nEpoch 39: Learning Rate is 1.03e-06\nEpoch 39/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 154ms/step - accuracy: 0.7065 - auc: 0.8640 - f1_macro: 0.4775 - loss: 2.6779 - val_accuracy: 0.4648 - val_auc: 0.7129 - val_f1_macro: 0.4424 - val_loss: 4.7513\n\nEpoch 40: Learning Rate is 9.06e-07\nEpoch 40/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.6443 - auc: 0.8174 - f1_macro: 0.4487 - loss: 3.3184 - val_accuracy: 0.4682 - val_auc: 0.7150 - val_f1_macro: 0.4441 - val_loss: 4.7399\n\nEpoch 41: Learning Rate is 7.89e-07\nEpoch 41/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.6518 - auc: 0.8265 - f1_macro: 0.4566 - loss: 3.2019 - val_accuracy: 0.4721 - val_auc: 0.7165 - val_f1_macro: 0.4480 - val_loss: 4.7240\n\nEpoch 42: Learning Rate is 6.78e-07\nEpoch 42/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 141ms/step - accuracy: 0.6553 - auc: 0.8331 - f1_macro: 0.4442 - loss: 2.9957 - val_accuracy: 0.4674 - val_auc: 0.7148 - val_f1_macro: 0.4446 - val_loss: 4.7347\n\nEpoch 43: Learning Rate is 5.74e-07\nEpoch 43/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.6685 - auc: 0.8371 - f1_macro: 0.4601 - loss: 3.0196 - val_accuracy: 0.4715 - val_auc: 0.7160 - val_f1_macro: 0.4488 - val_loss: 4.7196\n\nEpoch 44: Learning Rate is 4.77e-07\nEpoch 44/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 145ms/step - accuracy: 0.6687 - auc: 0.8408 - f1_macro: 0.4646 - loss: 2.9757 - val_accuracy: 0.4715 - val_auc: 0.7177 - val_f1_macro: 0.4468 - val_loss: 4.7125\n\nEpoch 45: Learning Rate is 3.89e-07\nEpoch 45/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 146ms/step - accuracy: 0.6305 - auc: 0.8108 - f1_macro: 0.4411 - loss: 3.2964 - val_accuracy: 0.4730 - val_auc: 0.7184 - val_f1_macro: 0.4485 - val_loss: 4.7106\n\nEpoch 46: Learning Rate is 3.09e-07\nEpoch 46/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.6476 - auc: 0.8314 - f1_macro: 0.4617 - loss: 3.0586 - val_accuracy: 0.4784 - val_auc: 0.7212 - val_f1_macro: 0.4517 - val_loss: 4.6939\n\nEpoch 47: Learning Rate is 2.38e-07\nEpoch 47/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 146ms/step - accuracy: 0.6280 - auc: 0.8123 - f1_macro: 0.4446 - loss: 3.2074 - val_accuracy: 0.4762 - val_auc: 0.7210 - val_f1_macro: 0.4498 - val_loss: 4.6944\n\nEpoch 48: Learning Rate is 1.76e-07\nEpoch 48/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.6392 - auc: 0.8229 - f1_macro: 0.4528 - loss: 3.1464 - val_accuracy: 0.4844 - val_auc: 0.7217 - val_f1_macro: 0.4578 - val_loss: 4.6827\n\nEpoch 49: Learning Rate is 1.22e-07\nEpoch 49/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.6129 - auc: 0.8132 - f1_macro: 0.4384 - loss: 3.2244 - val_accuracy: 0.4829 - val_auc: 0.7219 - val_f1_macro: 0.4566 - val_loss: 4.6811\n\nEpoch 50: Learning Rate is 7.85e-08\nEpoch 50/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.6256 - auc: 0.8067 - f1_macro: 0.4474 - loss: 3.2231 - val_accuracy: 0.4847 - val_auc: 0.7236 - val_f1_macro: 0.4574 - val_loss: 4.6740\n\nEpoch 51: Learning Rate is 4.43e-08\nEpoch 51/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.6281 - auc: 0.8126 - f1_macro: 0.4503 - loss: 3.1692 - val_accuracy: 0.4808 - val_auc: 0.7227 - val_f1_macro: 0.4539 - val_loss: 4.6792\n\nEpoch 52: Learning Rate is 1.97e-08\nEpoch 52/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 136ms/step - accuracy: 0.6313 - auc: 0.8146 - f1_macro: 0.4608 - loss: 3.1927 - val_accuracy: 0.4868 - val_auc: 0.7250 - val_f1_macro: 0.4577 - val_loss: 4.6716\n\nEpoch 53: Learning Rate is 4.93e-09\nEpoch 53/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.6199 - auc: 0.8109 - f1_macro: 0.4484 - loss: 3.1060 - val_accuracy: 0.4870 - val_auc: 0.7258 - val_f1_macro: 0.4576 - val_loss: 4.6677\n\nEpoch 54: Learning Rate is 4.50e-06\nEpoch 54/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.5749 - auc: 0.7908 - f1_macro: 0.4002 - loss: 3.4096 - val_accuracy: 0.4667 - val_auc: 0.7172 - val_f1_macro: 0.4437 - val_loss: 4.6967\n\nEpoch 55: Learning Rate is 4.50e-06\nEpoch 55/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.5724 - auc: 0.7875 - f1_macro: 0.4084 - loss: 3.4194 - val_accuracy: 0.4732 - val_auc: 0.7193 - val_f1_macro: 0.4511 - val_loss: 4.6513\n\nEpoch 56: Learning Rate is 4.50e-06\nEpoch 56/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 140ms/step - accuracy: 0.5257 - auc: 0.7662 - f1_macro: 0.3796 - loss: 3.6852 - val_accuracy: 0.4747 - val_auc: 0.7217 - val_f1_macro: 0.4550 - val_loss: 4.6195\n\nEpoch 57: Learning Rate is 4.49e-06\nEpoch 57/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 134ms/step - accuracy: 0.5886 - auc: 0.7958 - f1_macro: 0.4221 - loss: 3.2151 - val_accuracy: 0.4589 - val_auc: 0.7156 - val_f1_macro: 0.4422 - val_loss: 4.6449\n\nEpoch 58: Learning Rate is 4.48e-06\nEpoch 58/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 151ms/step - accuracy: 0.5586 - auc: 0.7812 - f1_macro: 0.4012 - loss: 3.4655 - val_accuracy: 0.4736 - val_auc: 0.7240 - val_f1_macro: 0.4570 - val_loss: 4.5567\n\nEpoch 59: Learning Rate is 4.47e-06\nEpoch 59/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 145ms/step - accuracy: 0.5485 - auc: 0.7754 - f1_macro: 0.4045 - loss: 3.5193 - val_accuracy: 0.4944 - val_auc: 0.7341 - val_f1_macro: 0.4744 - val_loss: 4.4767\n\nEpoch 60: Learning Rate is 4.46e-06\nEpoch 60/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 138ms/step - accuracy: 0.5615 - auc: 0.7859 - f1_macro: 0.4034 - loss: 3.3038 - val_accuracy: 0.4942 - val_auc: 0.7374 - val_f1_macro: 0.4721 - val_loss: 4.4543\n\nEpoch 61: Learning Rate is 4.45e-06\nEpoch 61/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 153ms/step - accuracy: 0.5585 - auc: 0.7852 - f1_macro: 0.4038 - loss: 3.2303 - val_accuracy: 0.4942 - val_auc: 0.7383 - val_f1_macro: 0.4703 - val_loss: 4.4236\n\nEpoch 62: Learning Rate is 4.43e-06\nEpoch 62/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.5497 - auc: 0.7758 - f1_macro: 0.3983 - loss: 3.2607 - val_accuracy: 0.4968 - val_auc: 0.7414 - val_f1_macro: 0.4715 - val_loss: 4.3975\n\nEpoch 63: Learning Rate is 4.41e-06\nEpoch 63/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 136ms/step - accuracy: 0.5376 - auc: 0.7791 - f1_macro: 0.4030 - loss: 3.2659 - val_accuracy: 0.4940 - val_auc: 0.7376 - val_f1_macro: 0.4710 - val_loss: 4.4010\n\nEpoch 64: Learning Rate is 4.39e-06\nEpoch 64/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.5481 - auc: 0.7791 - f1_macro: 0.4126 - loss: 3.3435 - val_accuracy: 0.5013 - val_auc: 0.7431 - val_f1_macro: 0.4741 - val_loss: 4.3744\n\nEpoch 65: Learning Rate is 4.37e-06\nEpoch 65/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 156ms/step - accuracy: 0.5230 - auc: 0.7684 - f1_macro: 0.3983 - loss: 3.4344 - val_accuracy: 0.4965 - val_auc: 0.7408 - val_f1_macro: 0.4716 - val_loss: 4.3788\n\nEpoch 66: Learning Rate is 4.34e-06\nEpoch 66/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 145ms/step - accuracy: 0.5348 - auc: 0.7655 - f1_macro: 0.3944 - loss: 3.4187 - val_accuracy: 0.5037 - val_auc: 0.7456 - val_f1_macro: 0.4756 - val_loss: 4.3505\n\nEpoch 67: Learning Rate is 4.31e-06\nEpoch 67/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 147ms/step - accuracy: 0.5161 - auc: 0.7641 - f1_macro: 0.3836 - loss: 3.4247 - val_accuracy: 0.5093 - val_auc: 0.7493 - val_f1_macro: 0.4769 - val_loss: 4.3342\n\nEpoch 68: Learning Rate is 4.29e-06\nEpoch 68/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 156ms/step - accuracy: 0.5212 - auc: 0.7644 - f1_macro: 0.3962 - loss: 3.4290 - val_accuracy: 0.5123 - val_auc: 0.7551 - val_f1_macro: 0.4765 - val_loss: 4.2850\n\nEpoch 69: Learning Rate is 4.25e-06\nEpoch 69/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 152ms/step - accuracy: 0.5144 - auc: 0.7618 - f1_macro: 0.3870 - loss: 3.5017 - val_accuracy: 0.5156 - val_auc: 0.7572 - val_f1_macro: 0.4798 - val_loss: 4.2513\n\nEpoch 70: Learning Rate is 4.22e-06\nEpoch 70/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 142ms/step - accuracy: 0.5102 - auc: 0.7585 - f1_macro: 0.3787 - loss: 3.4676 - val_accuracy: 0.5074 - val_auc: 0.7561 - val_f1_macro: 0.4715 - val_loss: 4.2541\n\nEpoch 71: Learning Rate is 4.19e-06\nEpoch 71/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 132ms/step - accuracy: 0.5122 - auc: 0.7617 - f1_macro: 0.3876 - loss: 3.5072 - val_accuracy: 0.5208 - val_auc: 0.7623 - val_f1_macro: 0.4778 - val_loss: 4.2080\n\nEpoch 72: Learning Rate is 4.15e-06\nEpoch 72/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.5057 - auc: 0.7587 - f1_macro: 0.3883 - loss: 3.4981 - val_accuracy: 0.5236 - val_auc: 0.7632 - val_f1_macro: 0.4773 - val_loss: 4.1910\n\nEpoch 73: Learning Rate is 4.11e-06\nEpoch 73/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 144ms/step - accuracy: 0.5137 - auc: 0.7666 - f1_macro: 0.4106 - loss: 3.4217 - val_accuracy: 0.5193 - val_auc: 0.7626 - val_f1_macro: 0.4700 - val_loss: 4.2110\n\nEpoch 74: Learning Rate is 4.07e-06\nEpoch 74/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 146ms/step - accuracy: 0.5110 - auc: 0.7528 - f1_macro: 0.4036 - loss: 3.4958 - val_accuracy: 0.5240 - val_auc: 0.7656 - val_f1_macro: 0.4717 - val_loss: 4.1982\n\nEpoch 75: Learning Rate is 4.03e-06\nEpoch 75/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.5037 - auc: 0.7601 - f1_macro: 0.3954 - loss: 3.4942 - val_accuracy: 0.5285 - val_auc: 0.7675 - val_f1_macro: 0.4748 - val_loss: 4.1753\n\nEpoch 76: Learning Rate is 3.98e-06\nEpoch 76/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 152ms/step - accuracy: 0.4940 - auc: 0.7507 - f1_macro: 0.3895 - loss: 3.5815 - val_accuracy: 0.5257 - val_auc: 0.7674 - val_f1_macro: 0.4708 - val_loss: 4.1763\n\nEpoch 77: Learning Rate is 3.94e-06\nEpoch 77/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 141ms/step - accuracy: 0.5089 - auc: 0.7562 - f1_macro: 0.3951 - loss: 3.5419 - val_accuracy: 0.5294 - val_auc: 0.7684 - val_f1_macro: 0.4740 - val_loss: 4.1634\n\nEpoch 78: Learning Rate is 3.89e-06\nEpoch 78/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 146ms/step - accuracy: 0.4898 - auc: 0.7504 - f1_macro: 0.3953 - loss: 3.6598 - val_accuracy: 0.5298 - val_auc: 0.7672 - val_f1_macro: 0.4727 - val_loss: 4.1705\n\nEpoch 79: Learning Rate is 3.84e-06\nEpoch 79/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.4914 - auc: 0.7527 - f1_macro: 0.3863 - loss: 3.6589 - val_accuracy: 0.5298 - val_auc: 0.7682 - val_f1_macro: 0.4724 - val_loss: 4.1598\n\nEpoch 80: Learning Rate is 3.79e-06\nEpoch 80/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.4969 - auc: 0.7554 - f1_macro: 0.3981 - loss: 3.7112 - val_accuracy: 0.5329 - val_auc: 0.7697 - val_f1_macro: 0.4748 - val_loss: 4.1537\n\nEpoch 81: Learning Rate is 3.74e-06\nEpoch 81/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 147ms/step - accuracy: 0.4871 - auc: 0.7539 - f1_macro: 0.3998 - loss: 3.7906 - val_accuracy: 0.5355 - val_auc: 0.7728 - val_f1_macro: 0.4731 - val_loss: 4.1248\n\nEpoch 82: Learning Rate is 3.68e-06\nEpoch 82/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 158ms/step - accuracy: 0.4820 - auc: 0.7492 - f1_macro: 0.3982 - loss: 3.8527 - val_accuracy: 0.5391 - val_auc: 0.7750 - val_f1_macro: 0.4757 - val_loss: 4.1111\n\nEpoch 83: Learning Rate is 3.63e-06\nEpoch 83/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 138ms/step - accuracy: 0.4670 - auc: 0.7321 - f1_macro: 0.3916 - loss: 3.9996 - val_accuracy: 0.5419 - val_auc: 0.7747 - val_f1_macro: 0.4752 - val_loss: 4.1160\n\nEpoch 84: Learning Rate is 3.57e-06\nEpoch 84/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 142ms/step - accuracy: 0.4872 - auc: 0.7560 - f1_macro: 0.4050 - loss: 3.8105 - val_accuracy: 0.5376 - val_auc: 0.7749 - val_f1_macro: 0.4713 - val_loss: 4.1159\n\nEpoch 85: Learning Rate is 3.51e-06\nEpoch 85/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 147ms/step - accuracy: 0.4895 - auc: 0.7427 - f1_macro: 0.4043 - loss: 4.0546 - val_accuracy: 0.5378 - val_auc: 0.7763 - val_f1_macro: 0.4686 - val_loss: 4.1028\n\nEpoch 86: Learning Rate is 3.46e-06\nEpoch 86/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 147ms/step - accuracy: 0.4693 - auc: 0.7438 - f1_macro: 0.3965 - loss: 4.0705 - val_accuracy: 0.5402 - val_auc: 0.7770 - val_f1_macro: 0.4707 - val_loss: 4.1030\n\nEpoch 87: Learning Rate is 3.40e-06\nEpoch 87/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 137ms/step - accuracy: 0.4627 - auc: 0.7402 - f1_macro: 0.3925 - loss: 4.1827 - val_accuracy: 0.5376 - val_auc: 0.7772 - val_f1_macro: 0.4676 - val_loss: 4.0968\n\nEpoch 88: Learning Rate is 3.33e-06\nEpoch 88/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 149ms/step - accuracy: 0.4551 - auc: 0.7389 - f1_macro: 0.3828 - loss: 4.2938 - val_accuracy: 0.5374 - val_auc: 0.7776 - val_f1_macro: 0.4632 - val_loss: 4.1159\n\nEpoch 89: Learning Rate is 3.27e-06\nEpoch 89/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.4538 - auc: 0.7266 - f1_macro: 0.3871 - loss: 4.4881 - val_accuracy: 0.5357 - val_auc: 0.7799 - val_f1_macro: 0.4620 - val_loss: 4.0859\n\nEpoch 90: Learning Rate is 3.21e-06\nEpoch 90/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 139ms/step - accuracy: 0.4484 - auc: 0.7361 - f1_macro: 0.3817 - loss: 4.4246 - val_accuracy: 0.5387 - val_auc: 0.7794 - val_f1_macro: 0.4648 - val_loss: 4.0834\n\nEpoch 91: Learning Rate is 3.14e-06\nEpoch 91/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 139ms/step - accuracy: 0.4915 - auc: 0.7452 - f1_macro: 0.4265 - loss: 4.3521 - val_accuracy: 0.5392 - val_auc: 0.7803 - val_f1_macro: 0.4636 - val_loss: 4.0804\n\nEpoch 92: Learning Rate is 3.08e-06\nEpoch 92/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 140ms/step - accuracy: 0.4658 - auc: 0.7394 - f1_macro: 0.3998 - loss: 4.4682 - val_accuracy: 0.5379 - val_auc: 0.7789 - val_f1_macro: 0.4628 - val_loss: 4.0779\n\nEpoch 93: Learning Rate is 3.01e-06\nEpoch 93/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 154ms/step - accuracy: 0.4311 - auc: 0.7230 - f1_macro: 0.3709 - loss: 4.7817 - val_accuracy: 0.5406 - val_auc: 0.7790 - val_f1_macro: 0.4651 - val_loss: 4.0804\n\nEpoch 94: Learning Rate is 2.95e-06\nEpoch 94/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.4465 - auc: 0.7310 - f1_macro: 0.3849 - loss: 4.6435 - val_accuracy: 0.5379 - val_auc: 0.7804 - val_f1_macro: 0.4602 - val_loss: 4.0714\n\nEpoch 95: Learning Rate is 2.88e-06\nEpoch 95/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 145ms/step - accuracy: 0.4498 - auc: 0.7316 - f1_macro: 0.3906 - loss: 4.6204 - val_accuracy: 0.5383 - val_auc: 0.7808 - val_f1_macro: 0.4614 - val_loss: 4.0618\n\nEpoch 96: Learning Rate is 2.81e-06\nEpoch 96/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4373 - auc: 0.7409 - f1_macro: 0.3812 - loss: 4.4065 - val_accuracy: 0.5385 - val_auc: 0.7824 - val_f1_macro: 0.4575 - val_loss: 4.0626\n\nEpoch 97: Learning Rate is 2.74e-06\nEpoch 97/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 156ms/step - accuracy: 0.4724 - auc: 0.7440 - f1_macro: 0.4083 - loss: 4.3474 - val_accuracy: 0.5419 - val_auc: 0.7822 - val_f1_macro: 0.4591 - val_loss: 4.0592\n\nEpoch 98: Learning Rate is 2.67e-06\nEpoch 98/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 139ms/step - accuracy: 0.4753 - auc: 0.7452 - f1_macro: 0.4105 - loss: 4.3920 - val_accuracy: 0.5406 - val_auc: 0.7825 - val_f1_macro: 0.4594 - val_loss: 4.0552\n\nEpoch 99: Learning Rate is 2.60e-06\nEpoch 99/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 170ms/step - accuracy: 0.4622 - auc: 0.7402 - f1_macro: 0.3967 - loss: 4.4656 - val_accuracy: 0.5389 - val_auc: 0.7830 - val_f1_macro: 0.4551 - val_loss: 4.0491\n\nEpoch 100: Learning Rate is 2.53e-06\nEpoch 100/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.4576 - auc: 0.7371 - f1_macro: 0.3949 - loss: 4.5270 - val_accuracy: 0.5417 - val_auc: 0.7817 - val_f1_macro: 0.4577 - val_loss: 4.0489\n\nEpoch 101: Learning Rate is 2.46e-06\nEpoch 101/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 147ms/step - accuracy: 0.4511 - auc: 0.7365 - f1_macro: 0.3909 - loss: 4.5397 - val_accuracy: 0.5422 - val_auc: 0.7836 - val_f1_macro: 0.4566 - val_loss: 4.0423\n\nEpoch 102: Learning Rate is 2.39e-06\nEpoch 102/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 151ms/step - accuracy: 0.4613 - auc: 0.7322 - f1_macro: 0.3972 - loss: 4.4250 - val_accuracy: 0.5446 - val_auc: 0.7844 - val_f1_macro: 0.4570 - val_loss: 4.0383\n\nEpoch 103: Learning Rate is 2.32e-06\nEpoch 103/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.4638 - auc: 0.7378 - f1_macro: 0.3963 - loss: 4.3480 - val_accuracy: 0.5443 - val_auc: 0.7845 - val_f1_macro: 0.4566 - val_loss: 4.0304\n\nEpoch 104: Learning Rate is 2.25e-06\nEpoch 104/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4663 - auc: 0.7493 - f1_macro: 0.4008 - loss: 4.1942 - val_accuracy: 0.5446 - val_auc: 0.7849 - val_f1_macro: 0.4567 - val_loss: 4.0277\n\nEpoch 105: Learning Rate is 2.18e-06\nEpoch 105/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 145ms/step - accuracy: 0.4463 - auc: 0.7214 - f1_macro: 0.3814 - loss: 4.4976 - val_accuracy: 0.5437 - val_auc: 0.7852 - val_f1_macro: 0.4524 - val_loss: 4.0295\n\nEpoch 106: Learning Rate is 2.11e-06\nEpoch 106/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 142ms/step - accuracy: 0.4586 - auc: 0.7370 - f1_macro: 0.3871 - loss: 4.2653 - val_accuracy: 0.5452 - val_auc: 0.7853 - val_f1_macro: 0.4529 - val_loss: 4.0321\n\nEpoch 107: Learning Rate is 2.04e-06\nEpoch 107/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.4545 - auc: 0.7271 - f1_macro: 0.3740 - loss: 4.3445 - val_accuracy: 0.5446 - val_auc: 0.7843 - val_f1_macro: 0.4527 - val_loss: 4.0414\n\nEpoch 108: Learning Rate is 1.97e-06\nEpoch 108/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 151ms/step - accuracy: 0.4374 - auc: 0.7259 - f1_macro: 0.3624 - loss: 4.3539 - val_accuracy: 0.5439 - val_auc: 0.7852 - val_f1_macro: 0.4505 - val_loss: 4.0386\n\nEpoch 109: Learning Rate is 1.90e-06\nEpoch 109/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.4686 - auc: 0.7430 - f1_macro: 0.3854 - loss: 4.1209 - val_accuracy: 0.5459 - val_auc: 0.7868 - val_f1_macro: 0.4503 - val_loss: 4.0304\n\nEpoch 110: Learning Rate is 1.83e-06\nEpoch 110/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 145ms/step - accuracy: 0.4545 - auc: 0.7347 - f1_macro: 0.3748 - loss: 4.1882 - val_accuracy: 0.5478 - val_auc: 0.7865 - val_f1_macro: 0.4535 - val_loss: 4.0298\n\nEpoch 111: Learning Rate is 1.76e-06\nEpoch 111/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 156ms/step - accuracy: 0.4476 - auc: 0.7304 - f1_macro: 0.3672 - loss: 4.1452 - val_accuracy: 0.5452 - val_auc: 0.7861 - val_f1_macro: 0.4476 - val_loss: 4.0513\n\nEpoch 112: Learning Rate is 1.69e-06\nEpoch 112/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 150ms/step - accuracy: 0.4608 - auc: 0.7385 - f1_macro: 0.3736 - loss: 4.0531 - val_accuracy: 0.5458 - val_auc: 0.7860 - val_f1_macro: 0.4483 - val_loss: 4.0514\n\nEpoch 113: Learning Rate is 1.62e-06\nEpoch 113/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.4765 - auc: 0.7417 - f1_macro: 0.3741 - loss: 4.0374 - val_accuracy: 0.5469 - val_auc: 0.7870 - val_f1_macro: 0.4475 - val_loss: 4.0434\n\nEpoch 114: Learning Rate is 1.55e-06\nEpoch 114/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4750 - auc: 0.7424 - f1_macro: 0.3757 - loss: 4.0208 - val_accuracy: 0.5482 - val_auc: 0.7863 - val_f1_macro: 0.4487 - val_loss: 4.0458\n\nEpoch 115: Learning Rate is 1.49e-06\nEpoch 115/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 150ms/step - accuracy: 0.4878 - auc: 0.7410 - f1_macro: 0.3816 - loss: 3.9949 - val_accuracy: 0.5465 - val_auc: 0.7867 - val_f1_macro: 0.4474 - val_loss: 4.0436\n\nEpoch 116: Learning Rate is 1.42e-06\nEpoch 116/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.4544 - auc: 0.7346 - f1_macro: 0.3480 - loss: 4.0558 - val_accuracy: 0.5469 - val_auc: 0.7852 - val_f1_macro: 0.4463 - val_loss: 4.0620\n\nEpoch 117: Learning Rate is 1.36e-06\nEpoch 117/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 147ms/step - accuracy: 0.4663 - auc: 0.7322 - f1_macro: 0.3641 - loss: 3.9886 - val_accuracy: 0.5459 - val_auc: 0.7864 - val_f1_macro: 0.4441 - val_loss: 4.0528\n\nEpoch 118: Learning Rate is 1.29e-06\nEpoch 118/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 169ms/step - accuracy: 0.4707 - auc: 0.7320 - f1_macro: 0.3580 - loss: 4.0176 - val_accuracy: 0.5459 - val_auc: 0.7868 - val_f1_macro: 0.4438 - val_loss: 4.0530\n\nEpoch 119: Learning Rate is 1.23e-06\nEpoch 119/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 152ms/step - accuracy: 0.4701 - auc: 0.7405 - f1_macro: 0.3706 - loss: 3.9142 - val_accuracy: 0.5467 - val_auc: 0.7872 - val_f1_macro: 0.4435 - val_loss: 4.0547\n\nEpoch 120: Learning Rate is 1.17e-06\nEpoch 120/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 158ms/step - accuracy: 0.4953 - auc: 0.7427 - f1_macro: 0.3631 - loss: 3.8987 - val_accuracy: 0.5471 - val_auc: 0.7873 - val_f1_macro: 0.4423 - val_loss: 4.0608\n\nEpoch 121: Learning Rate is 1.10e-06\nEpoch 121/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 170ms/step - accuracy: 0.4635 - auc: 0.7216 - f1_macro: 0.3462 - loss: 4.0917 - val_accuracy: 0.5465 - val_auc: 0.7877 - val_f1_macro: 0.4427 - val_loss: 4.0482\n\nEpoch 122: Learning Rate is 1.04e-06\nEpoch 122/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.4623 - auc: 0.7374 - f1_macro: 0.3388 - loss: 3.9014 - val_accuracy: 0.5476 - val_auc: 0.7877 - val_f1_macro: 0.4434 - val_loss: 4.0572\n\nEpoch 123: Learning Rate is 9.85e-07\nEpoch 123/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 147ms/step - accuracy: 0.4444 - auc: 0.7196 - f1_macro: 0.3379 - loss: 3.9804 - val_accuracy: 0.5493 - val_auc: 0.7874 - val_f1_macro: 0.4437 - val_loss: 4.0660\n\nEpoch 124: Learning Rate is 9.27e-07\nEpoch 124/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4453 - auc: 0.7280 - f1_macro: 0.3328 - loss: 3.9162 - val_accuracy: 0.5497 - val_auc: 0.7868 - val_f1_macro: 0.4420 - val_loss: 4.0725\n\nEpoch 125: Learning Rate is 8.71e-07\nEpoch 125/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 152ms/step - accuracy: 0.4582 - auc: 0.7305 - f1_macro: 0.3399 - loss: 3.8590 - val_accuracy: 0.5493 - val_auc: 0.7867 - val_f1_macro: 0.4421 - val_loss: 4.0813\n\nEpoch 126: Learning Rate is 8.16e-07\nEpoch 126/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 140ms/step - accuracy: 0.4724 - auc: 0.7375 - f1_macro: 0.3322 - loss: 3.8494 - val_accuracy: 0.5493 - val_auc: 0.7871 - val_f1_macro: 0.4424 - val_loss: 4.0787\n\nEpoch 127: Learning Rate is 7.62e-07\nEpoch 127/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 150ms/step - accuracy: 0.4575 - auc: 0.7242 - f1_macro: 0.3301 - loss: 3.9115 - val_accuracy: 0.5489 - val_auc: 0.7877 - val_f1_macro: 0.4410 - val_loss: 4.0749\n\nEpoch 128: Learning Rate is 7.10e-07\nEpoch 128/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 185ms/step - accuracy: 0.4840 - auc: 0.7351 - f1_macro: 0.3337 - loss: 3.7943 - val_accuracy: 0.5497 - val_auc: 0.7866 - val_f1_macro: 0.4422 - val_loss: 4.0816\n\nEpoch 129: Learning Rate is 6.59e-07\nEpoch 129/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 156ms/step - accuracy: 0.4575 - auc: 0.7342 - f1_macro: 0.4515 - loss: 3.8901 - val_accuracy: 0.5482 - val_auc: 0.7879 - val_f1_macro: 0.4423 - val_loss: 4.0644\n\nEpoch 130: Learning Rate is 6.10e-07\nEpoch 130/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5236 - auc: 0.7716 - f1_macro: 0.5088 - loss: 3.4604 - val_accuracy: 0.5478 - val_auc: 0.7887 - val_f1_macro: 0.4428 - val_loss: 4.0462\n\nEpoch 131: Learning Rate is 5.62e-07\nEpoch 131/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5470 - auc: 0.7735 - f1_macro: 0.5151 - loss: 3.5723 - val_accuracy: 0.5480 - val_auc: 0.7881 - val_f1_macro: 0.4447 - val_loss: 4.0433\n\nEpoch 132: Learning Rate is 5.16e-07\nEpoch 132/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4994 - auc: 0.7730 - f1_macro: 0.4806 - loss: 3.3787 - val_accuracy: 0.5461 - val_auc: 0.7877 - val_f1_macro: 0.4435 - val_loss: 4.0347\n\nEpoch 133: Learning Rate is 4.72e-07\nEpoch 133/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 158ms/step - accuracy: 0.5515 - auc: 0.7948 - f1_macro: 0.4895 - loss: 3.2574 - val_accuracy: 0.5441 - val_auc: 0.7874 - val_f1_macro: 0.4417 - val_loss: 4.0383\n\nEpoch 134: Learning Rate is 4.30e-07\nEpoch 134/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.5808 - auc: 0.8097 - f1_macro: 0.5081 - loss: 3.0835 - val_accuracy: 0.5439 - val_auc: 0.7873 - val_f1_macro: 0.4434 - val_loss: 4.0286\n\nEpoch 135: Learning Rate is 3.89e-07\nEpoch 135/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 174ms/step - accuracy: 0.5748 - auc: 0.8032 - f1_macro: 0.5080 - loss: 3.1785 - val_accuracy: 0.5439 - val_auc: 0.7863 - val_f1_macro: 0.4444 - val_loss: 4.0263\n\nEpoch 136: Learning Rate is 3.50e-07\nEpoch 136/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5819 - auc: 0.8185 - f1_macro: 0.4958 - loss: 3.0291 - val_accuracy: 0.5419 - val_auc: 0.7865 - val_f1_macro: 0.4435 - val_loss: 4.0105\n\nEpoch 137: Learning Rate is 3.13e-07\nEpoch 137/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 150ms/step - accuracy: 0.6070 - auc: 0.8244 - f1_macro: 0.5189 - loss: 2.9167 - val_accuracy: 0.5426 - val_auc: 0.7866 - val_f1_macro: 0.4457 - val_loss: 4.0049\n\nEpoch 138: Learning Rate is 2.78e-07\nEpoch 138/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.5848 - auc: 0.8186 - f1_macro: 0.4984 - loss: 3.0129 - val_accuracy: 0.5411 - val_auc: 0.7854 - val_f1_macro: 0.4471 - val_loss: 3.9938\n\nEpoch 139: Learning Rate is 2.45e-07\nEpoch 139/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.6358 - auc: 0.8410 - f1_macro: 0.5235 - loss: 2.7210 - val_accuracy: 0.5419 - val_auc: 0.7847 - val_f1_macro: 0.4471 - val_loss: 3.9995\n\nEpoch 140: Learning Rate is 2.14e-07\nEpoch 140/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 151ms/step - accuracy: 0.6390 - auc: 0.8481 - f1_macro: 0.5258 - loss: 2.6284 - val_accuracy: 0.5419 - val_auc: 0.7848 - val_f1_macro: 0.4491 - val_loss: 3.9914\n\nEpoch 141: Learning Rate is 1.85e-07\nEpoch 141/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 156ms/step - accuracy: 0.6204 - auc: 0.8414 - f1_macro: 0.5050 - loss: 2.8095 - val_accuracy: 0.5413 - val_auc: 0.7842 - val_f1_macro: 0.4531 - val_loss: 3.9731\n\nEpoch 142: Learning Rate is 1.58e-07\nEpoch 142/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 169ms/step - accuracy: 0.6514 - auc: 0.8479 - f1_macro: 0.5228 - loss: 2.6261 - val_accuracy: 0.5389 - val_auc: 0.7831 - val_f1_macro: 0.4527 - val_loss: 3.9695\n\nEpoch 143: Learning Rate is 1.33e-07\nEpoch 143/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 147ms/step - accuracy: 0.6497 - auc: 0.8550 - f1_macro: 0.5197 - loss: 2.5196 - val_accuracy: 0.5391 - val_auc: 0.7819 - val_f1_macro: 0.4529 - val_loss: 3.9800\n\nEpoch 144: Learning Rate is 1.10e-07\nEpoch 144/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 158ms/step - accuracy: 0.6501 - auc: 0.8540 - f1_macro: 0.5018 - loss: 2.5945 - val_accuracy: 0.5372 - val_auc: 0.7819 - val_f1_macro: 0.4527 - val_loss: 3.9685\nEpoch 144: early stopping\nRestoring model weights from the end of the best epoch: 69.\nĐã lưu mô hình cho Fold 1 tại: /kaggle/working/output_results/EfficienetB0_CV_TPU_fold_1.keras\nĐang tạo và lưu biểu đồ huấn luyện...\nĐã lưu biểu đồ cho Fold 1 tại: /kaggle/working/output_results/fold_1_metrics.png\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756902889.220986      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:17898631096700773813\nI0000 00:00:1756902890.596311     936 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(4469592503015015487), session_name()\nI0000 00:00:1756902902.635060     936 tpu_compile_op_common.cc:245] Compilation of 4469592503015015487 with session name  took 12.038585857s and succeeded\nI0000 00:00:1756902902.648940     936 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(4469592503015015487), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_17898631096700773813\", property.function_library_fingerprint = 5347424050414948715, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756902902.648975     936 tpu_compilation_cache_interface.cc:542] After adding entry for key 4469592503015015487 with session_name  cache is 7 entries (459429554 bytes),  marked for eviction 0 entries (0 bytes).\nI0000 00:00:1756902904.632761     914 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(18435700263821255356), session_name()\nI0000 00:00:1756902916.619739     914 tpu_compile_op_common.cc:245] Compilation of 18435700263821255356 with session name  took 11.986925776s and succeeded\nI0000 00:00:1756902916.637563     914 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(18435700263821255356), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_17898631096700773813\", property.function_library_fingerprint = 5347424050414948715, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"29,224,224,3,;29,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756902916.637598     914 tpu_compilation_cache_interface.cc:542] After adding entry for key 18435700263821255356 with session_name  cache is 8 entries (486881417 bytes),  marked for eviction 0 entries (0 bytes).\n/usr/local/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self._interrupted_warning()\n","output_type":"stream"},{"name":"stdout","text":"Fold 1 - Validation Loss: 4.1931, Validation Accuracy: 0.5283, Validation AUC: 0.7646, Validation F1-Macro: 0.4780\n==================================================\nKết quả Cross-Validation:\nValidation Accuracy trung bình: 0.5283 +/- 0.0000\nValidation Loss trung bình: 4.1931 +/- 0.0000\nValidation AUC trung bình: 0.7646 +/- 0.0000\nValidation F1-Macro trung bình: 0.4780 +/- 0.0000\n==================================================\n--------------------------------------------------\nBắt đầu Fold 2/5\n--------------------------------------------------\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fd0d45a5360> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd0d45a5360>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fd0d45a5360> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd0d45a5360>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function <lambda> at 0x7fd0d45a5360> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd0d45a5360>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fd0187a4940> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd0187a4940>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fd0187a4940> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd0187a4940>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function <lambda> at 0x7fd0187a4940> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd0187a4940>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fd82090b760> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd82090b760>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fd82090b760> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd82090b760>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function <lambda> at 0x7fd82090b760> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd82090b760>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fcac8a992d0> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcac8a992d0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fcac8a992d0> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcac8a992d0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function <lambda> at 0x7fcac8a992d0> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcac8a992d0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nSố bước mỗi epoch: 87 | Số bước validation: 22\n\n--- Bắt đầu Giai đoạn 1: Huấn luyện các lớp cuối ---\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756902934.910501      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:14434140049553276459\nI0000 00:00:1756902937.594782     888 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(5394205676549373105), session_name()\nI0000 00:00:1756902961.826418     888 tpu_compile_op_common.cc:245] Compilation of 5394205676549373105 with session name  took 24.231262716s and succeeded\nI0000 00:00:1756902961.898342     888 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(5394205676549373105), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_14434140049553276459\", property.function_library_fingerprint = 4636478008316618456, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756902961.898389     888 tpu_compilation_cache_interface.cc:542] After adding entry for key 5394205676549373105 with session_name  cache is 9 entries (563318446 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.6822 - auc: 0.8077 - loss: 6.6236","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756902976.421896      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:4688696284065735243\nI0000 00:00:1756902977.663835     964 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(843976407768711874), session_name()\nI0000 00:00:1756902989.630388     964 tpu_compile_op_common.cc:245] Compilation of 843976407768711874 with session name  took 11.96643064s and succeeded\nI0000 00:00:1756902989.647219     964 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(843976407768711874), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_4688696284065735243\", property.function_library_fingerprint = 5731633436769555765, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756902989.647254     964 tpu_compilation_cache_interface.cc:542] After adding entry for key 843976407768711874 with session_name  cache is 10 entries (589965390 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 360ms/step - accuracy: 0.6812 - auc: 0.8070 - loss: 6.6479 - val_accuracy: 0.3404 - val_auc: 0.5595 - val_loss: 20.3604\nEpoch 2/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 134ms/step - accuracy: 0.3424 - auc: 0.5649 - loss: 15.0856 - val_accuracy: 0.4077 - val_auc: 0.6098 - val_loss: 17.7443\nEpoch 3/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.3215 - auc: 0.5611 - loss: 13.9826 - val_accuracy: 0.4134 - val_auc: 0.6114 - val_loss: 18.3719\nEpoch 4/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 132ms/step - accuracy: 0.3467 - auc: 0.5780 - loss: 13.9129 - val_accuracy: 0.4233 - val_auc: 0.6218 - val_loss: 17.2486\nEpoch 5/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 142ms/step - accuracy: 0.3836 - auc: 0.6042 - loss: 12.4127 - val_accuracy: 0.4299 - val_auc: 0.6247 - val_loss: 16.5585\n\n--- Bắt đầu Giai đoạn 2: Fine-tuning ---\nTổng số lớp trong base model: 238\nSẽ mở băng và huấn luyện 119 lớp cuối cùng (bắt đầu từ lớp 119).\n\n--- Giai đoạn 2A: Bắt đầu Warmup trong 3 epochs ---\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756903063.077355      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:18129966825554828530\nI0000 00:00:1756903067.978841     885 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(5455365994395678337), session_name()\nI0000 00:00:1756903101.448072     885 tpu_compile_op_common.cc:245] Compilation of 5455365994395678337 with session name  took 33.468873626s and succeeded\nI0000 00:00:1756903101.540407     885 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(5455365994395678337), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_18129966825554828530\", property.function_library_fingerprint = 906974520687712946, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756903101.540468     885 tpu_compilation_cache_interface.cc:542] After adding entry for key 5455365994395678337 with session_name  cache is 11 entries (726960723 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.3802 - f1_macro: 0.2018 - loss: 6.8232","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756903117.909203      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:15509369078390245102\nI0000 00:00:1756903119.086953     880 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(7394058545066967456), session_name()\nI0000 00:00:1756903131.305872     880 tpu_compile_op_common.cc:245] Compilation of 7394058545066967456 with session name  took 12.218795845s and succeeded\nI0000 00:00:1756903131.324768     880 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(7394058545066967456), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_15509369078390245102\", property.function_library_fingerprint = 17040134991380135654, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756903131.324804     880 tpu_compilation_cache_interface.cc:542] After adding entry for key 7394058545066967456 with session_name  cache is 12 entries (752749322 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 369ms/step - accuracy: 0.3787 - f1_macro: 0.2019 - loss: 6.8495 - val_accuracy: 0.3026 - val_f1_macro: 0.2658 - val_loss: 6.7606\nEpoch 2/3\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.3551 - f1_macro: 0.2007 - loss: 7.0242 - val_accuracy: 0.2495 - val_f1_macro: 0.2296 - val_loss: 6.9152\nEpoch 3/3\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.3565 - f1_macro: 0.2076 - loss: 6.8219 - val_accuracy: 0.2518 - val_f1_macro: 0.1846 - val_loss: 7.6700\n\n--- Giai đoạn 2B: Bắt đầu huấn luyện chính ---\nSử dụng scheduler: CosineDecayRestarts\n\nEpoch 4: Learning Rate is 5.00e-06\nEpoch 4/200\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756903185.505289      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:16425938742490531465\nI0000 00:00:1756903191.406712     914 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(6475342849144595255), session_name()\nI0000 00:00:1756903226.554207     914 tpu_compile_op_common.cc:245] Compilation of 6475342849144595255 with session name  took 35.147230222s and succeeded\nI0000 00:00:1756903226.648722     914 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(6475342849144595255), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_16425938742490531465\", property.function_library_fingerprint = 15917191020655831201, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756903226.648794     914 tpu_compilation_cache_interface.cc:542] After adding entry for key 6475342849144595255 with session_name  cache is 13 entries (891715045 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.4115 - auc: 0.6382 - f1_macro: 0.2072 - loss: 6.4920","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756903242.694330      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:11318304230418999461\nI0000 00:00:1756903243.885819     920 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(10267364847299539812), session_name()\nI0000 00:00:1756903256.204151     920 tpu_compile_op_common.cc:245] Compilation of 10267364847299539812 with session name  took 12.318092916s and succeeded\nI0000 00:00:1756903256.222270     920 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(10267364847299539812), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_11318304230418999461\", property.function_library_fingerprint = 10999948571953657253, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756903256.222305     920 tpu_compilation_cache_interface.cc:542] After adding entry for key 10267364847299539812 with session_name  cache is 14 entries (919239124 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 374ms/step - accuracy: 0.4097 - auc: 0.6365 - f1_macro: 0.2072 - loss: 6.5159 - val_accuracy: 0.3509 - val_auc: 0.5789 - val_f1_macro: 0.3199 - val_loss: 6.1547\n\nEpoch 5: Learning Rate is 5.00e-06\nEpoch 5/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.3778 - auc: 0.6209 - f1_macro: 0.2333 - loss: 6.2261 - val_accuracy: 0.4043 - val_auc: 0.6137 - val_f1_macro: 0.3738 - val_loss: 5.7628\n\nEpoch 6: Learning Rate is 4.98e-06\nEpoch 6/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 170ms/step - accuracy: 0.3596 - auc: 0.5993 - f1_macro: 0.2565 - loss: 6.2090 - val_accuracy: 0.4300 - val_auc: 0.6240 - val_f1_macro: 0.3923 - val_loss: 5.6128\n\nEpoch 7: Learning Rate is 4.96e-06\nEpoch 7/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 180ms/step - accuracy: 0.3403 - auc: 0.5914 - f1_macro: 0.2535 - loss: 6.3533 - val_accuracy: 0.4412 - val_auc: 0.6318 - val_f1_macro: 0.3868 - val_loss: 5.5124\n\nEpoch 8: Learning Rate is 4.92e-06\nEpoch 8/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.3373 - auc: 0.5821 - f1_macro: 0.2777 - loss: 6.2161 - val_accuracy: 0.4512 - val_auc: 0.6405 - val_f1_macro: 0.3863 - val_loss: 5.4604\n\nEpoch 9: Learning Rate is 4.88e-06\nEpoch 9/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.3756 - auc: 0.6181 - f1_macro: 0.3165 - loss: 5.9727 - val_accuracy: 0.4485 - val_auc: 0.6427 - val_f1_macro: 0.3814 - val_loss: 5.4194\n\nEpoch 10: Learning Rate is 4.82e-06\nEpoch 10/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.3794 - auc: 0.6240 - f1_macro: 0.3149 - loss: 5.8190 - val_accuracy: 0.4508 - val_auc: 0.6451 - val_f1_macro: 0.3779 - val_loss: 5.4025\n\nEpoch 11: Learning Rate is 4.76e-06\nEpoch 11/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.3743 - auc: 0.6033 - f1_macro: 0.3219 - loss: 6.0285 - val_accuracy: 0.4533 - val_auc: 0.6507 - val_f1_macro: 0.3718 - val_loss: 5.3633\n\nEpoch 12: Learning Rate is 4.69e-06\nEpoch 12/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4035 - auc: 0.6451 - f1_macro: 0.3400 - loss: 5.5578 - val_accuracy: 0.4554 - val_auc: 0.6577 - val_f1_macro: 0.3708 - val_loss: 5.3166\n\nEpoch 13: Learning Rate is 4.61e-06\nEpoch 13/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.4182 - auc: 0.6449 - f1_macro: 0.3561 - loss: 5.4221 - val_accuracy: 0.4567 - val_auc: 0.6604 - val_f1_macro: 0.3678 - val_loss: 5.2948\n\nEpoch 14: Learning Rate is 4.52e-06\nEpoch 14/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 177ms/step - accuracy: 0.4474 - auc: 0.6707 - f1_macro: 0.3699 - loss: 5.1930 - val_accuracy: 0.4634 - val_auc: 0.6691 - val_f1_macro: 0.3735 - val_loss: 5.2419\n\nEpoch 15: Learning Rate is 4.43e-06\nEpoch 15/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.4242 - auc: 0.6700 - f1_macro: 0.3518 - loss: 5.2293 - val_accuracy: 0.4645 - val_auc: 0.6711 - val_f1_macro: 0.3733 - val_loss: 5.2157\n\nEpoch 16: Learning Rate is 4.32e-06\nEpoch 16/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 154ms/step - accuracy: 0.4797 - auc: 0.6869 - f1_macro: 0.3892 - loss: 4.9759 - val_accuracy: 0.4668 - val_auc: 0.6754 - val_f1_macro: 0.3754 - val_loss: 5.1752\n\nEpoch 17: Learning Rate is 4.21e-06\nEpoch 17/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 178ms/step - accuracy: 0.4720 - auc: 0.6897 - f1_macro: 0.3892 - loss: 4.9039 - val_accuracy: 0.4686 - val_auc: 0.6778 - val_f1_macro: 0.3783 - val_loss: 5.1398\n\nEpoch 18: Learning Rate is 4.09e-06\nEpoch 18/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5027 - auc: 0.7142 - f1_macro: 0.4046 - loss: 4.6763 - val_accuracy: 0.4684 - val_auc: 0.6804 - val_f1_macro: 0.3761 - val_loss: 5.1157\n\nEpoch 19: Learning Rate is 3.97e-06\nEpoch 19/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 152ms/step - accuracy: 0.5014 - auc: 0.7240 - f1_macro: 0.3968 - loss: 4.5584 - val_accuracy: 0.4721 - val_auc: 0.6850 - val_f1_macro: 0.3804 - val_loss: 5.0799\n\nEpoch 20: Learning Rate is 3.84e-06\nEpoch 20/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.5219 - auc: 0.7368 - f1_macro: 0.4183 - loss: 4.3691 - val_accuracy: 0.4739 - val_auc: 0.6891 - val_f1_macro: 0.3819 - val_loss: 5.0391\n\nEpoch 21: Learning Rate is 3.70e-06\nEpoch 21/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 174ms/step - accuracy: 0.5225 - auc: 0.7337 - f1_macro: 0.4085 - loss: 4.4229 - val_accuracy: 0.4743 - val_auc: 0.6910 - val_f1_macro: 0.3864 - val_loss: 4.9975\n\nEpoch 22: Learning Rate is 3.56e-06\nEpoch 22/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.5375 - auc: 0.7518 - f1_macro: 0.4280 - loss: 4.0900 - val_accuracy: 0.4783 - val_auc: 0.6943 - val_f1_macro: 0.3930 - val_loss: 4.9555\n\nEpoch 23: Learning Rate is 3.42e-06\nEpoch 23/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5240 - auc: 0.7436 - f1_macro: 0.4026 - loss: 4.2170 - val_accuracy: 0.4805 - val_auc: 0.6984 - val_f1_macro: 0.3958 - val_loss: 4.9345\n\nEpoch 24: Learning Rate is 3.27e-06\nEpoch 24/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 171ms/step - accuracy: 0.5704 - auc: 0.7711 - f1_macro: 0.4490 - loss: 3.9709 - val_accuracy: 0.4833 - val_auc: 0.7038 - val_f1_macro: 0.4006 - val_loss: 4.8826\n\nEpoch 25: Learning Rate is 3.12e-06\nEpoch 25/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 169ms/step - accuracy: 0.5487 - auc: 0.7656 - f1_macro: 0.4202 - loss: 3.9184 - val_accuracy: 0.4853 - val_auc: 0.7054 - val_f1_macro: 0.4022 - val_loss: 4.8613\n\nEpoch 26: Learning Rate is 2.97e-06\nEpoch 26/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 154ms/step - accuracy: 0.5796 - auc: 0.7850 - f1_macro: 0.4362 - loss: 3.7846 - val_accuracy: 0.4883 - val_auc: 0.7078 - val_f1_macro: 0.4081 - val_loss: 4.8336\n\nEpoch 27: Learning Rate is 2.81e-06\nEpoch 27/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.5702 - auc: 0.7777 - f1_macro: 0.4284 - loss: 3.8056 - val_accuracy: 0.4895 - val_auc: 0.7077 - val_f1_macro: 0.4121 - val_loss: 4.8231\n\nEpoch 28: Learning Rate is 2.66e-06\nEpoch 28/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.6059 - auc: 0.8009 - f1_macro: 0.4529 - loss: 3.5069 - val_accuracy: 0.4922 - val_auc: 0.7078 - val_f1_macro: 0.4180 - val_loss: 4.7995\n\nEpoch 29: Learning Rate is 2.50e-06\nEpoch 29/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.6021 - auc: 0.8040 - f1_macro: 0.4521 - loss: 3.5124 - val_accuracy: 0.4934 - val_auc: 0.7099 - val_f1_macro: 0.4225 - val_loss: 4.7741\n\nEpoch 30: Learning Rate is 2.34e-06\nEpoch 30/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 151ms/step - accuracy: 0.6009 - auc: 0.7982 - f1_macro: 0.4454 - loss: 3.5258 - val_accuracy: 0.4941 - val_auc: 0.7099 - val_f1_macro: 0.4281 - val_loss: 4.7585\n\nEpoch 31: Learning Rate is 2.19e-06\nEpoch 31/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.6182 - auc: 0.8150 - f1_macro: 0.4536 - loss: 3.2824 - val_accuracy: 0.4963 - val_auc: 0.7106 - val_f1_macro: 0.4319 - val_loss: 4.7518\n\nEpoch 32: Learning Rate is 2.03e-06\nEpoch 32/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 147ms/step - accuracy: 0.6312 - auc: 0.8092 - f1_macro: 0.4600 - loss: 3.3618 - val_accuracy: 0.4970 - val_auc: 0.7126 - val_f1_macro: 0.4309 - val_loss: 4.7369\n\nEpoch 33: Learning Rate is 1.88e-06\nEpoch 33/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 144ms/step - accuracy: 0.6291 - auc: 0.8209 - f1_macro: 0.4568 - loss: 3.3074 - val_accuracy: 0.4991 - val_auc: 0.7119 - val_f1_macro: 0.4363 - val_loss: 4.7215\n\nEpoch 34: Learning Rate is 1.73e-06\nEpoch 34/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.6466 - auc: 0.8324 - f1_macro: 0.4683 - loss: 3.1199 - val_accuracy: 0.5027 - val_auc: 0.7105 - val_f1_macro: 0.4427 - val_loss: 4.7208\n\nEpoch 35: Learning Rate is 1.58e-06\nEpoch 35/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 149ms/step - accuracy: 0.6309 - auc: 0.8217 - f1_macro: 0.4515 - loss: 3.2039 - val_accuracy: 0.4988 - val_auc: 0.7091 - val_f1_macro: 0.4392 - val_loss: 4.7116\n\nEpoch 36: Learning Rate is 1.44e-06\nEpoch 36/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 145ms/step - accuracy: 0.6509 - auc: 0.8350 - f1_macro: 0.4627 - loss: 3.1394 - val_accuracy: 0.5004 - val_auc: 0.7100 - val_f1_macro: 0.4427 - val_loss: 4.7040\n\nEpoch 37: Learning Rate is 1.30e-06\nEpoch 37/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 152ms/step - accuracy: 0.6391 - auc: 0.8270 - f1_macro: 0.4528 - loss: 3.1885 - val_accuracy: 0.5011 - val_auc: 0.7104 - val_f1_macro: 0.4481 - val_loss: 4.6898\n\nEpoch 38: Learning Rate is 1.16e-06\nEpoch 38/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.6477 - auc: 0.8356 - f1_macro: 0.4555 - loss: 3.0986 - val_accuracy: 0.4989 - val_auc: 0.7085 - val_f1_macro: 0.4486 - val_loss: 4.6810\n\nEpoch 39: Learning Rate is 1.03e-06\nEpoch 39/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.6462 - auc: 0.8334 - f1_macro: 0.4549 - loss: 3.1421 - val_accuracy: 0.5009 - val_auc: 0.7095 - val_f1_macro: 0.4519 - val_loss: 4.6610\n\nEpoch 40: Learning Rate is 9.06e-07\nEpoch 40/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.6654 - auc: 0.8451 - f1_macro: 0.4779 - loss: 2.9845 - val_accuracy: 0.4995 - val_auc: 0.7086 - val_f1_macro: 0.4509 - val_loss: 4.6622\n\nEpoch 41: Learning Rate is 7.89e-07\nEpoch 41/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 169ms/step - accuracy: 0.6753 - auc: 0.8560 - f1_macro: 0.4678 - loss: 2.8128 - val_accuracy: 0.4993 - val_auc: 0.7067 - val_f1_macro: 0.4539 - val_loss: 4.6673\n\nEpoch 42: Learning Rate is 6.78e-07\nEpoch 42/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 152ms/step - accuracy: 0.6785 - auc: 0.8533 - f1_macro: 0.4634 - loss: 2.8214 - val_accuracy: 0.5002 - val_auc: 0.7070 - val_f1_macro: 0.4555 - val_loss: 4.6659\n\nEpoch 43: Learning Rate is 5.74e-07\nEpoch 43/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 149ms/step - accuracy: 0.6817 - auc: 0.8579 - f1_macro: 0.4623 - loss: 2.7774 - val_accuracy: 0.5041 - val_auc: 0.7066 - val_f1_macro: 0.4598 - val_loss: 4.6604\n\nEpoch 44: Learning Rate is 4.77e-07\nEpoch 44/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 141ms/step - accuracy: 0.6793 - auc: 0.8524 - f1_macro: 0.4645 - loss: 2.8568 - val_accuracy: 0.5025 - val_auc: 0.7074 - val_f1_macro: 0.4589 - val_loss: 4.6481\n\nEpoch 45: Learning Rate is 3.89e-07\nEpoch 45/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.6906 - auc: 0.8688 - f1_macro: 0.4705 - loss: 2.6442 - val_accuracy: 0.4986 - val_auc: 0.7053 - val_f1_macro: 0.4572 - val_loss: 4.6565\n\nEpoch 46: Learning Rate is 3.09e-07\nEpoch 46/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.6669 - auc: 0.8514 - f1_macro: 0.4610 - loss: 2.8877 - val_accuracy: 0.4973 - val_auc: 0.7022 - val_f1_macro: 0.4585 - val_loss: 4.6645\n\nEpoch 47: Learning Rate is 2.38e-07\nEpoch 47/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.6969 - auc: 0.8674 - f1_macro: 0.4613 - loss: 2.6373 - val_accuracy: 0.4996 - val_auc: 0.7025 - val_f1_macro: 0.4597 - val_loss: 4.6649\n\nEpoch 48: Learning Rate is 1.76e-07\nEpoch 48/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.6856 - auc: 0.8572 - f1_macro: 0.4699 - loss: 2.8187 - val_accuracy: 0.4972 - val_auc: 0.7022 - val_f1_macro: 0.4600 - val_loss: 4.6647\n\nEpoch 49: Learning Rate is 1.22e-07\nEpoch 49/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 153ms/step - accuracy: 0.6928 - auc: 0.8586 - f1_macro: 0.4734 - loss: 2.8067 - val_accuracy: 0.4980 - val_auc: 0.7006 - val_f1_macro: 0.4619 - val_loss: 4.6633\n\nEpoch 50: Learning Rate is 7.85e-08\nEpoch 50/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 150ms/step - accuracy: 0.6885 - auc: 0.8518 - f1_macro: 0.4725 - loss: 2.9249 - val_accuracy: 0.4975 - val_auc: 0.6986 - val_f1_macro: 0.4640 - val_loss: 4.6731\n\nEpoch 51: Learning Rate is 4.43e-08\nEpoch 51/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 154ms/step - accuracy: 0.6915 - auc: 0.8614 - f1_macro: 0.4718 - loss: 2.7702 - val_accuracy: 0.4977 - val_auc: 0.6986 - val_f1_macro: 0.4643 - val_loss: 4.6721\n\nEpoch 52: Learning Rate is 1.97e-08\nEpoch 52/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 181ms/step - accuracy: 0.7039 - auc: 0.8687 - f1_macro: 0.4780 - loss: 2.6101 - val_accuracy: 0.4984 - val_auc: 0.7000 - val_f1_macro: 0.4648 - val_loss: 4.6654\n\nEpoch 53: Learning Rate is 4.93e-09\nEpoch 53/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 153ms/step - accuracy: 0.7080 - auc: 0.8707 - f1_macro: 0.4859 - loss: 2.5941 - val_accuracy: 0.4963 - val_auc: 0.6988 - val_f1_macro: 0.4630 - val_loss: 4.6706\n\nEpoch 54: Learning Rate is 4.50e-06\nEpoch 54/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.6956 - auc: 0.8591 - f1_macro: 0.4691 - loss: 2.7063 - val_accuracy: 0.4650 - val_auc: 0.6768 - val_f1_macro: 0.4453 - val_loss: 4.7350\n\nEpoch 55: Learning Rate is 4.50e-06\nEpoch 55/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 154ms/step - accuracy: 0.6647 - auc: 0.8422 - f1_macro: 0.4559 - loss: 2.9617 - val_accuracy: 0.4695 - val_auc: 0.6793 - val_f1_macro: 0.4489 - val_loss: 4.7139\n\nEpoch 56: Learning Rate is 4.50e-06\nEpoch 56/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.6683 - auc: 0.8445 - f1_macro: 0.4680 - loss: 3.0472 - val_accuracy: 0.4561 - val_auc: 0.6734 - val_f1_macro: 0.4389 - val_loss: 4.7443\n\nEpoch 57: Learning Rate is 4.49e-06\nEpoch 57/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.6513 - auc: 0.8329 - f1_macro: 0.4537 - loss: 3.2478 - val_accuracy: 0.4615 - val_auc: 0.6793 - val_f1_macro: 0.4425 - val_loss: 4.6899\n\nEpoch 58: Learning Rate is 4.48e-06\nEpoch 58/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 152ms/step - accuracy: 0.6714 - auc: 0.8425 - f1_macro: 0.4586 - loss: 3.0213 - val_accuracy: 0.4723 - val_auc: 0.6849 - val_f1_macro: 0.4531 - val_loss: 4.6213\n\nEpoch 59: Learning Rate is 4.47e-06\nEpoch 59/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.6317 - auc: 0.8259 - f1_macro: 0.4481 - loss: 3.2076 - val_accuracy: 0.4716 - val_auc: 0.6881 - val_f1_macro: 0.4508 - val_loss: 4.6011\n\nEpoch 60: Learning Rate is 4.46e-06\nEpoch 60/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 153ms/step - accuracy: 0.6686 - auc: 0.8489 - f1_macro: 0.4716 - loss: 2.8190 - val_accuracy: 0.4631 - val_auc: 0.6822 - val_f1_macro: 0.4462 - val_loss: 4.6105\n\nEpoch 61: Learning Rate is 4.45e-06\nEpoch 61/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.6617 - auc: 0.8467 - f1_macro: 0.4553 - loss: 2.9591 - val_accuracy: 0.4755 - val_auc: 0.6903 - val_f1_macro: 0.4579 - val_loss: 4.5415\n\nEpoch 62: Learning Rate is 4.43e-06\nEpoch 62/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.6401 - auc: 0.8242 - f1_macro: 0.4599 - loss: 3.2856 - val_accuracy: 0.4799 - val_auc: 0.6929 - val_f1_macro: 0.4616 - val_loss: 4.5149\n\nEpoch 63: Learning Rate is 4.41e-06\nEpoch 63/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.6777 - auc: 0.8455 - f1_macro: 0.4805 - loss: 2.9227 - val_accuracy: 0.4703 - val_auc: 0.6913 - val_f1_macro: 0.4529 - val_loss: 4.5217\n\nEpoch 64: Learning Rate is 4.39e-06\nEpoch 64/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.6442 - auc: 0.8408 - f1_macro: 0.4534 - loss: 2.8537 - val_accuracy: 0.4785 - val_auc: 0.6942 - val_f1_macro: 0.4583 - val_loss: 4.5159\n\nEpoch 65: Learning Rate is 4.37e-06\nEpoch 65/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 154ms/step - accuracy: 0.6220 - auc: 0.8166 - f1_macro: 0.4449 - loss: 3.1257 - val_accuracy: 0.4721 - val_auc: 0.6952 - val_f1_macro: 0.4526 - val_loss: 4.5206\n\nEpoch 66: Learning Rate is 4.34e-06\nEpoch 66/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 171ms/step - accuracy: 0.6047 - auc: 0.8182 - f1_macro: 0.4420 - loss: 3.1059 - val_accuracy: 0.4803 - val_auc: 0.6997 - val_f1_macro: 0.4605 - val_loss: 4.4607\n\nEpoch 67: Learning Rate is 4.31e-06\nEpoch 67/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 171ms/step - accuracy: 0.6061 - auc: 0.8319 - f1_macro: 0.4349 - loss: 2.9518 - val_accuracy: 0.4840 - val_auc: 0.6997 - val_f1_macro: 0.4623 - val_loss: 4.4455\n\nEpoch 68: Learning Rate is 4.29e-06\nEpoch 68/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 177ms/step - accuracy: 0.6196 - auc: 0.8193 - f1_macro: 0.4495 - loss: 3.1419 - val_accuracy: 0.4963 - val_auc: 0.7097 - val_f1_macro: 0.4703 - val_loss: 4.3757\n\nEpoch 69: Learning Rate is 4.25e-06\nEpoch 69/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.6311 - auc: 0.8296 - f1_macro: 0.4536 - loss: 2.9307 - val_accuracy: 0.4945 - val_auc: 0.7081 - val_f1_macro: 0.4701 - val_loss: 4.3694\n\nEpoch 70: Learning Rate is 4.22e-06\nEpoch 70/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 180ms/step - accuracy: 0.6220 - auc: 0.8298 - f1_macro: 0.4469 - loss: 2.8857 - val_accuracy: 0.4952 - val_auc: 0.7085 - val_f1_macro: 0.4706 - val_loss: 4.3572\n\nEpoch 71: Learning Rate is 4.19e-06\nEpoch 71/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 167ms/step - accuracy: 0.6167 - auc: 0.8205 - f1_macro: 0.4393 - loss: 2.9671 - val_accuracy: 0.5094 - val_auc: 0.7175 - val_f1_macro: 0.4807 - val_loss: 4.2904\n\nEpoch 72: Learning Rate is 4.15e-06\nEpoch 72/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5883 - auc: 0.8228 - f1_macro: 0.4282 - loss: 2.9497 - val_accuracy: 0.5043 - val_auc: 0.7190 - val_f1_macro: 0.4743 - val_loss: 4.2887\n\nEpoch 73: Learning Rate is 4.11e-06\nEpoch 73/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.6178 - auc: 0.8279 - f1_macro: 0.4621 - loss: 2.8638 - val_accuracy: 0.4984 - val_auc: 0.7166 - val_f1_macro: 0.4674 - val_loss: 4.3107\n\nEpoch 74: Learning Rate is 4.07e-06\nEpoch 74/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.5733 - auc: 0.8095 - f1_macro: 0.4166 - loss: 3.0997 - val_accuracy: 0.5062 - val_auc: 0.7237 - val_f1_macro: 0.4748 - val_loss: 4.2470\n\nEpoch 75: Learning Rate is 4.03e-06\nEpoch 75/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.6084 - auc: 0.8230 - f1_macro: 0.4381 - loss: 2.9401 - val_accuracy: 0.5059 - val_auc: 0.7199 - val_f1_macro: 0.4759 - val_loss: 4.2349\n\nEpoch 76: Learning Rate is 3.98e-06\nEpoch 76/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.5926 - auc: 0.8082 - f1_macro: 0.4383 - loss: 3.0507 - val_accuracy: 0.5002 - val_auc: 0.7158 - val_f1_macro: 0.4730 - val_loss: 4.2752\n\nEpoch 77: Learning Rate is 3.94e-06\nEpoch 77/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 174ms/step - accuracy: 0.5713 - auc: 0.8112 - f1_macro: 0.4253 - loss: 2.9760 - val_accuracy: 0.5162 - val_auc: 0.7310 - val_f1_macro: 0.4810 - val_loss: 4.1784\n\nEpoch 78: Learning Rate is 3.89e-06\nEpoch 78/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5757 - auc: 0.8077 - f1_macro: 0.4239 - loss: 3.0140 - val_accuracy: 0.5133 - val_auc: 0.7302 - val_f1_macro: 0.4772 - val_loss: 4.1799\n\nEpoch 79: Learning Rate is 3.84e-06\nEpoch 79/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.5657 - auc: 0.7954 - f1_macro: 0.4189 - loss: 3.2288 - val_accuracy: 0.5156 - val_auc: 0.7311 - val_f1_macro: 0.4794 - val_loss: 4.1660\n\nEpoch 80: Learning Rate is 3.79e-06\nEpoch 80/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 178ms/step - accuracy: 0.6144 - auc: 0.8266 - f1_macro: 0.4500 - loss: 2.8288 - val_accuracy: 0.5240 - val_auc: 0.7369 - val_f1_macro: 0.4850 - val_loss: 4.1401\n\nEpoch 81: Learning Rate is 3.74e-06\nEpoch 81/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.5837 - auc: 0.8074 - f1_macro: 0.4281 - loss: 3.0288 - val_accuracy: 0.5284 - val_auc: 0.7391 - val_f1_macro: 0.4874 - val_loss: 4.1023\n\nEpoch 82: Learning Rate is 3.68e-06\nEpoch 82/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.6111 - auc: 0.8217 - f1_macro: 0.4378 - loss: 2.7892 - val_accuracy: 0.5305 - val_auc: 0.7429 - val_f1_macro: 0.4874 - val_loss: 4.0723\n\nEpoch 83: Learning Rate is 3.63e-06\nEpoch 83/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.5545 - auc: 0.7910 - f1_macro: 0.4278 - loss: 3.2382 - val_accuracy: 0.5360 - val_auc: 0.7470 - val_f1_macro: 0.4878 - val_loss: 4.0370\n\nEpoch 84: Learning Rate is 3.57e-06\nEpoch 84/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 187ms/step - accuracy: 0.5847 - auc: 0.8185 - f1_macro: 0.4285 - loss: 2.7466 - val_accuracy: 0.5373 - val_auc: 0.7478 - val_f1_macro: 0.4880 - val_loss: 4.0435\n\nEpoch 85: Learning Rate is 3.51e-06\nEpoch 85/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 177ms/step - accuracy: 0.5587 - auc: 0.7943 - f1_macro: 0.4336 - loss: 3.0972 - val_accuracy: 0.5408 - val_auc: 0.7477 - val_f1_macro: 0.4889 - val_loss: 4.0204\n\nEpoch 86: Learning Rate is 3.46e-06\nEpoch 86/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 176ms/step - accuracy: 0.5529 - auc: 0.7956 - f1_macro: 0.4166 - loss: 3.0599 - val_accuracy: 0.5355 - val_auc: 0.7462 - val_f1_macro: 0.4818 - val_loss: 4.0258\n\nEpoch 87: Learning Rate is 3.40e-06\nEpoch 87/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 167ms/step - accuracy: 0.5478 - auc: 0.7909 - f1_macro: 0.4105 - loss: 3.1257 - val_accuracy: 0.5328 - val_auc: 0.7474 - val_f1_macro: 0.4781 - val_loss: 4.0454\n\nEpoch 88: Learning Rate is 3.33e-06\nEpoch 88/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.5233 - auc: 0.7858 - f1_macro: 0.3885 - loss: 3.2487 - val_accuracy: 0.5433 - val_auc: 0.7499 - val_f1_macro: 0.4871 - val_loss: 4.0042\n\nEpoch 89: Learning Rate is 3.27e-06\nEpoch 89/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 150ms/step - accuracy: 0.5321 - auc: 0.7807 - f1_macro: 0.4024 - loss: 3.1437 - val_accuracy: 0.5421 - val_auc: 0.7514 - val_f1_macro: 0.4856 - val_loss: 3.9922\n\nEpoch 90: Learning Rate is 3.21e-06\nEpoch 90/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 145ms/step - accuracy: 0.5436 - auc: 0.7920 - f1_macro: 0.4063 - loss: 3.0417 - val_accuracy: 0.5380 - val_auc: 0.7501 - val_f1_macro: 0.4832 - val_loss: 4.0179\n\nEpoch 91: Learning Rate is 3.14e-06\nEpoch 91/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 177ms/step - accuracy: 0.5467 - auc: 0.7839 - f1_macro: 0.4063 - loss: 3.1160 - val_accuracy: 0.5453 - val_auc: 0.7532 - val_f1_macro: 0.4875 - val_loss: 3.9763\n\nEpoch 92: Learning Rate is 3.08e-06\nEpoch 92/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.5439 - auc: 0.7894 - f1_macro: 0.4165 - loss: 3.1515 - val_accuracy: 0.5460 - val_auc: 0.7552 - val_f1_macro: 0.4829 - val_loss: 3.9755\n\nEpoch 93: Learning Rate is 3.01e-06\nEpoch 93/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 145ms/step - accuracy: 0.5442 - auc: 0.7824 - f1_macro: 0.4183 - loss: 3.1863 - val_accuracy: 0.5440 - val_auc: 0.7566 - val_f1_macro: 0.4819 - val_loss: 3.9731\n\nEpoch 94: Learning Rate is 2.95e-06\nEpoch 94/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 172ms/step - accuracy: 0.5310 - auc: 0.7811 - f1_macro: 0.4088 - loss: 3.1602 - val_accuracy: 0.5458 - val_auc: 0.7550 - val_f1_macro: 0.4813 - val_loss: 3.9846\n\nEpoch 95: Learning Rate is 2.88e-06\nEpoch 95/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 154ms/step - accuracy: 0.5583 - auc: 0.7928 - f1_macro: 0.4381 - loss: 3.0660 - val_accuracy: 0.5405 - val_auc: 0.7560 - val_f1_macro: 0.4762 - val_loss: 3.9896\n\nEpoch 96: Learning Rate is 2.81e-06\nEpoch 96/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5236 - auc: 0.7757 - f1_macro: 0.4086 - loss: 3.2767 - val_accuracy: 0.5481 - val_auc: 0.7610 - val_f1_macro: 0.4792 - val_loss: 3.9291\n\nEpoch 97: Learning Rate is 2.74e-06\nEpoch 97/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.5379 - auc: 0.7819 - f1_macro: 0.4208 - loss: 3.1964 - val_accuracy: 0.5469 - val_auc: 0.7614 - val_f1_macro: 0.4764 - val_loss: 3.9307\n\nEpoch 98: Learning Rate is 2.67e-06\nEpoch 98/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 189ms/step - accuracy: 0.5266 - auc: 0.7771 - f1_macro: 0.4072 - loss: 3.3082 - val_accuracy: 0.5490 - val_auc: 0.7609 - val_f1_macro: 0.4779 - val_loss: 3.9331\n\nEpoch 99: Learning Rate is 2.60e-06\nEpoch 99/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 167ms/step - accuracy: 0.4923 - auc: 0.7628 - f1_macro: 0.3918 - loss: 3.4526 - val_accuracy: 0.5515 - val_auc: 0.7641 - val_f1_macro: 0.4761 - val_loss: 3.9085\n\nEpoch 100: Learning Rate is 2.53e-06\nEpoch 100/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.4982 - auc: 0.7742 - f1_macro: 0.4038 - loss: 3.2336 - val_accuracy: 0.5504 - val_auc: 0.7643 - val_f1_macro: 0.4758 - val_loss: 3.9240\n\nEpoch 101: Learning Rate is 2.46e-06\nEpoch 101/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 169ms/step - accuracy: 0.4982 - auc: 0.7669 - f1_macro: 0.3951 - loss: 3.3516 - val_accuracy: 0.5479 - val_auc: 0.7651 - val_f1_macro: 0.4671 - val_loss: 3.9287\n\nEpoch 102: Learning Rate is 2.39e-06\nEpoch 102/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.5120 - auc: 0.7734 - f1_macro: 0.3976 - loss: 3.2941 - val_accuracy: 0.5465 - val_auc: 0.7630 - val_f1_macro: 0.4643 - val_loss: 3.9360\n\nEpoch 103: Learning Rate is 2.32e-06\nEpoch 103/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.5095 - auc: 0.7698 - f1_macro: 0.4022 - loss: 3.3688 - val_accuracy: 0.5504 - val_auc: 0.7659 - val_f1_macro: 0.4676 - val_loss: 3.9080\n\nEpoch 104: Learning Rate is 2.25e-06\nEpoch 104/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.5085 - auc: 0.7743 - f1_macro: 0.3996 - loss: 3.2965 - val_accuracy: 0.5478 - val_auc: 0.7661 - val_f1_macro: 0.4614 - val_loss: 3.9128\n\nEpoch 105: Learning Rate is 2.18e-06\nEpoch 105/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 170ms/step - accuracy: 0.5077 - auc: 0.7717 - f1_macro: 0.4009 - loss: 3.3386 - val_accuracy: 0.5476 - val_auc: 0.7666 - val_f1_macro: 0.4614 - val_loss: 3.9218\n\nEpoch 106: Learning Rate is 2.11e-06\nEpoch 106/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 158ms/step - accuracy: 0.5053 - auc: 0.7678 - f1_macro: 0.4030 - loss: 3.4417 - val_accuracy: 0.5471 - val_auc: 0.7666 - val_f1_macro: 0.4582 - val_loss: 3.9171\n\nEpoch 107: Learning Rate is 2.04e-06\nEpoch 107/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 154ms/step - accuracy: 0.5187 - auc: 0.7733 - f1_macro: 0.4109 - loss: 3.3457 - val_accuracy: 0.5501 - val_auc: 0.7687 - val_f1_macro: 0.4561 - val_loss: 3.9197\n\nEpoch 108: Learning Rate is 1.97e-06\nEpoch 108/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 178ms/step - accuracy: 0.5166 - auc: 0.7713 - f1_macro: 0.4213 - loss: 3.3701 - val_accuracy: 0.5495 - val_auc: 0.7699 - val_f1_macro: 0.4565 - val_loss: 3.9147\n\nEpoch 109: Learning Rate is 1.90e-06\nEpoch 109/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.4871 - auc: 0.7633 - f1_macro: 0.3886 - loss: 3.4622 - val_accuracy: 0.5497 - val_auc: 0.7710 - val_f1_macro: 0.4540 - val_loss: 3.9008\n\nEpoch 110: Learning Rate is 1.83e-06\nEpoch 110/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.4823 - auc: 0.7657 - f1_macro: 0.3950 - loss: 3.6418 - val_accuracy: 0.5488 - val_auc: 0.7707 - val_f1_macro: 0.4514 - val_loss: 3.9268\n\nEpoch 111: Learning Rate is 1.76e-06\nEpoch 111/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 172ms/step - accuracy: 0.5272 - auc: 0.7713 - f1_macro: 0.4384 - loss: 3.5483 - val_accuracy: 0.5506 - val_auc: 0.7714 - val_f1_macro: 0.4514 - val_loss: 3.9035\n\nEpoch 112: Learning Rate is 1.69e-06\nEpoch 112/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 171ms/step - accuracy: 0.4816 - auc: 0.7617 - f1_macro: 0.3976 - loss: 3.5804 - val_accuracy: 0.5483 - val_auc: 0.7709 - val_f1_macro: 0.4496 - val_loss: 3.9114\n\nEpoch 113: Learning Rate is 1.62e-06\nEpoch 113/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4915 - auc: 0.7606 - f1_macro: 0.4091 - loss: 3.7235 - val_accuracy: 0.5490 - val_auc: 0.7720 - val_f1_macro: 0.4496 - val_loss: 3.9035\n\nEpoch 114: Learning Rate is 1.55e-06\nEpoch 114/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.5178 - auc: 0.7692 - f1_macro: 0.4209 - loss: 3.6145 - val_accuracy: 0.5502 - val_auc: 0.7723 - val_f1_macro: 0.4487 - val_loss: 3.9099\n\nEpoch 115: Learning Rate is 1.49e-06\nEpoch 115/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 183ms/step - accuracy: 0.4960 - auc: 0.7630 - f1_macro: 0.4133 - loss: 3.7100 - val_accuracy: 0.5499 - val_auc: 0.7733 - val_f1_macro: 0.4459 - val_loss: 3.9030\n\nEpoch 116: Learning Rate is 1.42e-06\nEpoch 116/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.4981 - auc: 0.7620 - f1_macro: 0.4143 - loss: 3.8108 - val_accuracy: 0.5526 - val_auc: 0.7722 - val_f1_macro: 0.4503 - val_loss: 3.9159\n\nEpoch 117: Learning Rate is 1.36e-06\nEpoch 117/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 155ms/step - accuracy: 0.5144 - auc: 0.7639 - f1_macro: 0.4305 - loss: 3.9121 - val_accuracy: 0.5499 - val_auc: 0.7728 - val_f1_macro: 0.4436 - val_loss: 3.9095\n\nEpoch 118: Learning Rate is 1.29e-06\nEpoch 118/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.5170 - auc: 0.7628 - f1_macro: 0.4222 - loss: 3.8490 - val_accuracy: 0.5504 - val_auc: 0.7729 - val_f1_macro: 0.4398 - val_loss: 3.9242\n\nEpoch 119: Learning Rate is 1.23e-06\nEpoch 119/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.5075 - auc: 0.7591 - f1_macro: 0.4245 - loss: 3.9221 - val_accuracy: 0.5504 - val_auc: 0.7733 - val_f1_macro: 0.4429 - val_loss: 3.9303\n\nEpoch 120: Learning Rate is 1.17e-06\nEpoch 120/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.5074 - auc: 0.7601 - f1_macro: 0.4330 - loss: 3.9202 - val_accuracy: 0.5501 - val_auc: 0.7740 - val_f1_macro: 0.4405 - val_loss: 3.9223\n\nEpoch 121: Learning Rate is 1.10e-06\nEpoch 121/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 207ms/step - accuracy: 0.4825 - auc: 0.7531 - f1_macro: 0.4060 - loss: 4.1891 - val_accuracy: 0.5499 - val_auc: 0.7744 - val_f1_macro: 0.4389 - val_loss: 3.9189\n\nEpoch 122: Learning Rate is 1.04e-06\nEpoch 122/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.4841 - auc: 0.7562 - f1_macro: 0.4099 - loss: 4.1362 - val_accuracy: 0.5518 - val_auc: 0.7739 - val_f1_macro: 0.4432 - val_loss: 3.9212\n\nEpoch 123: Learning Rate is 9.85e-07\nEpoch 123/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.4941 - auc: 0.7629 - f1_macro: 0.4092 - loss: 4.0261 - val_accuracy: 0.5504 - val_auc: 0.7753 - val_f1_macro: 0.4381 - val_loss: 3.9325\n\nEpoch 124: Learning Rate is 9.27e-07\nEpoch 124/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.4926 - auc: 0.7594 - f1_macro: 0.4104 - loss: 4.0952 - val_accuracy: 0.5499 - val_auc: 0.7746 - val_f1_macro: 0.4381 - val_loss: 3.9101\n\nEpoch 125: Learning Rate is 8.71e-07\nEpoch 125/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 171ms/step - accuracy: 0.4977 - auc: 0.7646 - f1_macro: 0.4120 - loss: 4.0784 - val_accuracy: 0.5511 - val_auc: 0.7758 - val_f1_macro: 0.4391 - val_loss: 3.9097\n\nEpoch 126: Learning Rate is 8.16e-07\nEpoch 126/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 155ms/step - accuracy: 0.4881 - auc: 0.7550 - f1_macro: 0.4201 - loss: 4.2107 - val_accuracy: 0.5527 - val_auc: 0.7769 - val_f1_macro: 0.4406 - val_loss: 3.9154\n\nEpoch 127: Learning Rate is 7.62e-07\nEpoch 127/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 152ms/step - accuracy: 0.4945 - auc: 0.7618 - f1_macro: 0.4181 - loss: 4.1866 - val_accuracy: 0.5515 - val_auc: 0.7765 - val_f1_macro: 0.4349 - val_loss: 3.9199\n\nEpoch 128: Learning Rate is 7.10e-07\nEpoch 128/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 176ms/step - accuracy: 0.4877 - auc: 0.7535 - f1_macro: 0.4131 - loss: 4.1820 - val_accuracy: 0.5494 - val_auc: 0.7754 - val_f1_macro: 0.4328 - val_loss: 3.9326\n\nEpoch 129: Learning Rate is 6.59e-07\nEpoch 129/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 140ms/step - accuracy: 0.4651 - auc: 0.7489 - f1_macro: 0.3960 - loss: 4.3143 - val_accuracy: 0.5499 - val_auc: 0.7754 - val_f1_macro: 0.4350 - val_loss: 3.9233\n\nEpoch 130: Learning Rate is 6.10e-07\nEpoch 130/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 149ms/step - accuracy: 0.4988 - auc: 0.7520 - f1_macro: 0.4185 - loss: 4.2947 - val_accuracy: 0.5502 - val_auc: 0.7764 - val_f1_macro: 0.4332 - val_loss: 3.9229\n\nEpoch 131: Learning Rate is 5.62e-07\nEpoch 131/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.5252 - auc: 0.7664 - f1_macro: 0.4488 - loss: 3.9955 - val_accuracy: 0.5485 - val_auc: 0.7767 - val_f1_macro: 0.4276 - val_loss: 3.9326\n\nEpoch 132: Learning Rate is 5.16e-07\nEpoch 132/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.4791 - auc: 0.7516 - f1_macro: 0.4082 - loss: 4.3996 - val_accuracy: 0.5513 - val_auc: 0.7776 - val_f1_macro: 0.4298 - val_loss: 3.9396\n\nEpoch 133: Learning Rate is 4.72e-07\nEpoch 133/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 155ms/step - accuracy: 0.4739 - auc: 0.7471 - f1_macro: 0.4087 - loss: 4.3237 - val_accuracy: 0.5506 - val_auc: 0.7769 - val_f1_macro: 0.4284 - val_loss: 3.9406\n\nEpoch 134: Learning Rate is 4.30e-07\nEpoch 134/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 150ms/step - accuracy: 0.4963 - auc: 0.7527 - f1_macro: 0.4272 - loss: 4.3197 - val_accuracy: 0.5494 - val_auc: 0.7773 - val_f1_macro: 0.4281 - val_loss: 3.9317\n\nEpoch 135: Learning Rate is 3.89e-07\nEpoch 135/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.4657 - auc: 0.7443 - f1_macro: 0.4055 - loss: 4.3377 - val_accuracy: 0.5483 - val_auc: 0.7767 - val_f1_macro: 0.4260 - val_loss: 3.9397\n\nEpoch 136: Learning Rate is 3.50e-07\nEpoch 136/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.4787 - auc: 0.7461 - f1_macro: 0.4162 - loss: 4.2004 - val_accuracy: 0.5488 - val_auc: 0.7774 - val_f1_macro: 0.4255 - val_loss: 3.9417\n\nEpoch 137: Learning Rate is 3.13e-07\nEpoch 137/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5080 - auc: 0.7671 - f1_macro: 0.4357 - loss: 4.0993 - val_accuracy: 0.5499 - val_auc: 0.7783 - val_f1_macro: 0.4251 - val_loss: 3.9430\n\nEpoch 138: Learning Rate is 2.78e-07\nEpoch 138/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.4930 - auc: 0.7532 - f1_macro: 0.4235 - loss: 4.1884 - val_accuracy: 0.5485 - val_auc: 0.7781 - val_f1_macro: 0.4217 - val_loss: 3.9583\n\nEpoch 139: Learning Rate is 2.45e-07\nEpoch 139/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4598 - auc: 0.7419 - f1_macro: 0.3960 - loss: 4.3581 - val_accuracy: 0.5502 - val_auc: 0.7780 - val_f1_macro: 0.4233 - val_loss: 3.9569\n\nEpoch 140: Learning Rate is 2.14e-07\nEpoch 140/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 154ms/step - accuracy: 0.4556 - auc: 0.7384 - f1_macro: 0.3926 - loss: 4.3452 - val_accuracy: 0.5487 - val_auc: 0.7779 - val_f1_macro: 0.4218 - val_loss: 3.9450\n\nEpoch 141: Learning Rate is 1.85e-07\nEpoch 141/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 153ms/step - accuracy: 0.4810 - auc: 0.7531 - f1_macro: 0.4105 - loss: 4.2407 - val_accuracy: 0.5499 - val_auc: 0.7766 - val_f1_macro: 0.4222 - val_loss: 3.9530\n\nEpoch 142: Learning Rate is 1.58e-07\nEpoch 142/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.4681 - auc: 0.7456 - f1_macro: 0.3990 - loss: 4.1555 - val_accuracy: 0.5490 - val_auc: 0.7778 - val_f1_macro: 0.4206 - val_loss: 3.9608\n\nEpoch 143: Learning Rate is 1.33e-07\nEpoch 143/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.5043 - auc: 0.7609 - f1_macro: 0.4226 - loss: 4.0171 - val_accuracy: 0.5501 - val_auc: 0.7779 - val_f1_macro: 0.4199 - val_loss: 3.9739\n\nEpoch 144: Learning Rate is 1.10e-07\nEpoch 144/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 158ms/step - accuracy: 0.4806 - auc: 0.7510 - f1_macro: 0.4016 - loss: 3.9913 - val_accuracy: 0.5492 - val_auc: 0.7779 - val_f1_macro: 0.4173 - val_loss: 3.9686\n\nEpoch 145: Learning Rate is 8.93e-08\nEpoch 145/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 158ms/step - accuracy: 0.4886 - auc: 0.7486 - f1_macro: 0.4135 - loss: 3.9958 - val_accuracy: 0.5506 - val_auc: 0.7774 - val_f1_macro: 0.4196 - val_loss: 3.9706\n\nEpoch 146: Learning Rate is 7.07e-08\nEpoch 146/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 176ms/step - accuracy: 0.4955 - auc: 0.7534 - f1_macro: 0.4161 - loss: 3.9230 - val_accuracy: 0.5502 - val_auc: 0.7781 - val_f1_macro: 0.4203 - val_loss: 3.9604\n\nEpoch 147: Learning Rate is 5.42e-08\nEpoch 147/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4731 - auc: 0.7480 - f1_macro: 0.3954 - loss: 4.0158 - val_accuracy: 0.5488 - val_auc: 0.7786 - val_f1_macro: 0.4170 - val_loss: 3.9727\n\nEpoch 148: Learning Rate is 3.99e-08\nEpoch 148/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4365 - auc: 0.7314 - f1_macro: 0.3661 - loss: 4.1793 - val_accuracy: 0.5485 - val_auc: 0.7783 - val_f1_macro: 0.4173 - val_loss: 3.9707\n\nEpoch 149: Learning Rate is 2.77e-08\nEpoch 149/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 170ms/step - accuracy: 0.4647 - auc: 0.7497 - f1_macro: 0.3820 - loss: 3.9697 - val_accuracy: 0.5481 - val_auc: 0.7771 - val_f1_macro: 0.4135 - val_loss: 3.9755\n\nEpoch 150: Learning Rate is 1.77e-08\nEpoch 150/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 158ms/step - accuracy: 0.4453 - auc: 0.7344 - f1_macro: 0.3680 - loss: 4.1448 - val_accuracy: 0.5479 - val_auc: 0.7780 - val_f1_macro: 0.4127 - val_loss: 3.9814\n\nEpoch 151: Learning Rate is 9.99e-09\nEpoch 151/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.4583 - auc: 0.7444 - f1_macro: 0.3857 - loss: 4.0232 - val_accuracy: 0.5494 - val_auc: 0.7776 - val_f1_macro: 0.4124 - val_loss: 3.9925\n\nEpoch 152: Learning Rate is 4.44e-09\nEpoch 152/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.4650 - auc: 0.7409 - f1_macro: 0.3776 - loss: 4.0094 - val_accuracy: 0.5499 - val_auc: 0.7775 - val_f1_macro: 0.4140 - val_loss: 3.9915\n\nEpoch 153: Learning Rate is 1.11e-09\nEpoch 153/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 167ms/step - accuracy: 0.4624 - auc: 0.7347 - f1_macro: 0.3737 - loss: 4.0317 - val_accuracy: 0.5472 - val_auc: 0.7779 - val_f1_macro: 0.4076 - val_loss: 4.0077\n\nEpoch 154: Learning Rate is 4.05e-06\nEpoch 154/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.4787 - auc: 0.7478 - f1_macro: 0.3808 - loss: 4.0027 - val_accuracy: 0.5465 - val_auc: 0.7763 - val_f1_macro: 0.3984 - val_loss: 4.0553\n\nEpoch 155: Learning Rate is 4.05e-06\nEpoch 155/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4467 - auc: 0.7295 - f1_macro: 0.3599 - loss: 4.1856 - val_accuracy: 0.5460 - val_auc: 0.7751 - val_f1_macro: 0.3952 - val_loss: 4.0851\n\nEpoch 156: Learning Rate is 4.05e-06\nEpoch 156/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.4871 - auc: 0.7485 - f1_macro: 0.3790 - loss: 3.8596 - val_accuracy: 0.5440 - val_auc: 0.7753 - val_f1_macro: 0.3900 - val_loss: 4.1125\n\nEpoch 157: Learning Rate is 4.05e-06\nEpoch 157/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 152ms/step - accuracy: 0.4427 - auc: 0.7218 - f1_macro: 0.3522 - loss: 4.1095 - val_accuracy: 0.5451 - val_auc: 0.7742 - val_f1_macro: 0.3909 - val_loss: 4.1278\n\nEpoch 158: Learning Rate is 4.05e-06\nEpoch 158/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 154ms/step - accuracy: 0.4465 - auc: 0.7207 - f1_macro: 0.3552 - loss: 4.0461 - val_accuracy: 0.5462 - val_auc: 0.7760 - val_f1_macro: 0.3931 - val_loss: 4.1031\n\nEpoch 159: Learning Rate is 4.04e-06\nEpoch 159/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.4589 - auc: 0.7267 - f1_macro: 0.3651 - loss: 3.9938 - val_accuracy: 0.5476 - val_auc: 0.7774 - val_f1_macro: 0.3949 - val_loss: 4.0960\n\nEpoch 160: Learning Rate is 4.04e-06\nEpoch 160/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.4245 - auc: 0.7116 - f1_macro: 0.3379 - loss: 4.2049 - val_accuracy: 0.5460 - val_auc: 0.7773 - val_f1_macro: 0.3912 - val_loss: 4.0988\nEpoch 160: early stopping\nRestoring model weights from the end of the best epoch: 85.\nĐã lưu mô hình cho Fold 2 tại: /kaggle/working/output_results/EfficienetB0_CV_TPU_fold_2.keras\nĐang tạo và lưu biểu đồ huấn luyện...\nĐã lưu biểu đồ cho Fold 2 tại: /kaggle/working/output_results/fold_2_metrics.png\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756905473.134223      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:10044225326906660470\nI0000 00:00:1756905474.440974     929 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(3615831481463024170), session_name()\nI0000 00:00:1756905486.832920     929 tpu_compile_op_common.cc:245] Compilation of 3615831481463024170 with session name  took 12.391866972s and succeeded\nI0000 00:00:1756905486.850901     929 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(3615831481463024170), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_10044225326906660470\", property.function_library_fingerprint = 2232429282968489749, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756905486.850940     929 tpu_compilation_cache_interface.cc:542] After adding entry for key 3615831481463024170 with session_name  cache is 15 entries (946763203 bytes),  marked for eviction 0 entries (0 bytes).\nI0000 00:00:1756905489.070927     898 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(5085128291169083050), session_name()\nI0000 00:00:1756905495.121269     898 tpu_compile_op_common.cc:245] Compilation of 5085128291169083050 with session name  took 6.050297541s and succeeded\nI0000 00:00:1756905495.138467     898 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(5085128291169083050), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_10044225326906660470\", property.function_library_fingerprint = 2232429282968489749, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"3,224,224,3,;3,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756905495.138503     898 tpu_compilation_cache_interface.cc:542] After adding entry for key 5085128291169083050 with session_name  cache is 16 entries (969900724 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"Fold 2 - Validation Loss: 4.0438, Validation Accuracy: 0.5292, Validation AUC: 0.7424, Validation F1-Macro: 0.4844\n==================================================\nKết quả Cross-Validation:\nValidation Accuracy trung bình: 0.5287 +/- 0.0005\nValidation Loss trung bình: 4.1185 +/- 0.0747\nValidation AUC trung bình: 0.7535 +/- 0.0111\nValidation F1-Macro trung bình: 0.4812 +/- 0.0032\n==================================================\n--------------------------------------------------\nBắt đầu Fold 3/5\n--------------------------------------------------\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fca8ea77520> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fca8ea77520>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fca8ea77520> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fca8ea77520>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function <lambda> at 0x7fca8ea77520> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fca8ea77520>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fcb0c1b9d80> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcb0c1b9d80>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fcb0c1b9d80> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcb0c1b9d80>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function <lambda> at 0x7fcb0c1b9d80> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcb0c1b9d80>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fd0187a4940> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd0187a4940>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fd0187a4940> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd0187a4940>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function <lambda> at 0x7fd0187a4940> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd0187a4940>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fd0d45a5360> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd0d45a5360>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fd0d45a5360> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd0d45a5360>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function <lambda> at 0x7fd0d45a5360> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fd0d45a5360>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nSố bước mỗi epoch: 87 | Số bước validation: 21\n\n--- Bắt đầu Giai đoạn 1: Huấn luyện các lớp cuối ---\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756905516.688298      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:1307807639676886036\nI0000 00:00:1756905519.866382     915 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(9471939774999470288), session_name()\nI0000 00:00:1756905545.154592     915 tpu_compile_op_common.cc:245] Compilation of 9471939774999470288 with session name  took 25.288158549s and succeeded\nI0000 00:00:1756905545.226591     915 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(9471939774999470288), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_1307807639676886036\", property.function_library_fingerprint = 4600068981444386166, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756905545.226657     915 tpu_compilation_cache_interface.cc:542] After adding entry for key 9471939774999470288 with session_name  cache is 17 entries (1046337753 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.6098 - auc: 0.7605 - loss: 7.4673","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756905560.863159      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:15830642482226497575\nI0000 00:00:1756905562.266143     956 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(6649187362245809322), session_name()\nI0000 00:00:1756905574.418749     956 tpu_compile_op_common.cc:245] Compilation of 6649187362245809322 with session name  took 12.152507225s and succeeded\nI0000 00:00:1756905574.440441     956 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(6649187362245809322), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_15830642482226497575\", property.function_library_fingerprint = 15922573311347984286, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756905574.440477     956 tpu_compilation_cache_interface.cc:542] After adding entry for key 6649187362245809322 with session_name  cache is 18 entries (1072984697 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 374ms/step - accuracy: 0.6094 - auc: 0.7601 - loss: 7.4863 - val_accuracy: 0.2772 - val_auc: 0.5326 - val_loss: 21.5125\nEpoch 2/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.2970 - auc: 0.5216 - loss: 15.7716 - val_accuracy: 0.3975 - val_auc: 0.6020 - val_loss: 16.7445\nEpoch 3/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 150ms/step - accuracy: 0.3770 - auc: 0.5971 - loss: 12.5788 - val_accuracy: 0.4089 - val_auc: 0.6113 - val_loss: 17.4058\nEpoch 4/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 139ms/step - accuracy: 0.3514 - auc: 0.5811 - loss: 13.2491 - val_accuracy: 0.4195 - val_auc: 0.6170 - val_loss: 17.7062\nEpoch 5/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 140ms/step - accuracy: 0.3931 - auc: 0.6123 - loss: 12.5736 - val_accuracy: 0.4302 - val_auc: 0.6216 - val_loss: 17.4812\n\n--- Bắt đầu Giai đoạn 2: Fine-tuning ---\nTổng số lớp trong base model: 238\nSẽ mở băng và huấn luyện 119 lớp cuối cùng (bắt đầu từ lớp 119).\n\n--- Giai đoạn 2A: Bắt đầu Warmup trong 3 epochs ---\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756905648.071579      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:10190182886973840421\nI0000 00:00:1756905652.755138     902 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(13437670129072953183), session_name()\nI0000 00:00:1756905686.564466     902 tpu_compile_op_common.cc:245] Compilation of 13437670129072953183 with session name  took 33.809271417s and succeeded\nI0000 00:00:1756905686.656828     902 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(13437670129072953183), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_10190182886973840421\", property.function_library_fingerprint = 2721717127616884787, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756905686.656887     902 tpu_compilation_cache_interface.cc:542] After adding entry for key 13437670129072953183 with session_name  cache is 19 entries (1209980030 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.3915 - f1_macro: 0.2036 - loss: 7.0962","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756905702.992346      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:7257043115216955144\nI0000 00:00:1756905704.147924     958 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(10668003996678849258), session_name()\nI0000 00:00:1756905716.359695     958 tpu_compile_op_common.cc:245] Compilation of 10668003996678849258 with session name  took 12.211535095s and succeeded\nI0000 00:00:1756905716.377052     958 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(10668003996678849258), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_7257043115216955144\", property.function_library_fingerprint = 9111803173688161779, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756905716.377091     958 tpu_compilation_cache_interface.cc:542] After adding entry for key 10668003996678849258 with session_name  cache is 20 entries (1235768629 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 369ms/step - accuracy: 0.3898 - f1_macro: 0.2036 - loss: 7.1225 - val_accuracy: 0.2844 - val_f1_macro: 0.2544 - val_loss: 6.9295\nEpoch 2/3\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.3655 - f1_macro: 0.2055 - loss: 7.2495 - val_accuracy: 0.2852 - val_f1_macro: 0.2781 - val_loss: 7.1886\nEpoch 3/3\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 176ms/step - accuracy: 0.3388 - f1_macro: 0.1998 - loss: 7.2240 - val_accuracy: 0.2643 - val_f1_macro: 0.2026 - val_loss: 7.3815\n\n--- Giai đoạn 2B: Bắt đầu huấn luyện chính ---\nSử dụng scheduler: CosineDecayRestarts\n\nEpoch 4: Learning Rate is 5.00e-06\nEpoch 4/200\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756905773.504639      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:17638075076956033656\nI0000 00:00:1756905779.453965     934 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(745048919637648326), session_name()\nI0000 00:00:1756905816.214353     934 tpu_compile_op_common.cc:245] Compilation of 745048919637648326 with session name  took 36.760342818s and succeeded\nI0000 00:00:1756905816.335794     934 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(745048919637648326), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_17638075076956033656\", property.function_library_fingerprint = 7378250811433580484, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756905816.335852     934 tpu_compilation_cache_interface.cc:542] After adding entry for key 745048919637648326 with session_name  cache is 21 entries (1374734352 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.3984 - auc: 0.6282 - f1_macro: 0.1978 - loss: 6.4890","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756905834.020672      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:13411885727078202138\nI0000 00:00:1756905835.257356     886 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(10487588989142009538), session_name()\nI0000 00:00:1756905847.817950     886 tpu_compile_op_common.cc:245] Compilation of 10487588989142009538 with session name  took 12.560323602s and succeeded\nI0000 00:00:1756905847.840706     886 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(10487588989142009538), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_13411885727078202138\", property.function_library_fingerprint = 10540385212625126166, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756905847.840742     886 tpu_compilation_cache_interface.cc:542] After adding entry for key 10487588989142009538 with session_name  cache is 22 entries (1402258431 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 400ms/step - accuracy: 0.3968 - auc: 0.6267 - f1_macro: 0.1980 - loss: 6.5129 - val_accuracy: 0.3454 - val_auc: 0.5850 - val_f1_macro: 0.3348 - val_loss: 5.8482\n\nEpoch 5: Learning Rate is 5.00e-06\nEpoch 5/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 179ms/step - accuracy: 0.3668 - auc: 0.5983 - f1_macro: 0.2232 - loss: 6.3743 - val_accuracy: 0.3890 - val_auc: 0.6252 - val_f1_macro: 0.3812 - val_loss: 5.4810\n\nEpoch 6: Learning Rate is 4.98e-06\nEpoch 6/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 194ms/step - accuracy: 0.3408 - auc: 0.5869 - f1_macro: 0.2476 - loss: 6.3433 - val_accuracy: 0.4077 - val_auc: 0.6449 - val_f1_macro: 0.3910 - val_loss: 5.3731\n\nEpoch 7: Learning Rate is 4.96e-06\nEpoch 7/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 177ms/step - accuracy: 0.3614 - auc: 0.5874 - f1_macro: 0.2741 - loss: 6.1071 - val_accuracy: 0.4332 - val_auc: 0.6584 - val_f1_macro: 0.3996 - val_loss: 5.3409\n\nEpoch 8: Learning Rate is 4.92e-06\nEpoch 8/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.3484 - auc: 0.5893 - f1_macro: 0.2750 - loss: 6.1486 - val_accuracy: 0.4392 - val_auc: 0.6655 - val_f1_macro: 0.3951 - val_loss: 5.3114\n\nEpoch 9: Learning Rate is 4.88e-06\nEpoch 9/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 178ms/step - accuracy: 0.3532 - auc: 0.5992 - f1_macro: 0.2905 - loss: 6.0897 - val_accuracy: 0.4563 - val_auc: 0.6729 - val_f1_macro: 0.3982 - val_loss: 5.2686\n\nEpoch 10: Learning Rate is 4.82e-06\nEpoch 10/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 177ms/step - accuracy: 0.3911 - auc: 0.6223 - f1_macro: 0.3235 - loss: 5.7737 - val_accuracy: 0.4667 - val_auc: 0.6818 - val_f1_macro: 0.3956 - val_loss: 5.2236\n\nEpoch 11: Learning Rate is 4.76e-06\nEpoch 11/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.3865 - auc: 0.6230 - f1_macro: 0.3179 - loss: 5.7952 - val_accuracy: 0.4714 - val_auc: 0.6865 - val_f1_macro: 0.3947 - val_loss: 5.1890\n\nEpoch 12: Learning Rate is 4.69e-06\nEpoch 12/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 172ms/step - accuracy: 0.4001 - auc: 0.6394 - f1_macro: 0.3426 - loss: 5.6163 - val_accuracy: 0.4760 - val_auc: 0.6885 - val_f1_macro: 0.3941 - val_loss: 5.1543\n\nEpoch 13: Learning Rate is 4.61e-06\nEpoch 13/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.4346 - auc: 0.6621 - f1_macro: 0.3548 - loss: 5.2940 - val_accuracy: 0.4769 - val_auc: 0.6888 - val_f1_macro: 0.3950 - val_loss: 5.1283\n\nEpoch 14: Learning Rate is 4.52e-06\nEpoch 14/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 181ms/step - accuracy: 0.4391 - auc: 0.6684 - f1_macro: 0.3518 - loss: 5.2091 - val_accuracy: 0.4820 - val_auc: 0.6907 - val_f1_macro: 0.4014 - val_loss: 5.0841\n\nEpoch 15: Learning Rate is 4.43e-06\nEpoch 15/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.4658 - auc: 0.6840 - f1_macro: 0.3752 - loss: 5.0119 - val_accuracy: 0.4808 - val_auc: 0.6907 - val_f1_macro: 0.4062 - val_loss: 5.0422\n\nEpoch 16: Learning Rate is 4.32e-06\nEpoch 16/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 179ms/step - accuracy: 0.4952 - auc: 0.7121 - f1_macro: 0.4002 - loss: 4.6711 - val_accuracy: 0.4879 - val_auc: 0.6965 - val_f1_macro: 0.4094 - val_loss: 5.0017\n\nEpoch 17: Learning Rate is 4.21e-06\nEpoch 17/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 169ms/step - accuracy: 0.5233 - auc: 0.7305 - f1_macro: 0.4148 - loss: 4.4422 - val_accuracy: 0.4905 - val_auc: 0.7022 - val_f1_macro: 0.4074 - val_loss: 4.9438\n\nEpoch 18: Learning Rate is 4.09e-06\nEpoch 18/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.4987 - auc: 0.7210 - f1_macro: 0.4001 - loss: 4.5051 - val_accuracy: 0.4940 - val_auc: 0.7037 - val_f1_macro: 0.4142 - val_loss: 4.8989\n\nEpoch 19: Learning Rate is 3.97e-06\nEpoch 19/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 170ms/step - accuracy: 0.5208 - auc: 0.7345 - f1_macro: 0.4021 - loss: 4.3359 - val_accuracy: 0.4911 - val_auc: 0.7019 - val_f1_macro: 0.4196 - val_loss: 4.8864\n\nEpoch 20: Learning Rate is 3.84e-06\nEpoch 20/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 186ms/step - accuracy: 0.5399 - auc: 0.7491 - f1_macro: 0.4205 - loss: 4.1879 - val_accuracy: 0.4933 - val_auc: 0.7024 - val_f1_macro: 0.4236 - val_loss: 4.8703\n\nEpoch 21: Learning Rate is 3.70e-06\nEpoch 21/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 177ms/step - accuracy: 0.5778 - auc: 0.7712 - f1_macro: 0.4393 - loss: 3.8728 - val_accuracy: 0.4929 - val_auc: 0.7036 - val_f1_macro: 0.4253 - val_loss: 4.8237\n\nEpoch 22: Learning Rate is 3.56e-06\nEpoch 22/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 174ms/step - accuracy: 0.5800 - auc: 0.7776 - f1_macro: 0.4445 - loss: 3.7755 - val_accuracy: 0.4924 - val_auc: 0.7047 - val_f1_macro: 0.4250 - val_loss: 4.8083\n\nEpoch 23: Learning Rate is 3.42e-06\nEpoch 23/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 186ms/step - accuracy: 0.5818 - auc: 0.7850 - f1_macro: 0.4391 - loss: 3.6813 - val_accuracy: 0.4918 - val_auc: 0.7066 - val_f1_macro: 0.4242 - val_loss: 4.7814\n\nEpoch 24: Learning Rate is 3.27e-06\nEpoch 24/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 177ms/step - accuracy: 0.5958 - auc: 0.7912 - f1_macro: 0.4376 - loss: 3.6496 - val_accuracy: 0.4931 - val_auc: 0.7066 - val_f1_macro: 0.4295 - val_loss: 4.7711\n\nEpoch 25: Learning Rate is 3.12e-06\nEpoch 25/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 167ms/step - accuracy: 0.6190 - auc: 0.8100 - f1_macro: 0.4475 - loss: 3.3834 - val_accuracy: 0.4911 - val_auc: 0.7068 - val_f1_macro: 0.4288 - val_loss: 4.7658\n\nEpoch 26: Learning Rate is 2.97e-06\nEpoch 26/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 153ms/step - accuracy: 0.6020 - auc: 0.8024 - f1_macro: 0.4469 - loss: 3.4322 - val_accuracy: 0.4888 - val_auc: 0.7065 - val_f1_macro: 0.4278 - val_loss: 4.7584\n\nEpoch 27: Learning Rate is 2.81e-06\nEpoch 27/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 184ms/step - accuracy: 0.6217 - auc: 0.8138 - f1_macro: 0.4512 - loss: 3.4293 - val_accuracy: 0.4888 - val_auc: 0.7068 - val_f1_macro: 0.4314 - val_loss: 4.7358\n\nEpoch 28: Learning Rate is 2.66e-06\nEpoch 28/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 182ms/step - accuracy: 0.6248 - auc: 0.8238 - f1_macro: 0.4556 - loss: 3.1378 - val_accuracy: 0.4894 - val_auc: 0.7081 - val_f1_macro: 0.4324 - val_loss: 4.7155\n\nEpoch 29: Learning Rate is 2.50e-06\nEpoch 29/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 167ms/step - accuracy: 0.6270 - auc: 0.8169 - f1_macro: 0.4588 - loss: 3.3177 - val_accuracy: 0.4900 - val_auc: 0.7057 - val_f1_macro: 0.4327 - val_loss: 4.7205\n\nEpoch 30: Learning Rate is 2.34e-06\nEpoch 30/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 186ms/step - accuracy: 0.6482 - auc: 0.8275 - f1_macro: 0.4652 - loss: 3.1561 - val_accuracy: 0.4766 - val_auc: 0.7021 - val_f1_macro: 0.4269 - val_loss: 4.7182\n\nEpoch 31: Learning Rate is 2.19e-06\nEpoch 31/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.6353 - auc: 0.8279 - f1_macro: 0.4522 - loss: 3.0821 - val_accuracy: 0.4771 - val_auc: 0.7029 - val_f1_macro: 0.4281 - val_loss: 4.7038\n\nEpoch 32: Learning Rate is 2.03e-06\nEpoch 32/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 153ms/step - accuracy: 0.6549 - auc: 0.8401 - f1_macro: 0.4581 - loss: 2.9509 - val_accuracy: 0.4771 - val_auc: 0.7026 - val_f1_macro: 0.4289 - val_loss: 4.6999\n\nEpoch 33: Learning Rate is 1.88e-06\nEpoch 33/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.6703 - auc: 0.8481 - f1_macro: 0.4657 - loss: 2.8642 - val_accuracy: 0.4693 - val_auc: 0.6996 - val_f1_macro: 0.4231 - val_loss: 4.7064\n\nEpoch 34: Learning Rate is 1.73e-06\nEpoch 34/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 154ms/step - accuracy: 0.6819 - auc: 0.8522 - f1_macro: 0.4814 - loss: 2.7937 - val_accuracy: 0.4697 - val_auc: 0.6993 - val_f1_macro: 0.4269 - val_loss: 4.7028\n\nEpoch 35: Learning Rate is 1.58e-06\nEpoch 35/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.6557 - auc: 0.8413 - f1_macro: 0.4724 - loss: 3.0019 - val_accuracy: 0.4634 - val_auc: 0.6969 - val_f1_macro: 0.4215 - val_loss: 4.7058\n\nEpoch 36: Learning Rate is 1.44e-06\nEpoch 36/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.6706 - auc: 0.8564 - f1_macro: 0.4644 - loss: 2.7850 - val_accuracy: 0.4604 - val_auc: 0.6956 - val_f1_macro: 0.4189 - val_loss: 4.7154\n\nEpoch 37: Learning Rate is 1.30e-06\nEpoch 37/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.6846 - auc: 0.8556 - f1_macro: 0.4721 - loss: 2.7795 - val_accuracy: 0.4565 - val_auc: 0.6952 - val_f1_macro: 0.4167 - val_loss: 4.7161\n\nEpoch 38: Learning Rate is 1.16e-06\nEpoch 38/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.6875 - auc: 0.8608 - f1_macro: 0.4785 - loss: 2.7686 - val_accuracy: 0.4552 - val_auc: 0.6955 - val_f1_macro: 0.4160 - val_loss: 4.7145\n\nEpoch 39: Learning Rate is 1.03e-06\nEpoch 39/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 156ms/step - accuracy: 0.6760 - auc: 0.8558 - f1_macro: 0.4646 - loss: 2.6983 - val_accuracy: 0.4528 - val_auc: 0.6931 - val_f1_macro: 0.4138 - val_loss: 4.7308\n\nEpoch 40: Learning Rate is 9.06e-07\nEpoch 40/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 152ms/step - accuracy: 0.6894 - auc: 0.8624 - f1_macro: 0.4716 - loss: 2.7901 - val_accuracy: 0.4475 - val_auc: 0.6923 - val_f1_macro: 0.4082 - val_loss: 4.7266\n\nEpoch 41: Learning Rate is 7.89e-07\nEpoch 41/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 170ms/step - accuracy: 0.6958 - auc: 0.8663 - f1_macro: 0.4823 - loss: 2.5933 - val_accuracy: 0.4563 - val_auc: 0.6948 - val_f1_macro: 0.4171 - val_loss: 4.7036\n\nEpoch 42: Learning Rate is 6.78e-07\nEpoch 42/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.6931 - auc: 0.8663 - f1_macro: 0.4719 - loss: 2.6676 - val_accuracy: 0.4541 - val_auc: 0.6915 - val_f1_macro: 0.4159 - val_loss: 4.7093\n\nEpoch 43: Learning Rate is 5.74e-07\nEpoch 43/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.6950 - auc: 0.8648 - f1_macro: 0.4764 - loss: 2.6588 - val_accuracy: 0.4490 - val_auc: 0.6921 - val_f1_macro: 0.4111 - val_loss: 4.7066\n\nEpoch 44: Learning Rate is 4.77e-07\nEpoch 44/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 179ms/step - accuracy: 0.6733 - auc: 0.8501 - f1_macro: 0.4710 - loss: 2.8289 - val_accuracy: 0.4488 - val_auc: 0.6928 - val_f1_macro: 0.4129 - val_loss: 4.7005\n\nEpoch 45: Learning Rate is 3.89e-07\nEpoch 45/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.6922 - auc: 0.8571 - f1_macro: 0.4857 - loss: 2.7561 - val_accuracy: 0.4500 - val_auc: 0.6916 - val_f1_macro: 0.4121 - val_loss: 4.7061\n\nEpoch 46: Learning Rate is 3.09e-07\nEpoch 46/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.6966 - auc: 0.8594 - f1_macro: 0.4714 - loss: 2.7535 - val_accuracy: 0.4496 - val_auc: 0.6915 - val_f1_macro: 0.4127 - val_loss: 4.7180\n\nEpoch 47: Learning Rate is 2.38e-07\nEpoch 47/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.6836 - auc: 0.8503 - f1_macro: 0.4870 - loss: 2.8554 - val_accuracy: 0.4520 - val_auc: 0.6936 - val_f1_macro: 0.4143 - val_loss: 4.6982\n\nEpoch 48: Learning Rate is 1.76e-07\nEpoch 48/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 167ms/step - accuracy: 0.7026 - auc: 0.8645 - f1_macro: 0.4876 - loss: 2.6649 - val_accuracy: 0.4472 - val_auc: 0.6910 - val_f1_macro: 0.4090 - val_loss: 4.7240\n\nEpoch 49: Learning Rate is 1.22e-07\nEpoch 49/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 149ms/step - accuracy: 0.6691 - auc: 0.8396 - f1_macro: 0.4666 - loss: 2.9440 - val_accuracy: 0.4468 - val_auc: 0.6916 - val_f1_macro: 0.4094 - val_loss: 4.7143\n\nEpoch 50: Learning Rate is 7.85e-08\nEpoch 50/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.6682 - auc: 0.8435 - f1_macro: 0.4630 - loss: 2.9659 - val_accuracy: 0.4464 - val_auc: 0.6886 - val_f1_macro: 0.4081 - val_loss: 4.7296\n\nEpoch 51: Learning Rate is 4.43e-08\nEpoch 51/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.6689 - auc: 0.8525 - f1_macro: 0.4795 - loss: 2.8655 - val_accuracy: 0.4472 - val_auc: 0.6883 - val_f1_macro: 0.4075 - val_loss: 4.7327\n\nEpoch 52: Learning Rate is 1.97e-08\nEpoch 52/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 153ms/step - accuracy: 0.6338 - auc: 0.8181 - f1_macro: 0.4555 - loss: 3.2901 - val_accuracy: 0.4488 - val_auc: 0.6904 - val_f1_macro: 0.4097 - val_loss: 4.7219\n\nEpoch 53: Learning Rate is 4.93e-09\nEpoch 53/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 144ms/step - accuracy: 0.6388 - auc: 0.8343 - f1_macro: 0.4560 - loss: 2.9618 - val_accuracy: 0.4498 - val_auc: 0.6937 - val_f1_macro: 0.4111 - val_loss: 4.7074\n\nEpoch 54: Learning Rate is 4.50e-06\nEpoch 54/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 146ms/step - accuracy: 0.6218 - auc: 0.8227 - f1_macro: 0.4430 - loss: 3.0563 - val_accuracy: 0.4368 - val_auc: 0.6862 - val_f1_macro: 0.4018 - val_loss: 4.7272\n\nEpoch 55: Learning Rate is 4.50e-06\nEpoch 55/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.6256 - auc: 0.8244 - f1_macro: 0.4476 - loss: 3.0197 - val_accuracy: 0.4275 - val_auc: 0.6809 - val_f1_macro: 0.3945 - val_loss: 4.7510\n\nEpoch 56: Learning Rate is 4.50e-06\nEpoch 56/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 156ms/step - accuracy: 0.6298 - auc: 0.8215 - f1_macro: 0.4334 - loss: 3.1079 - val_accuracy: 0.4234 - val_auc: 0.6798 - val_f1_macro: 0.3889 - val_loss: 4.7476\n\nEpoch 57: Learning Rate is 4.49e-06\nEpoch 57/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.6012 - auc: 0.8115 - f1_macro: 0.4272 - loss: 3.1667 - val_accuracy: 0.4410 - val_auc: 0.6911 - val_f1_macro: 0.4037 - val_loss: 4.6705\n\nEpoch 58: Learning Rate is 4.48e-06\nEpoch 58/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.6140 - auc: 0.8169 - f1_macro: 0.4392 - loss: 3.1595 - val_accuracy: 0.4397 - val_auc: 0.6922 - val_f1_macro: 0.4016 - val_loss: 4.6629\n\nEpoch 59: Learning Rate is 4.47e-06\nEpoch 59/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 155ms/step - accuracy: 0.6051 - auc: 0.8140 - f1_macro: 0.4160 - loss: 3.1714 - val_accuracy: 0.4317 - val_auc: 0.6880 - val_f1_macro: 0.3970 - val_loss: 4.6724\n\nEpoch 60: Learning Rate is 4.46e-06\nEpoch 60/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 145ms/step - accuracy: 0.5676 - auc: 0.7989 - f1_macro: 0.4129 - loss: 3.3199 - val_accuracy: 0.4453 - val_auc: 0.6963 - val_f1_macro: 0.4081 - val_loss: 4.6113\n\nEpoch 61: Learning Rate is 4.45e-06\nEpoch 61/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 145ms/step - accuracy: 0.5943 - auc: 0.8112 - f1_macro: 0.4339 - loss: 3.1288 - val_accuracy: 0.4494 - val_auc: 0.6975 - val_f1_macro: 0.4076 - val_loss: 4.6191\n\nEpoch 62: Learning Rate is 4.43e-06\nEpoch 62/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5990 - auc: 0.8092 - f1_macro: 0.4269 - loss: 3.1494 - val_accuracy: 0.4606 - val_auc: 0.7078 - val_f1_macro: 0.4150 - val_loss: 4.5376\n\nEpoch 63: Learning Rate is 4.41e-06\nEpoch 63/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 203ms/step - accuracy: 0.5656 - auc: 0.7963 - f1_macro: 0.4161 - loss: 3.2594 - val_accuracy: 0.4604 - val_auc: 0.7071 - val_f1_macro: 0.4183 - val_loss: 4.5282\n\nEpoch 64: Learning Rate is 4.39e-06\nEpoch 64/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 151ms/step - accuracy: 0.5657 - auc: 0.7928 - f1_macro: 0.4130 - loss: 3.3061 - val_accuracy: 0.4613 - val_auc: 0.7075 - val_f1_macro: 0.4144 - val_loss: 4.5384\n\nEpoch 65: Learning Rate is 4.37e-06\nEpoch 65/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5547 - auc: 0.8023 - f1_macro: 0.4041 - loss: 3.0862 - val_accuracy: 0.4555 - val_auc: 0.7079 - val_f1_macro: 0.4102 - val_loss: 4.5322\n\nEpoch 66: Learning Rate is 4.34e-06\nEpoch 66/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.5536 - auc: 0.7861 - f1_macro: 0.4042 - loss: 3.3623 - val_accuracy: 0.4563 - val_auc: 0.7073 - val_f1_macro: 0.4094 - val_loss: 4.5299\n\nEpoch 67: Learning Rate is 4.31e-06\nEpoch 67/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 145ms/step - accuracy: 0.5513 - auc: 0.7972 - f1_macro: 0.4102 - loss: 3.1789 - val_accuracy: 0.4708 - val_auc: 0.7173 - val_f1_macro: 0.4223 - val_loss: 4.4537\n\nEpoch 68: Learning Rate is 4.29e-06\nEpoch 68/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5475 - auc: 0.7893 - f1_macro: 0.4000 - loss: 3.1842 - val_accuracy: 0.4773 - val_auc: 0.7213 - val_f1_macro: 0.4281 - val_loss: 4.4139\n\nEpoch 69: Learning Rate is 4.25e-06\nEpoch 69/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 145ms/step - accuracy: 0.5499 - auc: 0.7920 - f1_macro: 0.4114 - loss: 3.1259 - val_accuracy: 0.4764 - val_auc: 0.7223 - val_f1_macro: 0.4273 - val_loss: 4.3950\n\nEpoch 70: Learning Rate is 4.22e-06\nEpoch 70/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 142ms/step - accuracy: 0.5190 - auc: 0.7750 - f1_macro: 0.3977 - loss: 3.3189 - val_accuracy: 0.4872 - val_auc: 0.7264 - val_f1_macro: 0.4350 - val_loss: 4.3743\n\nEpoch 71: Learning Rate is 4.19e-06\nEpoch 71/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - accuracy: 0.5338 - auc: 0.7828 - f1_macro: 0.4060 - loss: 3.2010 - val_accuracy: 0.4913 - val_auc: 0.7312 - val_f1_macro: 0.4372 - val_loss: 4.3310\n\nEpoch 72: Learning Rate is 4.15e-06\nEpoch 72/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 170ms/step - accuracy: 0.5196 - auc: 0.7771 - f1_macro: 0.3938 - loss: 3.3615 - val_accuracy: 0.4955 - val_auc: 0.7327 - val_f1_macro: 0.4388 - val_loss: 4.3176\n\nEpoch 73: Learning Rate is 4.11e-06\nEpoch 73/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 149ms/step - accuracy: 0.5236 - auc: 0.7798 - f1_macro: 0.3894 - loss: 3.2226 - val_accuracy: 0.4927 - val_auc: 0.7319 - val_f1_macro: 0.4332 - val_loss: 4.3285\n\nEpoch 74: Learning Rate is 4.07e-06\nEpoch 74/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 150ms/step - accuracy: 0.5427 - auc: 0.7856 - f1_macro: 0.4149 - loss: 3.1946 - val_accuracy: 0.4872 - val_auc: 0.7280 - val_f1_macro: 0.4293 - val_loss: 4.3532\n\nEpoch 75: Learning Rate is 4.03e-06\nEpoch 75/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.5006 - auc: 0.7608 - f1_macro: 0.3822 - loss: 3.5976 - val_accuracy: 0.4950 - val_auc: 0.7349 - val_f1_macro: 0.4324 - val_loss: 4.3107\n\nEpoch 76: Learning Rate is 3.98e-06\nEpoch 76/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 149ms/step - accuracy: 0.5391 - auc: 0.7931 - f1_macro: 0.4126 - loss: 3.1387 - val_accuracy: 0.4924 - val_auc: 0.7332 - val_f1_macro: 0.4286 - val_loss: 4.3236\n\nEpoch 77: Learning Rate is 3.94e-06\nEpoch 77/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 146ms/step - accuracy: 0.4964 - auc: 0.7602 - f1_macro: 0.3806 - loss: 3.4233 - val_accuracy: 0.4937 - val_auc: 0.7357 - val_f1_macro: 0.4272 - val_loss: 4.3036\n\nEpoch 78: Learning Rate is 3.89e-06\nEpoch 78/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 149ms/step - accuracy: 0.5047 - auc: 0.7604 - f1_macro: 0.3863 - loss: 3.4297 - val_accuracy: 0.4965 - val_auc: 0.7363 - val_f1_macro: 0.4281 - val_loss: 4.3009\n\nEpoch 79: Learning Rate is 3.84e-06\nEpoch 79/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 156ms/step - accuracy: 0.5279 - auc: 0.7855 - f1_macro: 0.4100 - loss: 3.2105 - val_accuracy: 0.4933 - val_auc: 0.7341 - val_f1_macro: 0.4242 - val_loss: 4.3261\n\nEpoch 80: Learning Rate is 3.79e-06\nEpoch 80/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 145ms/step - accuracy: 0.4802 - auc: 0.7674 - f1_macro: 0.3793 - loss: 3.4219 - val_accuracy: 0.4957 - val_auc: 0.7387 - val_f1_macro: 0.4238 - val_loss: 4.3085\n\nEpoch 81: Learning Rate is 3.74e-06\nEpoch 81/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 140ms/step - accuracy: 0.4911 - auc: 0.7650 - f1_macro: 0.3842 - loss: 3.3825 - val_accuracy: 0.5033 - val_auc: 0.7418 - val_f1_macro: 0.4290 - val_loss: 4.2787\n\nEpoch 82: Learning Rate is 3.68e-06\nEpoch 82/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.5032 - auc: 0.7656 - f1_macro: 0.3985 - loss: 3.3429 - val_accuracy: 0.5043 - val_auc: 0.7445 - val_f1_macro: 0.4247 - val_loss: 4.2896\n\nEpoch 83: Learning Rate is 3.63e-06\nEpoch 83/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5063 - auc: 0.7721 - f1_macro: 0.3937 - loss: 3.2455 - val_accuracy: 0.5104 - val_auc: 0.7491 - val_f1_macro: 0.4299 - val_loss: 4.2488\n\nEpoch 84: Learning Rate is 3.57e-06\nEpoch 84/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 150ms/step - accuracy: 0.5147 - auc: 0.7676 - f1_macro: 0.4134 - loss: 3.4246 - val_accuracy: 0.5099 - val_auc: 0.7513 - val_f1_macro: 0.4242 - val_loss: 4.2383\n\nEpoch 85: Learning Rate is 3.51e-06\nEpoch 85/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 167ms/step - accuracy: 0.5304 - auc: 0.7742 - f1_macro: 0.4185 - loss: 3.3713 - val_accuracy: 0.5056 - val_auc: 0.7485 - val_f1_macro: 0.4200 - val_loss: 4.2515\n\nEpoch 86: Learning Rate is 3.46e-06\nEpoch 86/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 184ms/step - accuracy: 0.5237 - auc: 0.7731 - f1_macro: 0.4158 - loss: 3.3299 - val_accuracy: 0.5134 - val_auc: 0.7531 - val_f1_macro: 0.4276 - val_loss: 4.2225\n\nEpoch 87: Learning Rate is 3.40e-06\nEpoch 87/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.4915 - auc: 0.7567 - f1_macro: 0.3938 - loss: 3.5718 - val_accuracy: 0.5074 - val_auc: 0.7518 - val_f1_macro: 0.4206 - val_loss: 4.2489\n\nEpoch 88: Learning Rate is 3.33e-06\nEpoch 88/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.4876 - auc: 0.7621 - f1_macro: 0.3945 - loss: 3.4779 - val_accuracy: 0.5069 - val_auc: 0.7519 - val_f1_macro: 0.4181 - val_loss: 4.2641\n\nEpoch 89: Learning Rate is 3.27e-06\nEpoch 89/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 188ms/step - accuracy: 0.4883 - auc: 0.7573 - f1_macro: 0.3969 - loss: 3.5994 - val_accuracy: 0.5113 - val_auc: 0.7556 - val_f1_macro: 0.4198 - val_loss: 4.2404\n\nEpoch 90: Learning Rate is 3.21e-06\nEpoch 90/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.4976 - auc: 0.7644 - f1_macro: 0.4122 - loss: 3.5632 - val_accuracy: 0.5125 - val_auc: 0.7581 - val_f1_macro: 0.4201 - val_loss: 4.2206\n\nEpoch 91: Learning Rate is 3.14e-06\nEpoch 91/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4954 - auc: 0.7578 - f1_macro: 0.4004 - loss: 3.5926 - val_accuracy: 0.5112 - val_auc: 0.7569 - val_f1_macro: 0.4163 - val_loss: 4.2408\n\nEpoch 92: Learning Rate is 3.08e-06\nEpoch 92/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.4897 - auc: 0.7587 - f1_macro: 0.4112 - loss: 3.6314 - val_accuracy: 0.5162 - val_auc: 0.7598 - val_f1_macro: 0.4225 - val_loss: 4.2163\n\nEpoch 93: Learning Rate is 3.01e-06\nEpoch 93/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 186ms/step - accuracy: 0.4953 - auc: 0.7638 - f1_macro: 0.4119 - loss: 3.7243 - val_accuracy: 0.5192 - val_auc: 0.7605 - val_f1_macro: 0.4229 - val_loss: 4.2062\n\nEpoch 94: Learning Rate is 2.95e-06\nEpoch 94/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 167ms/step - accuracy: 0.4747 - auc: 0.7543 - f1_macro: 0.3975 - loss: 3.7790 - val_accuracy: 0.5249 - val_auc: 0.7648 - val_f1_macro: 0.4234 - val_loss: 4.1838\n\nEpoch 95: Learning Rate is 2.88e-06\nEpoch 95/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 158ms/step - accuracy: 0.4823 - auc: 0.7518 - f1_macro: 0.4086 - loss: 3.9814 - val_accuracy: 0.5188 - val_auc: 0.7625 - val_f1_macro: 0.4182 - val_loss: 4.2013\n\nEpoch 96: Learning Rate is 2.81e-06\nEpoch 96/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 180ms/step - accuracy: 0.4650 - auc: 0.7491 - f1_macro: 0.3874 - loss: 3.9935 - val_accuracy: 0.5303 - val_auc: 0.7671 - val_f1_macro: 0.4227 - val_loss: 4.1788\n\nEpoch 97: Learning Rate is 2.74e-06\nEpoch 97/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 170ms/step - accuracy: 0.4625 - auc: 0.7537 - f1_macro: 0.3895 - loss: 4.1317 - val_accuracy: 0.5305 - val_auc: 0.7687 - val_f1_macro: 0.4222 - val_loss: 4.1673\n\nEpoch 98: Learning Rate is 2.67e-06\nEpoch 98/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.5130 - auc: 0.7617 - f1_macro: 0.4318 - loss: 3.9294 - val_accuracy: 0.5299 - val_auc: 0.7690 - val_f1_macro: 0.4211 - val_loss: 4.1603\n\nEpoch 99: Learning Rate is 2.60e-06\nEpoch 99/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.4846 - auc: 0.7582 - f1_macro: 0.4068 - loss: 3.9963 - val_accuracy: 0.5314 - val_auc: 0.7697 - val_f1_macro: 0.4219 - val_loss: 4.1612\n\nEpoch 100: Learning Rate is 2.53e-06\nEpoch 100/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 176ms/step - accuracy: 0.4589 - auc: 0.7519 - f1_macro: 0.3961 - loss: 4.2441 - val_accuracy: 0.5311 - val_auc: 0.7697 - val_f1_macro: 0.4182 - val_loss: 4.1803\n\nEpoch 101: Learning Rate is 2.46e-06\nEpoch 101/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 167ms/step - accuracy: 0.4779 - auc: 0.7582 - f1_macro: 0.4053 - loss: 4.1268 - val_accuracy: 0.5314 - val_auc: 0.7687 - val_f1_macro: 0.4195 - val_loss: 4.1859\n\nEpoch 102: Learning Rate is 2.39e-06\nEpoch 102/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4904 - auc: 0.7553 - f1_macro: 0.4193 - loss: 4.1037 - val_accuracy: 0.5355 - val_auc: 0.7720 - val_f1_macro: 0.4200 - val_loss: 4.1745\n\nEpoch 103: Learning Rate is 2.32e-06\nEpoch 103/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 183ms/step - accuracy: 0.4812 - auc: 0.7478 - f1_macro: 0.4126 - loss: 4.3586 - val_accuracy: 0.5329 - val_auc: 0.7717 - val_f1_macro: 0.4171 - val_loss: 4.1806\n\nEpoch 104: Learning Rate is 2.25e-06\nEpoch 104/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 158ms/step - accuracy: 0.4828 - auc: 0.7554 - f1_macro: 0.4163 - loss: 4.1454 - val_accuracy: 0.5359 - val_auc: 0.7738 - val_f1_macro: 0.4168 - val_loss: 4.1851\n\nEpoch 105: Learning Rate is 2.18e-06\nEpoch 105/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 155ms/step - accuracy: 0.4635 - auc: 0.7584 - f1_macro: 0.3966 - loss: 4.2604 - val_accuracy: 0.5333 - val_auc: 0.7741 - val_f1_macro: 0.4125 - val_loss: 4.1953\n\nEpoch 106: Learning Rate is 2.11e-06\nEpoch 106/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4737 - auc: 0.7598 - f1_macro: 0.4086 - loss: 4.2906 - val_accuracy: 0.5346 - val_auc: 0.7739 - val_f1_macro: 0.4134 - val_loss: 4.1958\n\nEpoch 107: Learning Rate is 2.04e-06\nEpoch 107/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.4967 - auc: 0.7530 - f1_macro: 0.4328 - loss: 4.2323 - val_accuracy: 0.5346 - val_auc: 0.7740 - val_f1_macro: 0.4112 - val_loss: 4.2080\n\nEpoch 108: Learning Rate is 1.97e-06\nEpoch 108/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 155ms/step - accuracy: 0.4607 - auc: 0.7322 - f1_macro: 0.4002 - loss: 4.5781 - val_accuracy: 0.5352 - val_auc: 0.7756 - val_f1_macro: 0.4113 - val_loss: 4.1949\n\nEpoch 109: Learning Rate is 1.90e-06\nEpoch 109/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 156ms/step - accuracy: 0.4962 - auc: 0.7624 - f1_macro: 0.4263 - loss: 4.0769 - val_accuracy: 0.5344 - val_auc: 0.7759 - val_f1_macro: 0.4062 - val_loss: 4.2020\n\nEpoch 110: Learning Rate is 1.83e-06\nEpoch 110/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.4617 - auc: 0.7450 - f1_macro: 0.3987 - loss: 4.2810 - val_accuracy: 0.5353 - val_auc: 0.7754 - val_f1_macro: 0.4077 - val_loss: 4.2077\n\nEpoch 111: Learning Rate is 1.76e-06\nEpoch 111/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.4668 - auc: 0.7482 - f1_macro: 0.4017 - loss: 4.2989 - val_accuracy: 0.5366 - val_auc: 0.7757 - val_f1_macro: 0.4077 - val_loss: 4.2133\n\nEpoch 112: Learning Rate is 1.69e-06\nEpoch 112/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 167ms/step - accuracy: 0.4886 - auc: 0.7540 - f1_macro: 0.4236 - loss: 4.1792 - val_accuracy: 0.5357 - val_auc: 0.7745 - val_f1_macro: 0.4061 - val_loss: 4.2235\n\nEpoch 113: Learning Rate is 1.62e-06\nEpoch 113/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4739 - auc: 0.7450 - f1_macro: 0.4043 - loss: 4.3944 - val_accuracy: 0.5352 - val_auc: 0.7759 - val_f1_macro: 0.4043 - val_loss: 4.2145\n\nEpoch 114: Learning Rate is 1.55e-06\nEpoch 114/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 181ms/step - accuracy: 0.4801 - auc: 0.7556 - f1_macro: 0.4171 - loss: 4.2021 - val_accuracy: 0.5350 - val_auc: 0.7769 - val_f1_macro: 0.4031 - val_loss: 4.2121\n\nEpoch 115: Learning Rate is 1.49e-06\nEpoch 115/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4892 - auc: 0.7511 - f1_macro: 0.4164 - loss: 4.2252 - val_accuracy: 0.5348 - val_auc: 0.7777 - val_f1_macro: 0.4008 - val_loss: 4.2182\n\nEpoch 116: Learning Rate is 1.42e-06\nEpoch 116/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.4706 - auc: 0.7482 - f1_macro: 0.3983 - loss: 4.2886 - val_accuracy: 0.5376 - val_auc: 0.7800 - val_f1_macro: 0.4031 - val_loss: 4.2083\n\nEpoch 117: Learning Rate is 1.36e-06\nEpoch 117/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 188ms/step - accuracy: 0.4852 - auc: 0.7454 - f1_macro: 0.4133 - loss: 4.0753 - val_accuracy: 0.5398 - val_auc: 0.7815 - val_f1_macro: 0.4036 - val_loss: 4.2049\n\nEpoch 118: Learning Rate is 1.29e-06\nEpoch 118/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.4740 - auc: 0.7499 - f1_macro: 0.3997 - loss: 4.0639 - val_accuracy: 0.5374 - val_auc: 0.7811 - val_f1_macro: 0.4001 - val_loss: 4.2223\n\nEpoch 119: Learning Rate is 1.23e-06\nEpoch 119/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 169ms/step - accuracy: 0.4735 - auc: 0.7465 - f1_macro: 0.4016 - loss: 4.2507 - val_accuracy: 0.5391 - val_auc: 0.7813 - val_f1_macro: 0.4017 - val_loss: 4.2236\n\nEpoch 120: Learning Rate is 1.17e-06\nEpoch 120/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.4772 - auc: 0.7452 - f1_macro: 0.4051 - loss: 4.0908 - val_accuracy: 0.5385 - val_auc: 0.7816 - val_f1_macro: 0.4003 - val_loss: 4.2314\n\nEpoch 121: Learning Rate is 1.10e-06\nEpoch 121/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 182ms/step - accuracy: 0.4568 - auc: 0.7460 - f1_macro: 0.3881 - loss: 3.9946 - val_accuracy: 0.5398 - val_auc: 0.7808 - val_f1_macro: 0.4000 - val_loss: 4.2381\n\nEpoch 122: Learning Rate is 1.04e-06\nEpoch 122/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 170ms/step - accuracy: 0.4937 - auc: 0.7551 - f1_macro: 0.4039 - loss: 3.9760 - val_accuracy: 0.5396 - val_auc: 0.7817 - val_f1_macro: 0.3995 - val_loss: 4.2287\n\nEpoch 123: Learning Rate is 9.85e-07\nEpoch 123/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.4935 - auc: 0.7477 - f1_macro: 0.4019 - loss: 4.0050 - val_accuracy: 0.5381 - val_auc: 0.7821 - val_f1_macro: 0.3975 - val_loss: 4.2276\n\nEpoch 124: Learning Rate is 9.27e-07\nEpoch 124/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.4451 - auc: 0.7333 - f1_macro: 0.3689 - loss: 4.1407 - val_accuracy: 0.5409 - val_auc: 0.7828 - val_f1_macro: 0.3991 - val_loss: 4.2264\n\nEpoch 125: Learning Rate is 8.71e-07\nEpoch 125/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 150ms/step - accuracy: 0.4867 - auc: 0.7472 - f1_macro: 0.3913 - loss: 3.9742 - val_accuracy: 0.5409 - val_auc: 0.7838 - val_f1_macro: 0.3985 - val_loss: 4.2309\n\nEpoch 126: Learning Rate is 8.16e-07\nEpoch 126/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 167ms/step - accuracy: 0.4577 - auc: 0.7361 - f1_macro: 0.3742 - loss: 4.1055 - val_accuracy: 0.5402 - val_auc: 0.7838 - val_f1_macro: 0.3973 - val_loss: 4.2384\n\nEpoch 127: Learning Rate is 7.62e-07\nEpoch 127/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 169ms/step - accuracy: 0.5052 - auc: 0.7629 - f1_macro: 0.3976 - loss: 3.6229 - val_accuracy: 0.5409 - val_auc: 0.7837 - val_f1_macro: 0.3969 - val_loss: 4.2465\n\nEpoch 128: Learning Rate is 7.10e-07\nEpoch 128/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 189ms/step - accuracy: 0.4791 - auc: 0.7487 - f1_macro: 0.3755 - loss: 3.8781 - val_accuracy: 0.5404 - val_auc: 0.7839 - val_f1_macro: 0.3968 - val_loss: 4.2510\n\nEpoch 129: Learning Rate is 6.59e-07\nEpoch 129/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 155ms/step - accuracy: 0.4911 - auc: 0.7433 - f1_macro: 0.3799 - loss: 3.9163 - val_accuracy: 0.5415 - val_auc: 0.7844 - val_f1_macro: 0.3967 - val_loss: 4.2656\n\nEpoch 130: Learning Rate is 6.10e-07\nEpoch 130/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.5156 - auc: 0.7618 - f1_macro: 0.3965 - loss: 3.7110 - val_accuracy: 0.5419 - val_auc: 0.7853 - val_f1_macro: 0.3953 - val_loss: 4.2706\n\nEpoch 131: Learning Rate is 5.62e-07\nEpoch 131/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.4649 - auc: 0.7483 - f1_macro: 0.3603 - loss: 3.8331 - val_accuracy: 0.5419 - val_auc: 0.7859 - val_f1_macro: 0.3953 - val_loss: 4.2679\n\nEpoch 132: Learning Rate is 5.16e-07\nEpoch 132/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.4959 - auc: 0.7435 - f1_macro: 0.3827 - loss: 3.8811 - val_accuracy: 0.5422 - val_auc: 0.7862 - val_f1_macro: 0.3948 - val_loss: 4.2665\n\nEpoch 133: Learning Rate is 4.72e-07\nEpoch 133/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 169ms/step - accuracy: 0.4988 - auc: 0.7458 - f1_macro: 0.3802 - loss: 3.8016 - val_accuracy: 0.5432 - val_auc: 0.7863 - val_f1_macro: 0.3947 - val_loss: 4.2749\n\nEpoch 134: Learning Rate is 4.30e-07\nEpoch 134/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.4748 - auc: 0.7437 - f1_macro: 0.3645 - loss: 3.7744 - val_accuracy: 0.5422 - val_auc: 0.7865 - val_f1_macro: 0.3926 - val_loss: 4.2902\n\nEpoch 135: Learning Rate is 3.89e-07\nEpoch 135/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 186ms/step - accuracy: 0.4701 - auc: 0.7490 - f1_macro: 0.3590 - loss: 3.8073 - val_accuracy: 0.5422 - val_auc: 0.7866 - val_f1_macro: 0.3931 - val_loss: 4.2823\n\nEpoch 136: Learning Rate is 3.50e-07\nEpoch 136/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.4957 - auc: 0.7448 - f1_macro: 0.3624 - loss: 3.7636 - val_accuracy: 0.5422 - val_auc: 0.7871 - val_f1_macro: 0.3919 - val_loss: 4.2893\n\nEpoch 137: Learning Rate is 3.13e-07\nEpoch 137/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 150ms/step - accuracy: 0.4803 - auc: 0.7480 - f1_macro: 0.3600 - loss: 3.7505 - val_accuracy: 0.5430 - val_auc: 0.7867 - val_f1_macro: 0.3931 - val_loss: 4.2844\n\nEpoch 138: Learning Rate is 2.78e-07\nEpoch 138/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 172ms/step - accuracy: 0.4776 - auc: 0.7418 - f1_macro: 0.3577 - loss: 3.8075 - val_accuracy: 0.5428 - val_auc: 0.7870 - val_f1_macro: 0.3927 - val_loss: 4.3025\n\nEpoch 139: Learning Rate is 2.45e-07\nEpoch 139/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 158ms/step - accuracy: 0.4785 - auc: 0.7387 - f1_macro: 0.3598 - loss: 3.8407 - val_accuracy: 0.5430 - val_auc: 0.7876 - val_f1_macro: 0.3925 - val_loss: 4.3075\n\nEpoch 140: Learning Rate is 2.14e-07\nEpoch 140/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.4716 - auc: 0.7472 - f1_macro: 0.3417 - loss: 3.7325 - val_accuracy: 0.5448 - val_auc: 0.7882 - val_f1_macro: 0.3927 - val_loss: 4.3101\n\nEpoch 141: Learning Rate is 1.85e-07\nEpoch 141/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.4833 - auc: 0.7500 - f1_macro: 0.3435 - loss: 3.6931 - val_accuracy: 0.5433 - val_auc: 0.7878 - val_f1_macro: 0.3917 - val_loss: 4.3209\n\nEpoch 142: Learning Rate is 1.58e-07\nEpoch 142/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.5329 - auc: 0.7573 - f1_macro: 0.3693 - loss: 3.5239 - val_accuracy: 0.5446 - val_auc: 0.7876 - val_f1_macro: 0.3923 - val_loss: 4.3301\n\nEpoch 143: Learning Rate is 1.33e-07\nEpoch 143/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.4809 - auc: 0.7403 - f1_macro: 0.3438 - loss: 3.7105 - val_accuracy: 0.5450 - val_auc: 0.7883 - val_f1_macro: 0.3915 - val_loss: 4.3266\n\nEpoch 144: Learning Rate is 1.10e-07\nEpoch 144/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4871 - auc: 0.7473 - f1_macro: 0.3404 - loss: 3.6583 - val_accuracy: 0.5441 - val_auc: 0.7882 - val_f1_macro: 0.3924 - val_loss: 4.3159\n\nEpoch 145: Learning Rate is 8.93e-08\nEpoch 145/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 183ms/step - accuracy: 0.5499 - auc: 0.7728 - f1_macro: 0.5273 - loss: 3.3970 - val_accuracy: 0.5433 - val_auc: 0.7871 - val_f1_macro: 0.3937 - val_loss: 4.3022\n\nEpoch 146: Learning Rate is 7.07e-08\nEpoch 146/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5450 - auc: 0.7846 - f1_macro: 0.5180 - loss: 3.2904 - val_accuracy: 0.5433 - val_auc: 0.7871 - val_f1_macro: 0.3940 - val_loss: 4.2846\n\nEpoch 147: Learning Rate is 5.42e-08\nEpoch 147/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step - accuracy: 0.5299 - auc: 0.7734 - f1_macro: 0.5176 - loss: 3.3859 - val_accuracy: 0.5432 - val_auc: 0.7870 - val_f1_macro: 0.3939 - val_loss: 4.2927\nEpoch 147: early stopping\nRestoring model weights from the end of the best epoch: 72.\nĐã lưu mô hình cho Fold 3 tại: /kaggle/working/output_results/EfficienetB0_CV_TPU_fold_3.keras\nĐang tạo và lưu biểu đồ huấn luyện...\nĐã lưu biểu đồ cho Fold 3 tại: /kaggle/working/output_results/fold_3_metrics.png\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756907920.731355      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:729008158017643293\nI0000 00:00:1756907922.331306     896 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(8999508770187593427), session_name()\nI0000 00:00:1756907934.850315     896 tpu_compile_op_common.cc:245] Compilation of 8999508770187593427 with session name  took 12.51893236s and succeeded\nI0000 00:00:1756907934.870722     896 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(8999508770187593427), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_729008158017643293\", property.function_library_fingerprint = 14251331053899326903, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756907934.870754     896 tpu_compilation_cache_interface.cc:542] After adding entry for key 8999508770187593427 with session_name  cache is 23 entries (1429782510 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"Fold 3 - Validation Loss: 4.2663, Validation Accuracy: 0.5135, Validation AUC: 0.7414, Validation F1-Macro: 0.4434\n==================================================\nKết quả Cross-Validation:\nValidation Accuracy trung bình: 0.5236 +/- 0.0072\nValidation Loss trung bình: 4.1677 +/- 0.0926\nValidation AUC trung bình: 0.7495 +/- 0.0107\nValidation F1-Macro trung bình: 0.4686 +/- 0.0180\n==================================================\n--------------------------------------------------\nBắt đầu Fold 4/5\n--------------------------------------------------\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fcb925a5a20> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcb925a5a20>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fcb925a5a20> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcb925a5a20>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function <lambda> at 0x7fcb925a5a20> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcb925a5a20>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fce540fb1c0> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fce540fb1c0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fce540fb1c0> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fce540fb1c0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function <lambda> at 0x7fce540fb1c0> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fce540fb1c0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fcb0c1b9d80> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcb0c1b9d80>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fcb0c1b9d80> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcb0c1b9d80>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function <lambda> at 0x7fcb0c1b9d80> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcb0c1b9d80>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fcb0c21fd90> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcb0c21fd90>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fcb0c21fd90> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcb0c21fd90>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function <lambda> at 0x7fcb0c21fd90> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcb0c21fd90>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nSố bước mỗi epoch: 87 | Số bước validation: 21\n\n--- Bắt đầu Giai đoạn 1: Huấn luyện các lớp cuối ---\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756907956.249489      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:2894377006596246416\nI0000 00:00:1756907959.055446     956 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(7564658699213513100), session_name()\nI0000 00:00:1756907984.143827     956 tpu_compile_op_common.cc:245] Compilation of 7564658699213513100 with session name  took 25.088323769s and succeeded\nI0000 00:00:1756907984.203679     956 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(7564658699213513100), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_2894377006596246416\", property.function_library_fingerprint = 3866783292956290835, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756907984.203735     956 tpu_compilation_cache_interface.cc:542] After adding entry for key 7564658699213513100 with session_name  cache is 24 entries (1506219539 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.5471 - auc: 0.7050 - loss: 7.5652","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756907998.850971      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:11818639276781995526\nI0000 00:00:1756908000.211569     890 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(2147324796587422410), session_name()\nI0000 00:00:1756908012.417016     890 tpu_compile_op_common.cc:245] Compilation of 2147324796587422410 with session name  took 12.205386519s and succeeded\nI0000 00:00:1756908012.439397     890 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(2147324796587422410), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_11818639276781995526\", property.function_library_fingerprint = 16804455706925242856, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756908012.439437     890 tpu_compilation_cache_interface.cc:542] After adding entry for key 2147324796587422410 with session_name  cache is 25 entries (1532866483 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 365ms/step - accuracy: 0.5471 - auc: 0.7051 - loss: 7.5792 - val_accuracy: 0.3294 - val_auc: 0.5432 - val_loss: 19.7619\nEpoch 2/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 154ms/step - accuracy: 0.3166 - auc: 0.5444 - loss: 15.0804 - val_accuracy: 0.4003 - val_auc: 0.6060 - val_loss: 16.5451\nEpoch 3/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 154ms/step - accuracy: 0.3424 - auc: 0.5822 - loss: 13.3017 - val_accuracy: 0.4228 - val_auc: 0.6219 - val_loss: 16.2108\nEpoch 4/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 141ms/step - accuracy: 0.4028 - auc: 0.6209 - loss: 11.6382 - val_accuracy: 0.4243 - val_auc: 0.6215 - val_loss: 16.6128\nEpoch 5/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 154ms/step - accuracy: 0.4257 - auc: 0.6354 - loss: 11.4118 - val_accuracy: 0.4414 - val_auc: 0.6293 - val_loss: 16.7314\n\n--- Bắt đầu Giai đoạn 2: Fine-tuning ---\nTổng số lớp trong base model: 238\nSẽ mở băng và huấn luyện 119 lớp cuối cùng (bắt đầu từ lớp 119).\n\n--- Giai đoạn 2A: Bắt đầu Warmup trong 3 epochs ---\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756908087.003332      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:2277057125613232735\nI0000 00:00:1756908091.615074     965 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(5531243610398874191), session_name()\nI0000 00:00:1756908126.605819     965 tpu_compile_op_common.cc:245] Compilation of 5531243610398874191 with session name  took 34.990677836s and succeeded\nI0000 00:00:1756908126.724242     965 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(5531243610398874191), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_2277057125613232735\", property.function_library_fingerprint = 16767135304347986050, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756908126.724299     965 tpu_compilation_cache_interface.cc:542] After adding entry for key 5531243610398874191 with session_name  cache is 26 entries (1669861816 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.3495 - f1_macro: 0.2048 - loss: 6.5903","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756908144.131347      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:12353112837602269810\nI0000 00:00:1756908145.387382     890 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(14213309458273381887), session_name()\nI0000 00:00:1756908157.527865     890 tpu_compile_op_common.cc:245] Compilation of 14213309458273381887 with session name  took 12.140387031s and succeeded\nI0000 00:00:1756908157.552539     890 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(14213309458273381887), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_12353112837602269810\", property.function_library_fingerprint = 17016494166091959302, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756908157.552579     890 tpu_compilation_cache_interface.cc:542] After adding entry for key 14213309458273381887 with session_name  cache is 27 entries (1695650415 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 385ms/step - accuracy: 0.3482 - f1_macro: 0.2049 - loss: 6.6169 - val_accuracy: 0.3309 - val_f1_macro: 0.2711 - val_loss: 6.8432\nEpoch 2/3\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 176ms/step - accuracy: 0.3159 - f1_macro: 0.2027 - loss: 6.9189 - val_accuracy: 0.3307 - val_f1_macro: 0.3039 - val_loss: 6.7290\nEpoch 3/3\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.3260 - f1_macro: 0.2064 - loss: 7.0350 - val_accuracy: 0.2833 - val_f1_macro: 0.1825 - val_loss: 7.4014\n\n--- Giai đoạn 2B: Bắt đầu huấn luyện chính ---\nSử dụng scheduler: CosineDecayRestarts\n\nEpoch 4: Learning Rate is 5.00e-06\nEpoch 4/200\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756908216.162343      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:10596629595552438735\nI0000 00:00:1756908221.973955     974 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(13273222830305705899), session_name()\nI0000 00:00:1756908258.105778     974 tpu_compile_op_common.cc:245] Compilation of 13273222830305705899 with session name  took 36.131746873s and succeeded\nI0000 00:00:1756908258.199964     974 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(13273222830305705899), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_10596629595552438735\", property.function_library_fingerprint = 17986972462557434109, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756908258.200033     974 tpu_compilation_cache_interface.cc:542] After adding entry for key 13273222830305705899 with session_name  cache is 28 entries (1834616138 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.3768 - auc: 0.6155 - f1_macro: 0.2034 - loss: 6.2688","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756908276.273571      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:6882890737358612163\nI0000 00:00:1756908277.555605     888 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(14579348394302645973), session_name()\nI0000 00:00:1756908289.962916     888 tpu_compile_op_common.cc:245] Compilation of 14579348394302645973 with session name  took 12.40706572s and succeeded\nI0000 00:00:1756908289.987590     888 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(14579348394302645973), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_6882890737358612163\", property.function_library_fingerprint = 13484747504488797744, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756908289.987628     888 tpu_compilation_cache_interface.cc:542] After adding entry for key 14579348394302645973 with session_name  cache is 29 entries (1862140217 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 403ms/step - accuracy: 0.3752 - auc: 0.6141 - f1_macro: 0.2034 - loss: 6.2921 - val_accuracy: 0.3815 - val_auc: 0.6034 - val_f1_macro: 0.3335 - val_loss: 6.1106\n\nEpoch 5: Learning Rate is 5.00e-06\nEpoch 5/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.3384 - auc: 0.5811 - f1_macro: 0.2097 - loss: 6.3732 - val_accuracy: 0.4170 - val_auc: 0.6371 - val_f1_macro: 0.3767 - val_loss: 5.7220\n\nEpoch 6: Learning Rate is 4.98e-06\nEpoch 6/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 171ms/step - accuracy: 0.3264 - auc: 0.5781 - f1_macro: 0.2382 - loss: 6.3482 - val_accuracy: 0.4347 - val_auc: 0.6528 - val_f1_macro: 0.3867 - val_loss: 5.5866\n\nEpoch 7: Learning Rate is 4.96e-06\nEpoch 7/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 174ms/step - accuracy: 0.3530 - auc: 0.5851 - f1_macro: 0.2798 - loss: 6.1614 - val_accuracy: 0.4494 - val_auc: 0.6635 - val_f1_macro: 0.3832 - val_loss: 5.4859\n\nEpoch 8: Learning Rate is 4.92e-06\nEpoch 8/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.3483 - auc: 0.5833 - f1_macro: 0.2767 - loss: 6.1473 - val_accuracy: 0.4535 - val_auc: 0.6673 - val_f1_macro: 0.3746 - val_loss: 5.4407\n\nEpoch 9: Learning Rate is 4.88e-06\nEpoch 9/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 195ms/step - accuracy: 0.3718 - auc: 0.6064 - f1_macro: 0.3177 - loss: 5.8360 - val_accuracy: 0.4576 - val_auc: 0.6711 - val_f1_macro: 0.3680 - val_loss: 5.4078\n\nEpoch 10: Learning Rate is 4.82e-06\nEpoch 10/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 187ms/step - accuracy: 0.4105 - auc: 0.6321 - f1_macro: 0.3366 - loss: 5.5440 - val_accuracy: 0.4613 - val_auc: 0.6727 - val_f1_macro: 0.3671 - val_loss: 5.3618\n\nEpoch 11: Learning Rate is 4.76e-06\nEpoch 11/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 188ms/step - accuracy: 0.4029 - auc: 0.6373 - f1_macro: 0.3240 - loss: 5.5415 - val_accuracy: 0.4615 - val_auc: 0.6763 - val_f1_macro: 0.3659 - val_loss: 5.3021\n\nEpoch 12: Learning Rate is 4.69e-06\nEpoch 12/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 189ms/step - accuracy: 0.4393 - auc: 0.6503 - f1_macro: 0.3421 - loss: 5.3175 - val_accuracy: 0.4667 - val_auc: 0.6781 - val_f1_macro: 0.3702 - val_loss: 5.2713\n\nEpoch 13: Learning Rate is 4.61e-06\nEpoch 13/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 176ms/step - accuracy: 0.4763 - auc: 0.6706 - f1_macro: 0.3751 - loss: 5.1153 - val_accuracy: 0.4693 - val_auc: 0.6806 - val_f1_macro: 0.3718 - val_loss: 5.2167\n\nEpoch 14: Learning Rate is 4.52e-06\nEpoch 14/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 190ms/step - accuracy: 0.4635 - auc: 0.6745 - f1_macro: 0.3616 - loss: 5.0960 - val_accuracy: 0.4710 - val_auc: 0.6853 - val_f1_macro: 0.3735 - val_loss: 5.1749\n\nEpoch 15: Learning Rate is 4.43e-06\nEpoch 15/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 171ms/step - accuracy: 0.5019 - auc: 0.7068 - f1_macro: 0.3869 - loss: 4.7849 - val_accuracy: 0.4784 - val_auc: 0.6868 - val_f1_macro: 0.3824 - val_loss: 5.1379\n\nEpoch 16: Learning Rate is 4.32e-06\nEpoch 16/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 203ms/step - accuracy: 0.5170 - auc: 0.7071 - f1_macro: 0.3896 - loss: 4.7027 - val_accuracy: 0.4866 - val_auc: 0.6905 - val_f1_macro: 0.3949 - val_loss: 5.0838\n\nEpoch 17: Learning Rate is 4.21e-06\nEpoch 17/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 176ms/step - accuracy: 0.5155 - auc: 0.7125 - f1_macro: 0.3854 - loss: 4.6391 - val_accuracy: 0.4933 - val_auc: 0.6928 - val_f1_macro: 0.4085 - val_loss: 5.0451\n\nEpoch 18: Learning Rate is 4.09e-06\nEpoch 18/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 178ms/step - accuracy: 0.5550 - auc: 0.7427 - f1_macro: 0.4057 - loss: 4.1940 - val_accuracy: 0.4957 - val_auc: 0.6967 - val_f1_macro: 0.4131 - val_loss: 5.0076\n\nEpoch 19: Learning Rate is 3.97e-06\nEpoch 19/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 185ms/step - accuracy: 0.5615 - auc: 0.7590 - f1_macro: 0.4070 - loss: 4.0855 - val_accuracy: 0.5013 - val_auc: 0.6999 - val_f1_macro: 0.4191 - val_loss: 4.9694\n\nEpoch 20: Learning Rate is 3.84e-06\nEpoch 20/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 178ms/step - accuracy: 0.5878 - auc: 0.7740 - f1_macro: 0.4251 - loss: 3.8321 - val_accuracy: 0.5065 - val_auc: 0.7022 - val_f1_macro: 0.4290 - val_loss: 4.9425\n\nEpoch 21: Learning Rate is 3.70e-06\nEpoch 21/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.6099 - auc: 0.7847 - f1_macro: 0.4343 - loss: 3.7746 - val_accuracy: 0.5089 - val_auc: 0.7020 - val_f1_macro: 0.4340 - val_loss: 4.9060\n\nEpoch 22: Learning Rate is 3.56e-06\nEpoch 22/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 167ms/step - accuracy: 0.6198 - auc: 0.7925 - f1_macro: 0.4233 - loss: 3.5730 - val_accuracy: 0.5095 - val_auc: 0.7078 - val_f1_macro: 0.4356 - val_loss: 4.8803\n\nEpoch 23: Learning Rate is 3.42e-06\nEpoch 23/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 194ms/step - accuracy: 0.6350 - auc: 0.8002 - f1_macro: 0.4469 - loss: 3.5124 - val_accuracy: 0.5115 - val_auc: 0.7084 - val_f1_macro: 0.4425 - val_loss: 4.8445\n\nEpoch 24: Learning Rate is 3.27e-06\nEpoch 24/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 185ms/step - accuracy: 0.6246 - auc: 0.8045 - f1_macro: 0.4408 - loss: 3.4360 - val_accuracy: 0.5125 - val_auc: 0.7088 - val_f1_macro: 0.4457 - val_loss: 4.8080\n\nEpoch 25: Learning Rate is 3.12e-06\nEpoch 25/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 184ms/step - accuracy: 0.6252 - auc: 0.8087 - f1_macro: 0.4411 - loss: 3.3617 - val_accuracy: 0.5113 - val_auc: 0.7084 - val_f1_macro: 0.4468 - val_loss: 4.8008\n\nEpoch 26: Learning Rate is 2.97e-06\nEpoch 26/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 203ms/step - accuracy: 0.6561 - auc: 0.8204 - f1_macro: 0.4547 - loss: 3.2614 - val_accuracy: 0.5099 - val_auc: 0.7096 - val_f1_macro: 0.4476 - val_loss: 4.7900\n\nEpoch 27: Learning Rate is 2.81e-06\nEpoch 27/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 197ms/step - accuracy: 0.6563 - auc: 0.8303 - f1_macro: 0.4534 - loss: 3.0720 - val_accuracy: 0.5100 - val_auc: 0.7117 - val_f1_macro: 0.4482 - val_loss: 4.7661\n\nEpoch 28: Learning Rate is 2.66e-06\nEpoch 28/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.6586 - auc: 0.8268 - f1_macro: 0.4460 - loss: 3.1430 - val_accuracy: 0.5113 - val_auc: 0.7099 - val_f1_macro: 0.4524 - val_loss: 4.7721\n\nEpoch 29: Learning Rate is 2.50e-06\nEpoch 29/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 183ms/step - accuracy: 0.6713 - auc: 0.8379 - f1_macro: 0.4457 - loss: 3.0182 - val_accuracy: 0.5126 - val_auc: 0.7109 - val_f1_macro: 0.4568 - val_loss: 4.7529\n\nEpoch 30: Learning Rate is 2.34e-06\nEpoch 30/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 194ms/step - accuracy: 0.6679 - auc: 0.8302 - f1_macro: 0.4545 - loss: 3.1253 - val_accuracy: 0.5080 - val_auc: 0.7087 - val_f1_macro: 0.4533 - val_loss: 4.7594\n\nEpoch 31: Learning Rate is 2.19e-06\nEpoch 31/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 180ms/step - accuracy: 0.6720 - auc: 0.8340 - f1_macro: 0.4520 - loss: 3.0737 - val_accuracy: 0.5076 - val_auc: 0.7098 - val_f1_macro: 0.4539 - val_loss: 4.7477\n\nEpoch 32: Learning Rate is 2.03e-06\nEpoch 32/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.6827 - auc: 0.8452 - f1_macro: 0.4567 - loss: 2.9832 - val_accuracy: 0.5039 - val_auc: 0.7077 - val_f1_macro: 0.4550 - val_loss: 4.7464\n\nEpoch 33: Learning Rate is 1.88e-06\nEpoch 33/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 197ms/step - accuracy: 0.6790 - auc: 0.8344 - f1_macro: 0.4714 - loss: 3.1001 - val_accuracy: 0.5032 - val_auc: 0.7068 - val_f1_macro: 0.4579 - val_loss: 4.7444\n\nEpoch 34: Learning Rate is 1.73e-06\nEpoch 34/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 187ms/step - accuracy: 0.6701 - auc: 0.8351 - f1_macro: 0.4546 - loss: 3.0598 - val_accuracy: 0.5015 - val_auc: 0.7066 - val_f1_macro: 0.4599 - val_loss: 4.7370\n\nEpoch 35: Learning Rate is 1.58e-06\nEpoch 35/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 186ms/step - accuracy: 0.6760 - auc: 0.8478 - f1_macro: 0.4632 - loss: 2.8682 - val_accuracy: 0.5013 - val_auc: 0.7081 - val_f1_macro: 0.4593 - val_loss: 4.7327\n\nEpoch 36: Learning Rate is 1.44e-06\nEpoch 36/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 171ms/step - accuracy: 0.6714 - auc: 0.8350 - f1_macro: 0.4631 - loss: 3.0025 - val_accuracy: 0.5026 - val_auc: 0.7089 - val_f1_macro: 0.4584 - val_loss: 4.7225\n\nEpoch 37: Learning Rate is 1.30e-06\nEpoch 37/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 194ms/step - accuracy: 0.6177 - auc: 0.8070 - f1_macro: 0.4305 - loss: 3.4021 - val_accuracy: 0.5037 - val_auc: 0.7086 - val_f1_macro: 0.4611 - val_loss: 4.7083\n\nEpoch 38: Learning Rate is 1.16e-06\nEpoch 38/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 188ms/step - accuracy: 0.6594 - auc: 0.8330 - f1_macro: 0.4631 - loss: 3.0056 - val_accuracy: 0.5033 - val_auc: 0.7075 - val_f1_macro: 0.4613 - val_loss: 4.7153\n\nEpoch 39: Learning Rate is 1.03e-06\nEpoch 39/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 179ms/step - accuracy: 0.6388 - auc: 0.8197 - f1_macro: 0.4493 - loss: 3.2246 - val_accuracy: 0.5065 - val_auc: 0.7106 - val_f1_macro: 0.4623 - val_loss: 4.6909\n\nEpoch 40: Learning Rate is 9.06e-07\nEpoch 40/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 195ms/step - accuracy: 0.6487 - auc: 0.8271 - f1_macro: 0.4544 - loss: 3.1506 - val_accuracy: 0.5089 - val_auc: 0.7106 - val_f1_macro: 0.4650 - val_loss: 4.6903\n\nEpoch 41: Learning Rate is 7.89e-07\nEpoch 41/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 193ms/step - accuracy: 0.6171 - auc: 0.8069 - f1_macro: 0.4343 - loss: 3.2006 - val_accuracy: 0.5132 - val_auc: 0.7153 - val_f1_macro: 0.4641 - val_loss: 4.6618\n\nEpoch 42: Learning Rate is 6.78e-07\nEpoch 42/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 185ms/step - accuracy: 0.6111 - auc: 0.8090 - f1_macro: 0.4369 - loss: 3.2600 - val_accuracy: 0.5108 - val_auc: 0.7164 - val_f1_macro: 0.4606 - val_loss: 4.6556\n\nEpoch 43: Learning Rate is 5.74e-07\nEpoch 43/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.6023 - auc: 0.7971 - f1_macro: 0.4311 - loss: 3.3294 - val_accuracy: 0.5162 - val_auc: 0.7178 - val_f1_macro: 0.4669 - val_loss: 4.6441\n\nEpoch 44: Learning Rate is 4.77e-07\nEpoch 44/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 195ms/step - accuracy: 0.5950 - auc: 0.8001 - f1_macro: 0.4223 - loss: 3.2802 - val_accuracy: 0.5153 - val_auc: 0.7193 - val_f1_macro: 0.4644 - val_loss: 4.6384\n\nEpoch 45: Learning Rate is 3.89e-07\nEpoch 45/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 186ms/step - accuracy: 0.5529 - auc: 0.7754 - f1_macro: 0.4071 - loss: 3.4798 - val_accuracy: 0.5167 - val_auc: 0.7192 - val_f1_macro: 0.4626 - val_loss: 4.6438\n\nEpoch 46: Learning Rate is 3.09e-07\nEpoch 46/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 186ms/step - accuracy: 0.5761 - auc: 0.7791 - f1_macro: 0.4245 - loss: 3.4094 - val_accuracy: 0.5166 - val_auc: 0.7212 - val_f1_macro: 0.4609 - val_loss: 4.6400\n\nEpoch 47: Learning Rate is 2.38e-07\nEpoch 47/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 195ms/step - accuracy: 0.5923 - auc: 0.7962 - f1_macro: 0.4267 - loss: 3.2301 - val_accuracy: 0.5199 - val_auc: 0.7222 - val_f1_macro: 0.4601 - val_loss: 4.6227\n\nEpoch 48: Learning Rate is 1.76e-07\nEpoch 48/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 176ms/step - accuracy: 0.5460 - auc: 0.7768 - f1_macro: 0.4074 - loss: 3.4480 - val_accuracy: 0.5188 - val_auc: 0.7228 - val_f1_macro: 0.4581 - val_loss: 4.6208\n\nEpoch 49: Learning Rate is 1.22e-07\nEpoch 49/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.5472 - auc: 0.7679 - f1_macro: 0.3988 - loss: 3.5245 - val_accuracy: 0.5192 - val_auc: 0.7263 - val_f1_macro: 0.4594 - val_loss: 4.6069\n\nEpoch 50: Learning Rate is 7.85e-08\nEpoch 50/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 176ms/step - accuracy: 0.5329 - auc: 0.7643 - f1_macro: 0.3958 - loss: 3.6010 - val_accuracy: 0.5201 - val_auc: 0.7260 - val_f1_macro: 0.4610 - val_loss: 4.6042\n\nEpoch 51: Learning Rate is 4.43e-08\nEpoch 51/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 192ms/step - accuracy: 0.5595 - auc: 0.7723 - f1_macro: 0.4122 - loss: 3.4858 - val_accuracy: 0.5221 - val_auc: 0.7263 - val_f1_macro: 0.4593 - val_loss: 4.5990\n\nEpoch 52: Learning Rate is 1.97e-08\nEpoch 52/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.5566 - auc: 0.7649 - f1_macro: 0.4141 - loss: 3.5233 - val_accuracy: 0.5208 - val_auc: 0.7255 - val_f1_macro: 0.4572 - val_loss: 4.6069\n\nEpoch 53: Learning Rate is 4.93e-09\nEpoch 53/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 179ms/step - accuracy: 0.5278 - auc: 0.7511 - f1_macro: 0.3870 - loss: 3.5703 - val_accuracy: 0.5234 - val_auc: 0.7280 - val_f1_macro: 0.4576 - val_loss: 4.5900\n\nEpoch 54: Learning Rate is 4.50e-06\nEpoch 54/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 190ms/step - accuracy: 0.5265 - auc: 0.7608 - f1_macro: 0.3802 - loss: 3.6930 - val_accuracy: 0.5223 - val_auc: 0.7278 - val_f1_macro: 0.4635 - val_loss: 4.5621\n\nEpoch 55: Learning Rate is 4.50e-06\nEpoch 55/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 181ms/step - accuracy: 0.5488 - auc: 0.7592 - f1_macro: 0.4017 - loss: 3.5941 - val_accuracy: 0.5236 - val_auc: 0.7298 - val_f1_macro: 0.4716 - val_loss: 4.5299\n\nEpoch 56: Learning Rate is 4.50e-06\nEpoch 56/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.5087 - auc: 0.7403 - f1_macro: 0.3701 - loss: 3.7655 - val_accuracy: 0.5188 - val_auc: 0.7312 - val_f1_macro: 0.4701 - val_loss: 4.4792\n\nEpoch 57: Learning Rate is 4.49e-06\nEpoch 57/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 158ms/step - accuracy: 0.4885 - auc: 0.7405 - f1_macro: 0.3713 - loss: 3.7384 - val_accuracy: 0.5208 - val_auc: 0.7313 - val_f1_macro: 0.4707 - val_loss: 4.4727\n\nEpoch 58: Learning Rate is 4.48e-06\nEpoch 58/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 183ms/step - accuracy: 0.5055 - auc: 0.7420 - f1_macro: 0.3804 - loss: 3.6845 - val_accuracy: 0.5273 - val_auc: 0.7361 - val_f1_macro: 0.4734 - val_loss: 4.4313\n\nEpoch 59: Learning Rate is 4.47e-06\nEpoch 59/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 167ms/step - accuracy: 0.5063 - auc: 0.7458 - f1_macro: 0.3826 - loss: 3.6750 - val_accuracy: 0.5281 - val_auc: 0.7358 - val_f1_macro: 0.4770 - val_loss: 4.4098\n\nEpoch 60: Learning Rate is 4.46e-06\nEpoch 60/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4962 - auc: 0.7383 - f1_macro: 0.3856 - loss: 3.8709 - val_accuracy: 0.5312 - val_auc: 0.7403 - val_f1_macro: 0.4754 - val_loss: 4.3735\n\nEpoch 61: Learning Rate is 4.45e-06\nEpoch 61/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 189ms/step - accuracy: 0.4912 - auc: 0.7407 - f1_macro: 0.3757 - loss: 3.6754 - val_accuracy: 0.5286 - val_auc: 0.7400 - val_f1_macro: 0.4711 - val_loss: 4.3676\n\nEpoch 62: Learning Rate is 4.43e-06\nEpoch 62/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4795 - auc: 0.7303 - f1_macro: 0.3623 - loss: 3.8833 - val_accuracy: 0.5318 - val_auc: 0.7415 - val_f1_macro: 0.4750 - val_loss: 4.3644\n\nEpoch 63: Learning Rate is 4.41e-06\nEpoch 63/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 170ms/step - accuracy: 0.5006 - auc: 0.7493 - f1_macro: 0.3979 - loss: 3.7822 - val_accuracy: 0.5346 - val_auc: 0.7480 - val_f1_macro: 0.4718 - val_loss: 4.3138\n\nEpoch 64: Learning Rate is 4.39e-06\nEpoch 64/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 180ms/step - accuracy: 0.4826 - auc: 0.7333 - f1_macro: 0.3719 - loss: 3.9402 - val_accuracy: 0.5350 - val_auc: 0.7437 - val_f1_macro: 0.4751 - val_loss: 4.3279\n\nEpoch 65: Learning Rate is 4.37e-06\nEpoch 65/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 154ms/step - accuracy: 0.4708 - auc: 0.7241 - f1_macro: 0.3739 - loss: 4.0427 - val_accuracy: 0.5352 - val_auc: 0.7486 - val_f1_macro: 0.4702 - val_loss: 4.2884\n\nEpoch 66: Learning Rate is 4.34e-06\nEpoch 66/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.4878 - auc: 0.7447 - f1_macro: 0.3875 - loss: 3.7183 - val_accuracy: 0.5417 - val_auc: 0.7540 - val_f1_macro: 0.4691 - val_loss: 4.2668\n\nEpoch 67: Learning Rate is 4.31e-06\nEpoch 67/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 170ms/step - accuracy: 0.4588 - auc: 0.7257 - f1_macro: 0.3754 - loss: 4.0105 - val_accuracy: 0.5441 - val_auc: 0.7558 - val_f1_macro: 0.4672 - val_loss: 4.2665\n\nEpoch 68: Learning Rate is 4.29e-06\nEpoch 68/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 185ms/step - accuracy: 0.4590 - auc: 0.7368 - f1_macro: 0.3648 - loss: 3.9598 - val_accuracy: 0.5394 - val_auc: 0.7579 - val_f1_macro: 0.4593 - val_loss: 4.2641\n\nEpoch 69: Learning Rate is 4.25e-06\nEpoch 69/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 207ms/step - accuracy: 0.4530 - auc: 0.7235 - f1_macro: 0.3666 - loss: 4.0320 - val_accuracy: 0.5430 - val_auc: 0.7609 - val_f1_macro: 0.4551 - val_loss: 4.2496\n\nEpoch 70: Learning Rate is 4.22e-06\nEpoch 70/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.4753 - auc: 0.7273 - f1_macro: 0.3965 - loss: 4.1454 - val_accuracy: 0.5454 - val_auc: 0.7634 - val_f1_macro: 0.4562 - val_loss: 4.2308\n\nEpoch 71: Learning Rate is 4.19e-06\nEpoch 71/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 191ms/step - accuracy: 0.4555 - auc: 0.7282 - f1_macro: 0.3712 - loss: 4.1392 - val_accuracy: 0.5445 - val_auc: 0.7651 - val_f1_macro: 0.4501 - val_loss: 4.2197\n\nEpoch 72: Learning Rate is 4.15e-06\nEpoch 72/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 172ms/step - accuracy: 0.4448 - auc: 0.7178 - f1_macro: 0.3724 - loss: 4.6248 - val_accuracy: 0.5482 - val_auc: 0.7667 - val_f1_macro: 0.4528 - val_loss: 4.2073\n\nEpoch 73: Learning Rate is 4.11e-06\nEpoch 73/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 186ms/step - accuracy: 0.4589 - auc: 0.7371 - f1_macro: 0.3820 - loss: 4.2897 - val_accuracy: 0.5478 - val_auc: 0.7697 - val_f1_macro: 0.4484 - val_loss: 4.1992\n\nEpoch 74: Learning Rate is 4.07e-06\nEpoch 74/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 180ms/step - accuracy: 0.4657 - auc: 0.7415 - f1_macro: 0.3944 - loss: 4.3291 - val_accuracy: 0.5480 - val_auc: 0.7716 - val_f1_macro: 0.4429 - val_loss: 4.1814\n\nEpoch 75: Learning Rate is 4.03e-06\nEpoch 75/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 190ms/step - accuracy: 0.4372 - auc: 0.7157 - f1_macro: 0.3709 - loss: 4.7544 - val_accuracy: 0.5482 - val_auc: 0.7713 - val_f1_macro: 0.4452 - val_loss: 4.1584\n\nEpoch 76: Learning Rate is 3.98e-06\nEpoch 76/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 177ms/step - accuracy: 0.4516 - auc: 0.7296 - f1_macro: 0.3905 - loss: 4.6162 - val_accuracy: 0.5471 - val_auc: 0.7741 - val_f1_macro: 0.4434 - val_loss: 4.1563\n\nEpoch 77: Learning Rate is 3.94e-06\nEpoch 77/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 180ms/step - accuracy: 0.4477 - auc: 0.7203 - f1_macro: 0.3746 - loss: 4.6762 - val_accuracy: 0.5487 - val_auc: 0.7735 - val_f1_macro: 0.4444 - val_loss: 4.1652\n\nEpoch 78: Learning Rate is 3.89e-06\nEpoch 78/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 190ms/step - accuracy: 0.4524 - auc: 0.7310 - f1_macro: 0.3862 - loss: 4.5952 - val_accuracy: 0.5495 - val_auc: 0.7741 - val_f1_macro: 0.4458 - val_loss: 4.1672\n\nEpoch 79: Learning Rate is 3.84e-06\nEpoch 79/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 180ms/step - accuracy: 0.4359 - auc: 0.7212 - f1_macro: 0.3750 - loss: 4.8967 - val_accuracy: 0.5478 - val_auc: 0.7754 - val_f1_macro: 0.4411 - val_loss: 4.1520\n\nEpoch 80: Learning Rate is 3.79e-06\nEpoch 80/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 169ms/step - accuracy: 0.4334 - auc: 0.7230 - f1_macro: 0.3754 - loss: 4.9498 - val_accuracy: 0.5493 - val_auc: 0.7772 - val_f1_macro: 0.4401 - val_loss: 4.1558\n\nEpoch 81: Learning Rate is 3.74e-06\nEpoch 81/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 192ms/step - accuracy: 0.4602 - auc: 0.7304 - f1_macro: 0.3952 - loss: 4.5785 - val_accuracy: 0.5493 - val_auc: 0.7770 - val_f1_macro: 0.4376 - val_loss: 4.1582\n\nEpoch 82: Learning Rate is 3.68e-06\nEpoch 82/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 169ms/step - accuracy: 0.4655 - auc: 0.7298 - f1_macro: 0.3983 - loss: 4.7012 - val_accuracy: 0.5485 - val_auc: 0.7774 - val_f1_macro: 0.4341 - val_loss: 4.1480\n\nEpoch 83: Learning Rate is 3.63e-06\nEpoch 83/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 179ms/step - accuracy: 0.4595 - auc: 0.7332 - f1_macro: 0.3993 - loss: 4.7092 - val_accuracy: 0.5502 - val_auc: 0.7769 - val_f1_macro: 0.4377 - val_loss: 4.1478\n\nEpoch 84: Learning Rate is 3.57e-06\nEpoch 84/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 174ms/step - accuracy: 0.4349 - auc: 0.7205 - f1_macro: 0.3747 - loss: 4.9317 - val_accuracy: 0.5491 - val_auc: 0.7782 - val_f1_macro: 0.4335 - val_loss: 4.1566\n\nEpoch 85: Learning Rate is 3.51e-06\nEpoch 85/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 185ms/step - accuracy: 0.4187 - auc: 0.7086 - f1_macro: 0.3641 - loss: 5.0351 - val_accuracy: 0.5485 - val_auc: 0.7776 - val_f1_macro: 0.4315 - val_loss: 4.1557\n\nEpoch 86: Learning Rate is 3.46e-06\nEpoch 86/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 183ms/step - accuracy: 0.4242 - auc: 0.7132 - f1_macro: 0.3688 - loss: 4.7933 - val_accuracy: 0.5439 - val_auc: 0.7774 - val_f1_macro: 0.4267 - val_loss: 4.1618\n\nEpoch 87: Learning Rate is 3.40e-06\nEpoch 87/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 176ms/step - accuracy: 0.4478 - auc: 0.7267 - f1_macro: 0.3845 - loss: 4.6347 - val_accuracy: 0.5461 - val_auc: 0.7795 - val_f1_macro: 0.4244 - val_loss: 4.1554\n\nEpoch 88: Learning Rate is 3.33e-06\nEpoch 88/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 195ms/step - accuracy: 0.4478 - auc: 0.7240 - f1_macro: 0.3818 - loss: 4.6576 - val_accuracy: 0.5454 - val_auc: 0.7808 - val_f1_macro: 0.4208 - val_loss: 4.1392\n\nEpoch 89: Learning Rate is 3.27e-06\nEpoch 89/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 186ms/step - accuracy: 0.4103 - auc: 0.7127 - f1_macro: 0.3585 - loss: 4.7118 - val_accuracy: 0.5461 - val_auc: 0.7795 - val_f1_macro: 0.4234 - val_loss: 4.1361\n\nEpoch 90: Learning Rate is 3.21e-06\nEpoch 90/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 185ms/step - accuracy: 0.4346 - auc: 0.7210 - f1_macro: 0.3788 - loss: 4.5016 - val_accuracy: 0.5463 - val_auc: 0.7816 - val_f1_macro: 0.4194 - val_loss: 4.1424\n\nEpoch 91: Learning Rate is 3.14e-06\nEpoch 91/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 180ms/step - accuracy: 0.4520 - auc: 0.7149 - f1_macro: 0.3713 - loss: 4.4929 - val_accuracy: 0.5450 - val_auc: 0.7811 - val_f1_macro: 0.4155 - val_loss: 4.1479\n\nEpoch 92: Learning Rate is 3.08e-06\nEpoch 92/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 177ms/step - accuracy: 0.4260 - auc: 0.7189 - f1_macro: 0.3581 - loss: 4.4502 - val_accuracy: 0.5430 - val_auc: 0.7817 - val_f1_macro: 0.4106 - val_loss: 4.1393\n\nEpoch 93: Learning Rate is 3.01e-06\nEpoch 93/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 174ms/step - accuracy: 0.4506 - auc: 0.7228 - f1_macro: 0.3665 - loss: 4.3995 - val_accuracy: 0.5435 - val_auc: 0.7816 - val_f1_macro: 0.4101 - val_loss: 4.1616\n\nEpoch 94: Learning Rate is 2.95e-06\nEpoch 94/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 171ms/step - accuracy: 0.4506 - auc: 0.7249 - f1_macro: 0.3704 - loss: 4.2494 - val_accuracy: 0.5435 - val_auc: 0.7823 - val_f1_macro: 0.4065 - val_loss: 4.1687\n\nEpoch 95: Learning Rate is 2.88e-06\nEpoch 95/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 179ms/step - accuracy: 0.4490 - auc: 0.7192 - f1_macro: 0.3646 - loss: 4.3364 - val_accuracy: 0.5420 - val_auc: 0.7828 - val_f1_macro: 0.4037 - val_loss: 4.1815\n\nEpoch 96: Learning Rate is 2.81e-06\nEpoch 96/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 169ms/step - accuracy: 0.4262 - auc: 0.7122 - f1_macro: 0.3420 - loss: 4.4243 - val_accuracy: 0.5415 - val_auc: 0.7835 - val_f1_macro: 0.4046 - val_loss: 4.1849\n\nEpoch 97: Learning Rate is 2.74e-06\nEpoch 97/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 174ms/step - accuracy: 0.4453 - auc: 0.7145 - f1_macro: 0.3571 - loss: 4.3125 - val_accuracy: 0.5417 - val_auc: 0.7840 - val_f1_macro: 0.4010 - val_loss: 4.1894\n\nEpoch 98: Learning Rate is 2.67e-06\nEpoch 98/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.4243 - auc: 0.7156 - f1_macro: 0.3422 - loss: 4.2486 - val_accuracy: 0.5437 - val_auc: 0.7825 - val_f1_macro: 0.4022 - val_loss: 4.1973\n\nEpoch 99: Learning Rate is 2.60e-06\nEpoch 99/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 186ms/step - accuracy: 0.4165 - auc: 0.7170 - f1_macro: 0.3283 - loss: 4.2270 - val_accuracy: 0.5417 - val_auc: 0.7826 - val_f1_macro: 0.3981 - val_loss: 4.1859\n\nEpoch 100: Learning Rate is 2.53e-06\nEpoch 100/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 181ms/step - accuracy: 0.4569 - auc: 0.7215 - f1_macro: 0.3533 - loss: 4.2163 - val_accuracy: 0.5428 - val_auc: 0.7821 - val_f1_macro: 0.4015 - val_loss: 4.1747\n\nEpoch 101: Learning Rate is 2.46e-06\nEpoch 101/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 178ms/step - accuracy: 0.4250 - auc: 0.7034 - f1_macro: 0.3292 - loss: 4.3528 - val_accuracy: 0.5430 - val_auc: 0.7809 - val_f1_macro: 0.3983 - val_loss: 4.2077\n\nEpoch 102: Learning Rate is 2.39e-06\nEpoch 102/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 189ms/step - accuracy: 0.4295 - auc: 0.7165 - f1_macro: 0.3301 - loss: 4.2014 - val_accuracy: 0.5415 - val_auc: 0.7814 - val_f1_macro: 0.3955 - val_loss: 4.2141\n\nEpoch 103: Learning Rate is 2.32e-06\nEpoch 103/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 167ms/step - accuracy: 0.4047 - auc: 0.7038 - f1_macro: 0.3136 - loss: 4.2099 - val_accuracy: 0.5426 - val_auc: 0.7824 - val_f1_macro: 0.3966 - val_loss: 4.2117\n\nEpoch 104: Learning Rate is 2.25e-06\nEpoch 104/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 178ms/step - accuracy: 0.4389 - auc: 0.7246 - f1_macro: 0.3280 - loss: 4.0471 - val_accuracy: 0.5445 - val_auc: 0.7831 - val_f1_macro: 0.3998 - val_loss: 4.2012\n\nEpoch 105: Learning Rate is 2.18e-06\nEpoch 105/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.3944 - auc: 0.6947 - f1_macro: 0.3007 - loss: 4.3384 - val_accuracy: 0.5432 - val_auc: 0.7836 - val_f1_macro: 0.3967 - val_loss: 4.2190\n\nEpoch 106: Learning Rate is 2.11e-06\nEpoch 106/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 189ms/step - accuracy: 0.4207 - auc: 0.7141 - f1_macro: 0.3099 - loss: 4.1599 - val_accuracy: 0.5419 - val_auc: 0.7818 - val_f1_macro: 0.3937 - val_loss: 4.2139\n\nEpoch 107: Learning Rate is 2.04e-06\nEpoch 107/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 185ms/step - accuracy: 0.4009 - auc: 0.6957 - f1_macro: 0.3025 - loss: 4.2581 - val_accuracy: 0.5437 - val_auc: 0.7833 - val_f1_macro: 0.3978 - val_loss: 4.2177\n\nEpoch 108: Learning Rate is 1.97e-06\nEpoch 108/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.4860 - auc: 0.7502 - f1_macro: 0.4709 - loss: 3.7760 - val_accuracy: 0.5435 - val_auc: 0.7834 - val_f1_macro: 0.3988 - val_loss: 4.2026\n\nEpoch 109: Learning Rate is 1.90e-06\nEpoch 109/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 170ms/step - accuracy: 0.4972 - auc: 0.7625 - f1_macro: 0.4925 - loss: 3.6346 - val_accuracy: 0.5435 - val_auc: 0.7855 - val_f1_macro: 0.4008 - val_loss: 4.1921\n\nEpoch 110: Learning Rate is 1.83e-06\nEpoch 110/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.4810 - auc: 0.7499 - f1_macro: 0.4721 - loss: 3.7082 - val_accuracy: 0.5437 - val_auc: 0.7852 - val_f1_macro: 0.4025 - val_loss: 4.1714\n\nEpoch 111: Learning Rate is 1.76e-06\nEpoch 111/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.5209 - auc: 0.7712 - f1_macro: 0.5007 - loss: 3.5296 - val_accuracy: 0.5437 - val_auc: 0.7847 - val_f1_macro: 0.4043 - val_loss: 4.1629\n\nEpoch 112: Learning Rate is 1.69e-06\nEpoch 112/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.5499 - auc: 0.7898 - f1_macro: 0.5018 - loss: 3.3283 - val_accuracy: 0.5443 - val_auc: 0.7853 - val_f1_macro: 0.4068 - val_loss: 4.1551\n\nEpoch 113: Learning Rate is 1.62e-06\nEpoch 113/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 180ms/step - accuracy: 0.5408 - auc: 0.7970 - f1_macro: 0.4921 - loss: 3.2523 - val_accuracy: 0.5445 - val_auc: 0.7852 - val_f1_macro: 0.4071 - val_loss: 4.1467\n\nEpoch 114: Learning Rate is 1.55e-06\nEpoch 114/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 172ms/step - accuracy: 0.5558 - auc: 0.8043 - f1_macro: 0.4901 - loss: 3.1783 - val_accuracy: 0.5422 - val_auc: 0.7853 - val_f1_macro: 0.4070 - val_loss: 4.1196\n\nEpoch 115: Learning Rate is 1.49e-06\nEpoch 115/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 170ms/step - accuracy: 0.5952 - auc: 0.8227 - f1_macro: 0.5041 - loss: 2.9339 - val_accuracy: 0.5424 - val_auc: 0.7854 - val_f1_macro: 0.4074 - val_loss: 4.1179\n\nEpoch 116: Learning Rate is 1.42e-06\nEpoch 116/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 189ms/step - accuracy: 0.5960 - auc: 0.8163 - f1_macro: 0.5082 - loss: 3.0488 - val_accuracy: 0.5417 - val_auc: 0.7845 - val_f1_macro: 0.4079 - val_loss: 4.1024\n\nEpoch 117: Learning Rate is 1.36e-06\nEpoch 117/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 182ms/step - accuracy: 0.6151 - auc: 0.8306 - f1_macro: 0.5038 - loss: 2.8539 - val_accuracy: 0.5406 - val_auc: 0.7841 - val_f1_macro: 0.4078 - val_loss: 4.1073\n\nEpoch 118: Learning Rate is 1.29e-06\nEpoch 118/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 155ms/step - accuracy: 0.6082 - auc: 0.8295 - f1_macro: 0.5022 - loss: 2.8394 - val_accuracy: 0.5420 - val_auc: 0.7840 - val_f1_macro: 0.4111 - val_loss: 4.0805\n\nEpoch 119: Learning Rate is 1.23e-06\nEpoch 119/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 181ms/step - accuracy: 0.6238 - auc: 0.8378 - f1_macro: 0.4952 - loss: 2.7958 - val_accuracy: 0.5417 - val_auc: 0.7840 - val_f1_macro: 0.4118 - val_loss: 4.0874\n\nEpoch 120: Learning Rate is 1.17e-06\nEpoch 120/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 183ms/step - accuracy: 0.6718 - auc: 0.8584 - f1_macro: 0.5339 - loss: 2.5223 - val_accuracy: 0.5411 - val_auc: 0.7842 - val_f1_macro: 0.4127 - val_loss: 4.0756\n\nEpoch 121: Learning Rate is 1.10e-06\nEpoch 121/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 181ms/step - accuracy: 0.6415 - auc: 0.8492 - f1_macro: 0.4992 - loss: 2.6486 - val_accuracy: 0.5381 - val_auc: 0.7827 - val_f1_macro: 0.4133 - val_loss: 4.0719\n\nEpoch 122: Learning Rate is 1.04e-06\nEpoch 122/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 174ms/step - accuracy: 0.6579 - auc: 0.8606 - f1_macro: 0.5123 - loss: 2.5498 - val_accuracy: 0.5385 - val_auc: 0.7819 - val_f1_macro: 0.4154 - val_loss: 4.0682\n\nEpoch 123: Learning Rate is 9.85e-07\nEpoch 123/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 194ms/step - accuracy: 0.6516 - auc: 0.8557 - f1_macro: 0.4944 - loss: 2.5629 - val_accuracy: 0.5381 - val_auc: 0.7819 - val_f1_macro: 0.4160 - val_loss: 4.0620\n\nEpoch 124: Learning Rate is 9.27e-07\nEpoch 124/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 180ms/step - accuracy: 0.6702 - auc: 0.8663 - f1_macro: 0.5097 - loss: 2.4710 - val_accuracy: 0.5383 - val_auc: 0.7819 - val_f1_macro: 0.4189 - val_loss: 4.0502\n\nEpoch 125: Learning Rate is 8.71e-07\nEpoch 125/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 181ms/step - accuracy: 0.6900 - auc: 0.8740 - f1_macro: 0.5106 - loss: 2.3362 - val_accuracy: 0.5350 - val_auc: 0.7811 - val_f1_macro: 0.4152 - val_loss: 4.0533\n\nEpoch 126: Learning Rate is 8.16e-07\nEpoch 126/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 188ms/step - accuracy: 0.6750 - auc: 0.8678 - f1_macro: 0.5062 - loss: 2.4592 - val_accuracy: 0.5348 - val_auc: 0.7790 - val_f1_macro: 0.4172 - val_loss: 4.0542\n\nEpoch 127: Learning Rate is 7.62e-07\nEpoch 127/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 193ms/step - accuracy: 0.7026 - auc: 0.8818 - f1_macro: 0.5272 - loss: 2.2197 - val_accuracy: 0.5327 - val_auc: 0.7781 - val_f1_macro: 0.4164 - val_loss: 4.0574\n\nEpoch 128: Learning Rate is 7.10e-07\nEpoch 128/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 183ms/step - accuracy: 0.7041 - auc: 0.8838 - f1_macro: 0.5105 - loss: 2.2288 - val_accuracy: 0.5309 - val_auc: 0.7763 - val_f1_macro: 0.4182 - val_loss: 4.0661\n\nEpoch 129: Learning Rate is 6.59e-07\nEpoch 129/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.7292 - auc: 0.8950 - f1_macro: 0.5362 - loss: 2.1133 - val_accuracy: 0.5285 - val_auc: 0.7741 - val_f1_macro: 0.4179 - val_loss: 4.0719\n\nEpoch 130: Learning Rate is 6.10e-07\nEpoch 130/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 181ms/step - accuracy: 0.7148 - auc: 0.8885 - f1_macro: 0.5094 - loss: 2.2123 - val_accuracy: 0.5273 - val_auc: 0.7730 - val_f1_macro: 0.4184 - val_loss: 4.0760\n\nEpoch 131: Learning Rate is 5.62e-07\nEpoch 131/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 153ms/step - accuracy: 0.7373 - auc: 0.8964 - f1_macro: 0.5360 - loss: 2.0151 - val_accuracy: 0.5259 - val_auc: 0.7720 - val_f1_macro: 0.4201 - val_loss: 4.0767\n\nEpoch 132: Learning Rate is 5.16e-07\nEpoch 132/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.7342 - auc: 0.8943 - f1_macro: 0.5221 - loss: 2.1152 - val_accuracy: 0.5255 - val_auc: 0.7714 - val_f1_macro: 0.4216 - val_loss: 4.0789\n\nEpoch 133: Learning Rate is 4.72e-07\nEpoch 133/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.7413 - auc: 0.8980 - f1_macro: 0.5316 - loss: 2.0326 - val_accuracy: 0.5233 - val_auc: 0.7702 - val_f1_macro: 0.4209 - val_loss: 4.0871\n\nEpoch 134: Learning Rate is 4.30e-07\nEpoch 134/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.7465 - auc: 0.9055 - f1_macro: 0.5298 - loss: 1.9331 - val_accuracy: 0.5233 - val_auc: 0.7708 - val_f1_macro: 0.4198 - val_loss: 4.0732\nEpoch 134: early stopping\nRestoring model weights from the end of the best epoch: 59.\nĐã lưu mô hình cho Fold 4 tại: /kaggle/working/output_results/EfficienetB0_CV_TPU_fold_4.keras\nĐang tạo và lưu biểu đồ huấn luyện...\nĐã lưu biểu đồ cho Fold 4 tại: /kaggle/working/output_results/fold_4_metrics.png\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756910341.225022      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:4993359070636057906\nI0000 00:00:1756910342.677114     889 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(7679374278363856093), session_name()\nI0000 00:00:1756910355.346576     889 tpu_compile_op_common.cc:245] Compilation of 7679374278363856093 with session name  took 12.669122163s and succeeded\nI0000 00:00:1756910355.366315     889 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(7679374278363856093), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_4993359070636057906\", property.function_library_fingerprint = 3059030914162191735, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756910355.366361     889 tpu_compilation_cache_interface.cc:542] After adding entry for key 7679374278363856093 with session_name  cache is 30 entries (1889664296 bytes),  marked for eviction 0 entries (0 bytes).\nI0000 00:00:1756910357.661893     882 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(10097947904876023804), session_name()\nI0000 00:00:1756910370.665005     882 tpu_compile_op_common.cc:245] Compilation of 10097947904876023804 with session name  took 13.003053318s and succeeded\nI0000 00:00:1756910370.686046     882 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(10097947904876023804), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_4993359070636057906\", property.function_library_fingerprint = 3059030914162191735, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"25,224,224,3,;25,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756910370.686088     882 tpu_compilation_cache_interface.cc:542] After adding entry for key 10097947904876023804 with session_name  cache is 31 entries (1917135883 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"Fold 4 - Validation Loss: 4.3541, Validation Accuracy: 0.5371, Validation AUC: 0.7419, Validation F1-Macro: 0.4771\n==================================================\nKết quả Cross-Validation:\nValidation Accuracy trung bình: 0.5270 +/- 0.0085\nValidation Loss trung bình: 4.2143 +/- 0.1138\nValidation AUC trung bình: 0.7476 +/- 0.0098\nValidation F1-Macro trung bình: 0.4707 +/- 0.0160\n==================================================\n--------------------------------------------------\nBắt đầu Fold 5/5\n--------------------------------------------------\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fcc54868d30> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcc54868d30>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fcc54868d30> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcc54868d30>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function <lambda> at 0x7fcc54868d30> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcc54868d30>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fcaa0596440> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcaa0596440>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fcaa0596440> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcaa0596440>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function <lambda> at 0x7fcaa0596440> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcaa0596440>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == train_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fce540fb1c0> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fce540fb1c0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fce540fb1c0> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fce540fb1c0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function <lambda> at 0x7fce540fb1c0> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fce540fb1c0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fcb925a5a20> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcb925a5a20>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fcb925a5a20> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcb925a5a20>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function <lambda> at 0x7fcb925a5a20> and will run it as-is.\nCause: could not parse the source code of <function <lambda> at 0x7fcb925a5a20>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda i, data: data\n\nMatch 1:\nlambda i, data: tf.reduce_any(i == val_indices_tf)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nSố bước mỗi epoch: 87 | Số bước validation: 21\n\n--- Bắt đầu Giai đoạn 1: Huấn luyện các lớp cuối ---\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756910390.891924      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:12891552254319108348\nI0000 00:00:1756910393.555599     940 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(17015632347269623739), session_name()\nI0000 00:00:1756910419.606679     940 tpu_compile_op_common.cc:245] Compilation of 17015632347269623739 with session name  took 26.050984649s and succeeded\nI0000 00:00:1756910419.672932     940 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(17015632347269623739), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_12891552254319108348\", property.function_library_fingerprint = 4492831089384570478, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756910419.672985     940 tpu_compilation_cache_interface.cc:542] After adding entry for key 17015632347269623739 with session_name  cache is 32 entries (1993572912 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.6258 - auc: 0.7710 - loss: 6.7918","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756910440.707179      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:10432932651621129316\nI0000 00:00:1756910442.904529     947 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(8478824358013955853), session_name()\nI0000 00:00:1756910455.518575     947 tpu_compile_op_common.cc:245] Compilation of 8478824358013955853 with session name  took 12.613986775s and succeeded\nI0000 00:00:1756910455.541930     947 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(8478824358013955853), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_10432932651621129316\", property.function_library_fingerprint = 14326735119484653217, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756910455.541971     947 tpu_compilation_cache_interface.cc:542] After adding entry for key 8478824358013955853 with session_name  cache is 33 entries (2020219856 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 453ms/step - accuracy: 0.6251 - auc: 0.7705 - loss: 6.8165 - val_accuracy: 0.2667 - val_auc: 0.5309 - val_loss: 21.5463\nEpoch 2/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 155ms/step - accuracy: 0.2995 - auc: 0.5354 - loss: 16.0330 - val_accuracy: 0.3932 - val_auc: 0.5969 - val_loss: 17.2272\nEpoch 3/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.3735 - auc: 0.5955 - loss: 12.7910 - val_accuracy: 0.4076 - val_auc: 0.6079 - val_loss: 17.6566\nEpoch 4/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 180ms/step - accuracy: 0.3764 - auc: 0.6058 - loss: 12.4536 - val_accuracy: 0.4182 - val_auc: 0.6152 - val_loss: 17.1562\nEpoch 5/5\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.3859 - auc: 0.6072 - loss: 12.8890 - val_accuracy: 0.4226 - val_auc: 0.6183 - val_loss: 16.8913\n\n--- Bắt đầu Giai đoạn 2: Fine-tuning ---\nTổng số lớp trong base model: 238\nSẽ mở băng và huấn luyện 119 lớp cuối cùng (bắt đầu từ lớp 119).\n\n--- Giai đoạn 2A: Bắt đầu Warmup trong 3 epochs ---\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756910537.734251      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:16126906635174007584\nI0000 00:00:1756910543.421926     897 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(10982933545336000643), session_name()\nI0000 00:00:1756910580.876058     897 tpu_compile_op_common.cc:245] Compilation of 10982933545336000643 with session name  took 37.454078257s and succeeded\nI0000 00:00:1756910581.000142     897 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(10982933545336000643), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_16126906635174007584\", property.function_library_fingerprint = 4148868128972511691, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756910581.000206     897 tpu_compilation_cache_interface.cc:542] After adding entry for key 10982933545336000643 with session_name  cache is 34 entries (2157215189 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.3802 - f1_macro: 0.2018 - loss: 6.9095","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756910600.038064      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:3569686915815734926\nI0000 00:00:1756910601.338145     903 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(13344343737307879248), session_name()\nI0000 00:00:1756910613.949007     903 tpu_compile_op_common.cc:245] Compilation of 13344343737307879248 with session name  took 12.610451313s and succeeded\nI0000 00:00:1756910613.974732     903 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(13344343737307879248), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_3569686915815734926\", property.function_library_fingerprint = 13754115811323722201, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756910613.974790     903 tpu_compilation_cache_interface.cc:542] After adding entry for key 13344343737307879248 with session_name  cache is 35 entries (2183003788 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 412ms/step - accuracy: 0.3787 - f1_macro: 0.2019 - loss: 6.9365 - val_accuracy: 0.3265 - val_f1_macro: 0.2907 - val_loss: 6.6529\nEpoch 2/3\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.3740 - f1_macro: 0.2071 - loss: 7.0419 - val_accuracy: 0.3666 - val_f1_macro: 0.3274 - val_loss: 7.0638\nEpoch 3/3\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 171ms/step - accuracy: 0.3595 - f1_macro: 0.2049 - loss: 7.2051 - val_accuracy: 0.3452 - val_f1_macro: 0.2777 - val_loss: 7.2393\n\n--- Giai đoạn 2B: Bắt đầu huấn luyện chính ---\nSử dụng scheduler: CosineDecayRestarts\n\nEpoch 4: Learning Rate is 5.00e-06\nEpoch 4/200\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756910671.871752      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:11678124864831075964\nI0000 00:00:1756910678.087927     951 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(2040063779233264683), session_name()\nI0000 00:00:1756910716.885201     951 tpu_compile_op_common.cc:245] Compilation of 2040063779233264683 with session name  took 38.79722915s and succeeded\nI0000 00:00:1756910717.039082     951 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(2040063779233264683), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_11678124864831075964\", property.function_library_fingerprint = 12825590225206992135, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756910717.039144     951 tpu_compilation_cache_interface.cc:542] After adding entry for key 2040063779233264683 with session_name  cache is 36 entries (2321969511 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.4124 - auc: 0.6216 - f1_macro: 0.2088 - loss: 6.2783","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756910736.529493      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:8491900130754903393\nI0000 00:00:1756910737.765494     901 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(9251591330270504796), session_name()\nI0000 00:00:1756910750.839646     901 tpu_compile_op_common.cc:245] Compilation of 9251591330270504796 with session name  took 13.07377707s and succeeded\nI0000 00:00:1756910750.871468     901 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(9251591330270504796), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_8491900130754903393\", property.function_library_fingerprint = 554851425115163362, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756910750.871508     901 tpu_compilation_cache_interface.cc:542] After adding entry for key 9251591330270504796 with session_name  cache is 37 entries (2349493590 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 428ms/step - accuracy: 0.4106 - auc: 0.6202 - f1_macro: 0.2090 - loss: 6.3022 - val_accuracy: 0.4018 - val_auc: 0.6182 - val_f1_macro: 0.3576 - val_loss: 5.7184\n\nEpoch 5: Learning Rate is 5.00e-06\nEpoch 5/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 185ms/step - accuracy: 0.3706 - auc: 0.6101 - f1_macro: 0.2246 - loss: 6.1609 - val_accuracy: 0.4183 - val_auc: 0.6570 - val_f1_macro: 0.3911 - val_loss: 5.4105\n\nEpoch 6: Learning Rate is 4.98e-06\nEpoch 6/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 179ms/step - accuracy: 0.3768 - auc: 0.6001 - f1_macro: 0.2745 - loss: 6.0784 - val_accuracy: 0.4319 - val_auc: 0.6678 - val_f1_macro: 0.4007 - val_loss: 5.3030\n\nEpoch 7: Learning Rate is 4.96e-06\nEpoch 7/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 199ms/step - accuracy: 0.3760 - auc: 0.6000 - f1_macro: 0.3086 - loss: 5.9803 - val_accuracy: 0.4403 - val_auc: 0.6741 - val_f1_macro: 0.4003 - val_loss: 5.2324\n\nEpoch 8: Learning Rate is 4.92e-06\nEpoch 8/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 199ms/step - accuracy: 0.3743 - auc: 0.5956 - f1_macro: 0.2974 - loss: 6.0065 - val_accuracy: 0.4479 - val_auc: 0.6759 - val_f1_macro: 0.3997 - val_loss: 5.1779\n\nEpoch 9: Learning Rate is 4.88e-06\nEpoch 9/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 193ms/step - accuracy: 0.3887 - auc: 0.6124 - f1_macro: 0.3225 - loss: 5.8325 - val_accuracy: 0.4533 - val_auc: 0.6744 - val_f1_macro: 0.3924 - val_loss: 5.1671\n\nEpoch 10: Learning Rate is 4.82e-06\nEpoch 10/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 189ms/step - accuracy: 0.4097 - auc: 0.6169 - f1_macro: 0.3350 - loss: 5.7293 - val_accuracy: 0.4598 - val_auc: 0.6771 - val_f1_macro: 0.3900 - val_loss: 5.1121\n\nEpoch 11: Learning Rate is 4.76e-06\nEpoch 11/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 178ms/step - accuracy: 0.4074 - auc: 0.6271 - f1_macro: 0.3348 - loss: 5.7082 - val_accuracy: 0.4717 - val_auc: 0.6787 - val_f1_macro: 0.3963 - val_loss: 5.0669\n\nEpoch 12: Learning Rate is 4.69e-06\nEpoch 12/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 179ms/step - accuracy: 0.4310 - auc: 0.6538 - f1_macro: 0.3531 - loss: 5.2844 - val_accuracy: 0.4693 - val_auc: 0.6812 - val_f1_macro: 0.3925 - val_loss: 5.0136\n\nEpoch 13: Learning Rate is 4.61e-06\nEpoch 13/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 204ms/step - accuracy: 0.4551 - auc: 0.6580 - f1_macro: 0.3636 - loss: 5.2314 - val_accuracy: 0.4775 - val_auc: 0.6852 - val_f1_macro: 0.3968 - val_loss: 4.9831\n\nEpoch 14: Learning Rate is 4.52e-06\nEpoch 14/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 186ms/step - accuracy: 0.4514 - auc: 0.6720 - f1_macro: 0.3677 - loss: 5.0554 - val_accuracy: 0.4754 - val_auc: 0.6867 - val_f1_macro: 0.3962 - val_loss: 4.9519\n\nEpoch 15: Learning Rate is 4.43e-06\nEpoch 15/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 194ms/step - accuracy: 0.4734 - auc: 0.6774 - f1_macro: 0.3704 - loss: 4.9513 - val_accuracy: 0.4732 - val_auc: 0.6869 - val_f1_macro: 0.3980 - val_loss: 4.8907\n\nEpoch 16: Learning Rate is 4.32e-06\nEpoch 16/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 192ms/step - accuracy: 0.5010 - auc: 0.6986 - f1_macro: 0.3959 - loss: 4.7408 - val_accuracy: 0.4870 - val_auc: 0.6900 - val_f1_macro: 0.4063 - val_loss: 4.8639\n\nEpoch 17: Learning Rate is 4.21e-06\nEpoch 17/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 193ms/step - accuracy: 0.5267 - auc: 0.7250 - f1_macro: 0.4008 - loss: 4.3266 - val_accuracy: 0.4831 - val_auc: 0.6905 - val_f1_macro: 0.4086 - val_loss: 4.8105\n\nEpoch 18: Learning Rate is 4.09e-06\nEpoch 18/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 200ms/step - accuracy: 0.5415 - auc: 0.7354 - f1_macro: 0.4080 - loss: 4.2806 - val_accuracy: 0.4896 - val_auc: 0.6941 - val_f1_macro: 0.4204 - val_loss: 4.7580\n\nEpoch 19: Learning Rate is 3.97e-06\nEpoch 19/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 186ms/step - accuracy: 0.5515 - auc: 0.7469 - f1_macro: 0.4044 - loss: 4.1413 - val_accuracy: 0.4929 - val_auc: 0.6939 - val_f1_macro: 0.4280 - val_loss: 4.7371\n\nEpoch 20: Learning Rate is 3.84e-06\nEpoch 20/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 200ms/step - accuracy: 0.5626 - auc: 0.7495 - f1_macro: 0.4208 - loss: 4.0140 - val_accuracy: 0.4918 - val_auc: 0.6960 - val_f1_macro: 0.4288 - val_loss: 4.7093\n\nEpoch 21: Learning Rate is 3.70e-06\nEpoch 21/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 193ms/step - accuracy: 0.5630 - auc: 0.7494 - f1_macro: 0.4249 - loss: 4.1035 - val_accuracy: 0.4970 - val_auc: 0.6982 - val_f1_macro: 0.4353 - val_loss: 4.6736\n\nEpoch 22: Learning Rate is 3.56e-06\nEpoch 22/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 195ms/step - accuracy: 0.5737 - auc: 0.7671 - f1_macro: 0.4244 - loss: 3.8124 - val_accuracy: 0.5004 - val_auc: 0.7012 - val_f1_macro: 0.4423 - val_loss: 4.6366\n\nEpoch 23: Learning Rate is 3.42e-06\nEpoch 23/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 185ms/step - accuracy: 0.6127 - auc: 0.7831 - f1_macro: 0.4455 - loss: 3.6393 - val_accuracy: 0.4983 - val_auc: 0.6982 - val_f1_macro: 0.4452 - val_loss: 4.6100\n\nEpoch 24: Learning Rate is 3.27e-06\nEpoch 24/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 184ms/step - accuracy: 0.6224 - auc: 0.7918 - f1_macro: 0.4409 - loss: 3.5044 - val_accuracy: 0.5011 - val_auc: 0.7038 - val_f1_macro: 0.4504 - val_loss: 4.5765\n\nEpoch 25: Learning Rate is 3.12e-06\nEpoch 25/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 198ms/step - accuracy: 0.6117 - auc: 0.7942 - f1_macro: 0.4294 - loss: 3.4663 - val_accuracy: 0.4978 - val_auc: 0.7055 - val_f1_macro: 0.4524 - val_loss: 4.5514\n\nEpoch 26: Learning Rate is 2.97e-06\nEpoch 26/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 193ms/step - accuracy: 0.6249 - auc: 0.8020 - f1_macro: 0.4374 - loss: 3.4148 - val_accuracy: 0.4998 - val_auc: 0.7045 - val_f1_macro: 0.4544 - val_loss: 4.5462\n\nEpoch 27: Learning Rate is 2.81e-06\nEpoch 27/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 202ms/step - accuracy: 0.6337 - auc: 0.8075 - f1_macro: 0.4439 - loss: 3.3022 - val_accuracy: 0.4989 - val_auc: 0.7014 - val_f1_macro: 0.4583 - val_loss: 4.5361\n\nEpoch 28: Learning Rate is 2.66e-06\nEpoch 28/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 192ms/step - accuracy: 0.6387 - auc: 0.8148 - f1_macro: 0.4307 - loss: 3.2749 - val_accuracy: 0.4953 - val_auc: 0.7030 - val_f1_macro: 0.4584 - val_loss: 4.5142\n\nEpoch 29: Learning Rate is 2.50e-06\nEpoch 29/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 189ms/step - accuracy: 0.6443 - auc: 0.8212 - f1_macro: 0.4483 - loss: 3.2144 - val_accuracy: 0.4937 - val_auc: 0.7053 - val_f1_macro: 0.4584 - val_loss: 4.4945\n\nEpoch 30: Learning Rate is 2.34e-06\nEpoch 30/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 201ms/step - accuracy: 0.6642 - auc: 0.8326 - f1_macro: 0.4558 - loss: 2.9815 - val_accuracy: 0.4952 - val_auc: 0.7080 - val_f1_macro: 0.4572 - val_loss: 4.4854\n\nEpoch 31: Learning Rate is 2.19e-06\nEpoch 31/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 183ms/step - accuracy: 0.6632 - auc: 0.8228 - f1_macro: 0.4550 - loss: 3.2147 - val_accuracy: 0.4918 - val_auc: 0.7058 - val_f1_macro: 0.4601 - val_loss: 4.4774\n\nEpoch 32: Learning Rate is 2.03e-06\nEpoch 32/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 187ms/step - accuracy: 0.6608 - auc: 0.8275 - f1_macro: 0.4516 - loss: 3.0131 - val_accuracy: 0.4909 - val_auc: 0.7058 - val_f1_macro: 0.4634 - val_loss: 4.4738\n\nEpoch 33: Learning Rate is 1.88e-06\nEpoch 33/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 186ms/step - accuracy: 0.6967 - auc: 0.8511 - f1_macro: 0.4672 - loss: 2.7353 - val_accuracy: 0.4881 - val_auc: 0.7081 - val_f1_macro: 0.4609 - val_loss: 4.4608\n\nEpoch 34: Learning Rate is 1.73e-06\nEpoch 34/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 207ms/step - accuracy: 0.6864 - auc: 0.8492 - f1_macro: 0.4496 - loss: 2.8268 - val_accuracy: 0.4881 - val_auc: 0.7066 - val_f1_macro: 0.4630 - val_loss: 4.4629\n\nEpoch 35: Learning Rate is 1.58e-06\nEpoch 35/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.7000 - auc: 0.8541 - f1_macro: 0.4703 - loss: 2.6525 - val_accuracy: 0.4838 - val_auc: 0.7046 - val_f1_macro: 0.4631 - val_loss: 4.4631\n\nEpoch 36: Learning Rate is 1.44e-06\nEpoch 36/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.6623 - auc: 0.8298 - f1_macro: 0.4625 - loss: 3.0476 - val_accuracy: 0.4870 - val_auc: 0.7055 - val_f1_macro: 0.4643 - val_loss: 4.4531\n\nEpoch 37: Learning Rate is 1.30e-06\nEpoch 37/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 171ms/step - accuracy: 0.6747 - auc: 0.8402 - f1_macro: 0.4539 - loss: 2.9963 - val_accuracy: 0.4862 - val_auc: 0.7074 - val_f1_macro: 0.4660 - val_loss: 4.4408\n\nEpoch 38: Learning Rate is 1.16e-06\nEpoch 38/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 215ms/step - accuracy: 0.6967 - auc: 0.8556 - f1_macro: 0.4715 - loss: 2.7711 - val_accuracy: 0.4781 - val_auc: 0.7024 - val_f1_macro: 0.4601 - val_loss: 4.4584\n\nEpoch 39: Learning Rate is 1.03e-06\nEpoch 39/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.6733 - auc: 0.8313 - f1_macro: 0.4747 - loss: 2.9248 - val_accuracy: 0.4788 - val_auc: 0.7007 - val_f1_macro: 0.4615 - val_loss: 4.4595\n\nEpoch 40: Learning Rate is 9.06e-07\nEpoch 40/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.6834 - auc: 0.8310 - f1_macro: 0.4832 - loss: 2.9880 - val_accuracy: 0.4823 - val_auc: 0.7056 - val_f1_macro: 0.4641 - val_loss: 4.4355\n\nEpoch 41: Learning Rate is 7.89e-07\nEpoch 41/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 170ms/step - accuracy: 0.6757 - auc: 0.8408 - f1_macro: 0.4648 - loss: 2.8587 - val_accuracy: 0.4825 - val_auc: 0.7065 - val_f1_macro: 0.4646 - val_loss: 4.4294\n\nEpoch 42: Learning Rate is 6.78e-07\nEpoch 42/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.6600 - auc: 0.8265 - f1_macro: 0.4640 - loss: 3.1368 - val_accuracy: 0.4833 - val_auc: 0.7073 - val_f1_macro: 0.4649 - val_loss: 4.4211\n\nEpoch 43: Learning Rate is 5.74e-07\nEpoch 43/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.6597 - auc: 0.8289 - f1_macro: 0.4743 - loss: 2.9727 - val_accuracy: 0.4862 - val_auc: 0.7069 - val_f1_macro: 0.4672 - val_loss: 4.4178\n\nEpoch 44: Learning Rate is 4.77e-07\nEpoch 44/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 178ms/step - accuracy: 0.6588 - auc: 0.8269 - f1_macro: 0.4609 - loss: 2.9863 - val_accuracy: 0.4875 - val_auc: 0.7098 - val_f1_macro: 0.4657 - val_loss: 4.4064\n\nEpoch 45: Learning Rate is 3.89e-07\nEpoch 45/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.6350 - auc: 0.8215 - f1_macro: 0.4398 - loss: 3.1761 - val_accuracy: 0.4846 - val_auc: 0.7089 - val_f1_macro: 0.4652 - val_loss: 4.4045\n\nEpoch 46: Learning Rate is 3.09e-07\nEpoch 46/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 170ms/step - accuracy: 0.6472 - auc: 0.8250 - f1_macro: 0.4576 - loss: 2.9741 - val_accuracy: 0.4870 - val_auc: 0.7104 - val_f1_macro: 0.4655 - val_loss: 4.3987\n\nEpoch 47: Learning Rate is 2.38e-07\nEpoch 47/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 187ms/step - accuracy: 0.6112 - auc: 0.8047 - f1_macro: 0.4390 - loss: 3.1774 - val_accuracy: 0.4933 - val_auc: 0.7118 - val_f1_macro: 0.4684 - val_loss: 4.3840\n\nEpoch 48: Learning Rate is 1.76e-07\nEpoch 48/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.5990 - auc: 0.8060 - f1_macro: 0.4342 - loss: 3.1597 - val_accuracy: 0.4905 - val_auc: 0.7109 - val_f1_macro: 0.4673 - val_loss: 4.3854\n\nEpoch 49: Learning Rate is 1.22e-07\nEpoch 49/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.6012 - auc: 0.7934 - f1_macro: 0.4400 - loss: 3.3530 - val_accuracy: 0.4900 - val_auc: 0.7095 - val_f1_macro: 0.4663 - val_loss: 4.3917\n\nEpoch 50: Learning Rate is 7.85e-08\nEpoch 50/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.6134 - auc: 0.7973 - f1_macro: 0.4485 - loss: 3.2801 - val_accuracy: 0.4903 - val_auc: 0.7098 - val_f1_macro: 0.4658 - val_loss: 4.3909\n\nEpoch 51: Learning Rate is 4.43e-08\nEpoch 51/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 180ms/step - accuracy: 0.6039 - auc: 0.8017 - f1_macro: 0.4343 - loss: 3.1656 - val_accuracy: 0.4929 - val_auc: 0.7101 - val_f1_macro: 0.4664 - val_loss: 4.3880\n\nEpoch 52: Learning Rate is 1.97e-08\nEpoch 52/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5883 - auc: 0.8039 - f1_macro: 0.4248 - loss: 3.2006 - val_accuracy: 0.4931 - val_auc: 0.7114 - val_f1_macro: 0.4646 - val_loss: 4.3860\n\nEpoch 53: Learning Rate is 4.93e-09\nEpoch 53/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.5945 - auc: 0.7974 - f1_macro: 0.4359 - loss: 3.1983 - val_accuracy: 0.4948 - val_auc: 0.7123 - val_f1_macro: 0.4678 - val_loss: 4.3771\n\nEpoch 54: Learning Rate is 4.50e-06\nEpoch 54/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 178ms/step - accuracy: 0.6225 - auc: 0.8132 - f1_macro: 0.4373 - loss: 3.0901 - val_accuracy: 0.4864 - val_auc: 0.7121 - val_f1_macro: 0.4680 - val_loss: 4.3812\n\nEpoch 55: Learning Rate is 4.50e-06\nEpoch 55/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 185ms/step - accuracy: 0.5340 - auc: 0.7608 - f1_macro: 0.3895 - loss: 3.6002 - val_accuracy: 0.4816 - val_auc: 0.7063 - val_f1_macro: 0.4689 - val_loss: 4.3891\n\nEpoch 56: Learning Rate is 4.50e-06\nEpoch 56/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.5559 - auc: 0.7757 - f1_macro: 0.3976 - loss: 3.3799 - val_accuracy: 0.4844 - val_auc: 0.7097 - val_f1_macro: 0.4713 - val_loss: 4.3402\n\nEpoch 57: Learning Rate is 4.49e-06\nEpoch 57/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.5665 - auc: 0.7806 - f1_macro: 0.4113 - loss: 3.3145 - val_accuracy: 0.4857 - val_auc: 0.7103 - val_f1_macro: 0.4713 - val_loss: 4.3323\n\nEpoch 58: Learning Rate is 4.48e-06\nEpoch 58/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 183ms/step - accuracy: 0.5672 - auc: 0.7745 - f1_macro: 0.4202 - loss: 3.4640 - val_accuracy: 0.4836 - val_auc: 0.7092 - val_f1_macro: 0.4727 - val_loss: 4.2951\n\nEpoch 59: Learning Rate is 4.47e-06\nEpoch 59/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 172ms/step - accuracy: 0.5227 - auc: 0.7620 - f1_macro: 0.3844 - loss: 3.4751 - val_accuracy: 0.4920 - val_auc: 0.7163 - val_f1_macro: 0.4802 - val_loss: 4.2345\n\nEpoch 60: Learning Rate is 4.46e-06\nEpoch 60/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.5393 - auc: 0.7678 - f1_macro: 0.4039 - loss: 3.3650 - val_accuracy: 0.4862 - val_auc: 0.7187 - val_f1_macro: 0.4743 - val_loss: 4.2330\n\nEpoch 61: Learning Rate is 4.45e-06\nEpoch 61/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 178ms/step - accuracy: 0.5352 - auc: 0.7712 - f1_macro: 0.3894 - loss: 3.3373 - val_accuracy: 0.4913 - val_auc: 0.7175 - val_f1_macro: 0.4775 - val_loss: 4.2053\n\nEpoch 62: Learning Rate is 4.43e-06\nEpoch 62/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.5242 - auc: 0.7585 - f1_macro: 0.4041 - loss: 3.5745 - val_accuracy: 0.4948 - val_auc: 0.7213 - val_f1_macro: 0.4794 - val_loss: 4.1582\n\nEpoch 63: Learning Rate is 4.41e-06\nEpoch 63/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.5226 - auc: 0.7704 - f1_macro: 0.3929 - loss: 3.4330 - val_accuracy: 0.4968 - val_auc: 0.7227 - val_f1_macro: 0.4815 - val_loss: 4.1477\n\nEpoch 64: Learning Rate is 4.39e-06\nEpoch 64/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.5204 - auc: 0.7650 - f1_macro: 0.3955 - loss: 3.3859 - val_accuracy: 0.4976 - val_auc: 0.7200 - val_f1_macro: 0.4802 - val_loss: 4.1523\n\nEpoch 65: Learning Rate is 4.37e-06\nEpoch 65/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 182ms/step - accuracy: 0.5206 - auc: 0.7641 - f1_macro: 0.3862 - loss: 3.5153 - val_accuracy: 0.5011 - val_auc: 0.7209 - val_f1_macro: 0.4785 - val_loss: 4.1440\n\nEpoch 66: Learning Rate is 4.34e-06\nEpoch 66/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.5201 - auc: 0.7706 - f1_macro: 0.4023 - loss: 3.4275 - val_accuracy: 0.5069 - val_auc: 0.7234 - val_f1_macro: 0.4811 - val_loss: 4.1187\n\nEpoch 67: Learning Rate is 4.31e-06\nEpoch 67/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 169ms/step - accuracy: 0.5346 - auc: 0.7755 - f1_macro: 0.4082 - loss: 3.3256 - val_accuracy: 0.5065 - val_auc: 0.7194 - val_f1_macro: 0.4841 - val_loss: 4.1279\n\nEpoch 68: Learning Rate is 4.29e-06\nEpoch 68/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.5020 - auc: 0.7564 - f1_macro: 0.3856 - loss: 3.4894 - val_accuracy: 0.5039 - val_auc: 0.7252 - val_f1_macro: 0.4799 - val_loss: 4.0942\n\nEpoch 69: Learning Rate is 4.25e-06\nEpoch 69/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.4934 - auc: 0.7488 - f1_macro: 0.3806 - loss: 3.5727 - val_accuracy: 0.5108 - val_auc: 0.7245 - val_f1_macro: 0.4846 - val_loss: 4.0774\n\nEpoch 70: Learning Rate is 4.22e-06\nEpoch 70/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.4609 - auc: 0.7387 - f1_macro: 0.3651 - loss: 3.7716 - val_accuracy: 0.5050 - val_auc: 0.7212 - val_f1_macro: 0.4795 - val_loss: 4.0927\n\nEpoch 71: Learning Rate is 4.19e-06\nEpoch 71/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.4802 - auc: 0.7491 - f1_macro: 0.3803 - loss: 3.6004 - val_accuracy: 0.5123 - val_auc: 0.7248 - val_f1_macro: 0.4825 - val_loss: 4.0629\n\nEpoch 72: Learning Rate is 4.15e-06\nEpoch 72/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 180ms/step - accuracy: 0.5000 - auc: 0.7640 - f1_macro: 0.3918 - loss: 3.5596 - val_accuracy: 0.5138 - val_auc: 0.7251 - val_f1_macro: 0.4791 - val_loss: 4.0512\n\nEpoch 73: Learning Rate is 4.11e-06\nEpoch 73/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4706 - auc: 0.7465 - f1_macro: 0.3714 - loss: 3.7991 - val_accuracy: 0.5134 - val_auc: 0.7215 - val_f1_macro: 0.4787 - val_loss: 4.0642\n\nEpoch 74: Learning Rate is 4.07e-06\nEpoch 74/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.4675 - auc: 0.7411 - f1_macro: 0.3832 - loss: 3.7425 - val_accuracy: 0.5210 - val_auc: 0.7263 - val_f1_macro: 0.4798 - val_loss: 4.0352\n\nEpoch 75: Learning Rate is 4.03e-06\nEpoch 75/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.4766 - auc: 0.7439 - f1_macro: 0.3875 - loss: 3.7473 - val_accuracy: 0.5244 - val_auc: 0.7230 - val_f1_macro: 0.4810 - val_loss: 4.0576\n\nEpoch 76: Learning Rate is 3.98e-06\nEpoch 76/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 158ms/step - accuracy: 0.4878 - auc: 0.7495 - f1_macro: 0.4014 - loss: 3.8066 - val_accuracy: 0.5272 - val_auc: 0.7294 - val_f1_macro: 0.4817 - val_loss: 4.0141\n\nEpoch 77: Learning Rate is 3.94e-06\nEpoch 77/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 169ms/step - accuracy: 0.4513 - auc: 0.7291 - f1_macro: 0.3812 - loss: 3.9912 - val_accuracy: 0.5272 - val_auc: 0.7229 - val_f1_macro: 0.4799 - val_loss: 4.0548\n\nEpoch 78: Learning Rate is 3.89e-06\nEpoch 78/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 158ms/step - accuracy: 0.4711 - auc: 0.7457 - f1_macro: 0.3830 - loss: 3.8133 - val_accuracy: 0.5266 - val_auc: 0.7298 - val_f1_macro: 0.4749 - val_loss: 4.0238\n\nEpoch 79: Learning Rate is 3.84e-06\nEpoch 79/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 176ms/step - accuracy: 0.4692 - auc: 0.7539 - f1_macro: 0.3865 - loss: 3.8101 - val_accuracy: 0.5285 - val_auc: 0.7258 - val_f1_macro: 0.4735 - val_loss: 4.0501\n\nEpoch 80: Learning Rate is 3.79e-06\nEpoch 80/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4602 - auc: 0.7394 - f1_macro: 0.3764 - loss: 4.0381 - val_accuracy: 0.5255 - val_auc: 0.7224 - val_f1_macro: 0.4722 - val_loss: 4.0628\n\nEpoch 81: Learning Rate is 3.74e-06\nEpoch 81/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 171ms/step - accuracy: 0.4787 - auc: 0.7443 - f1_macro: 0.3953 - loss: 3.9960 - val_accuracy: 0.5275 - val_auc: 0.7251 - val_f1_macro: 0.4720 - val_loss: 4.0445\n\nEpoch 82: Learning Rate is 3.68e-06\nEpoch 82/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 182ms/step - accuracy: 0.4606 - auc: 0.7312 - f1_macro: 0.3836 - loss: 4.1691 - val_accuracy: 0.5342 - val_auc: 0.7227 - val_f1_macro: 0.4746 - val_loss: 4.0589\n\nEpoch 83: Learning Rate is 3.63e-06\nEpoch 83/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.4831 - auc: 0.7420 - f1_macro: 0.3998 - loss: 4.1378 - val_accuracy: 0.5359 - val_auc: 0.7259 - val_f1_macro: 0.4679 - val_loss: 4.0433\n\nEpoch 84: Learning Rate is 3.57e-06\nEpoch 84/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.4744 - auc: 0.7518 - f1_macro: 0.3998 - loss: 4.2752 - val_accuracy: 0.5352 - val_auc: 0.7290 - val_f1_macro: 0.4692 - val_loss: 4.0134\n\nEpoch 85: Learning Rate is 3.51e-06\nEpoch 85/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.4695 - auc: 0.7375 - f1_macro: 0.3970 - loss: 4.3865 - val_accuracy: 0.5406 - val_auc: 0.7258 - val_f1_macro: 0.4756 - val_loss: 4.0294\n\nEpoch 86: Learning Rate is 3.46e-06\nEpoch 86/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 180ms/step - accuracy: 0.4617 - auc: 0.7416 - f1_macro: 0.3992 - loss: 4.3863 - val_accuracy: 0.5392 - val_auc: 0.7303 - val_f1_macro: 0.4625 - val_loss: 4.0170\n\nEpoch 87: Learning Rate is 3.40e-06\nEpoch 87/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4681 - auc: 0.7409 - f1_macro: 0.4028 - loss: 4.4186 - val_accuracy: 0.5391 - val_auc: 0.7276 - val_f1_macro: 0.4630 - val_loss: 4.0264\n\nEpoch 88: Learning Rate is 3.33e-06\nEpoch 88/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.4688 - auc: 0.7419 - f1_macro: 0.4098 - loss: 4.3919 - val_accuracy: 0.5432 - val_auc: 0.7311 - val_f1_macro: 0.4660 - val_loss: 4.0188\n\nEpoch 89: Learning Rate is 3.27e-06\nEpoch 89/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.4592 - auc: 0.7368 - f1_macro: 0.3882 - loss: 4.5531 - val_accuracy: 0.5398 - val_auc: 0.7389 - val_f1_macro: 0.4560 - val_loss: 3.9854\n\nEpoch 90: Learning Rate is 3.21e-06\nEpoch 90/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.4685 - auc: 0.7396 - f1_macro: 0.4064 - loss: 4.5805 - val_accuracy: 0.5376 - val_auc: 0.7370 - val_f1_macro: 0.4534 - val_loss: 4.0002\n\nEpoch 91: Learning Rate is 3.14e-06\nEpoch 91/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 155ms/step - accuracy: 0.4500 - auc: 0.7350 - f1_macro: 0.3912 - loss: 4.5868 - val_accuracy: 0.5430 - val_auc: 0.7383 - val_f1_macro: 0.4571 - val_loss: 3.9860\n\nEpoch 92: Learning Rate is 3.08e-06\nEpoch 92/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 149ms/step - accuracy: 0.4596 - auc: 0.7456 - f1_macro: 0.3974 - loss: 4.5156 - val_accuracy: 0.5424 - val_auc: 0.7322 - val_f1_macro: 0.4574 - val_loss: 4.0071\n\nEpoch 93: Learning Rate is 3.01e-06\nEpoch 93/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 187ms/step - accuracy: 0.4452 - auc: 0.7268 - f1_macro: 0.3825 - loss: 4.6566 - val_accuracy: 0.5422 - val_auc: 0.7320 - val_f1_macro: 0.4581 - val_loss: 4.0070\n\nEpoch 94: Learning Rate is 2.95e-06\nEpoch 94/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 175ms/step - accuracy: 0.4361 - auc: 0.7330 - f1_macro: 0.3791 - loss: 4.6728 - val_accuracy: 0.5422 - val_auc: 0.7440 - val_f1_macro: 0.4506 - val_loss: 3.9733\n\nEpoch 95: Learning Rate is 2.88e-06\nEpoch 95/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.4695 - auc: 0.7355 - f1_macro: 0.4032 - loss: 4.5772 - val_accuracy: 0.5411 - val_auc: 0.7397 - val_f1_macro: 0.4481 - val_loss: 3.9854\n\nEpoch 96: Learning Rate is 2.81e-06\nEpoch 96/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 182ms/step - accuracy: 0.4691 - auc: 0.7386 - f1_macro: 0.4023 - loss: 4.5887 - val_accuracy: 0.5419 - val_auc: 0.7430 - val_f1_macro: 0.4465 - val_loss: 3.9777\n\nEpoch 97: Learning Rate is 2.74e-06\nEpoch 97/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.4655 - auc: 0.7308 - f1_macro: 0.3987 - loss: 4.4088 - val_accuracy: 0.5435 - val_auc: 0.7417 - val_f1_macro: 0.4511 - val_loss: 3.9746\n\nEpoch 98: Learning Rate is 2.67e-06\nEpoch 98/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 169ms/step - accuracy: 0.4670 - auc: 0.7399 - f1_macro: 0.3960 - loss: 4.4660 - val_accuracy: 0.5415 - val_auc: 0.7471 - val_f1_macro: 0.4427 - val_loss: 3.9578\n\nEpoch 99: Learning Rate is 2.60e-06\nEpoch 99/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 174ms/step - accuracy: 0.4613 - auc: 0.7392 - f1_macro: 0.3944 - loss: 4.3877 - val_accuracy: 0.5439 - val_auc: 0.7526 - val_f1_macro: 0.4446 - val_loss: 3.9423\n\nEpoch 100: Learning Rate is 2.53e-06\nEpoch 100/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 182ms/step - accuracy: 0.4475 - auc: 0.7370 - f1_macro: 0.3779 - loss: 4.4184 - val_accuracy: 0.5420 - val_auc: 0.7497 - val_f1_macro: 0.4417 - val_loss: 3.9578\n\nEpoch 101: Learning Rate is 2.46e-06\nEpoch 101/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 172ms/step - accuracy: 0.4154 - auc: 0.7195 - f1_macro: 0.3582 - loss: 4.5622 - val_accuracy: 0.5407 - val_auc: 0.7539 - val_f1_macro: 0.4354 - val_loss: 3.9497\n\nEpoch 102: Learning Rate is 2.39e-06\nEpoch 102/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 167ms/step - accuracy: 0.4515 - auc: 0.7352 - f1_macro: 0.3779 - loss: 4.3199 - val_accuracy: 0.5415 - val_auc: 0.7525 - val_f1_macro: 0.4349 - val_loss: 3.9536\n\nEpoch 103: Learning Rate is 2.32e-06\nEpoch 103/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 184ms/step - accuracy: 0.4502 - auc: 0.7305 - f1_macro: 0.3817 - loss: 4.2372 - val_accuracy: 0.5454 - val_auc: 0.7534 - val_f1_macro: 0.4415 - val_loss: 3.9463\n\nEpoch 104: Learning Rate is 2.25e-06\nEpoch 104/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.4702 - auc: 0.7341 - f1_macro: 0.3880 - loss: 4.1774 - val_accuracy: 0.5443 - val_auc: 0.7520 - val_f1_macro: 0.4430 - val_loss: 3.9481\n\nEpoch 105: Learning Rate is 2.18e-06\nEpoch 105/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.4711 - auc: 0.7413 - f1_macro: 0.3852 - loss: 4.1845 - val_accuracy: 0.5432 - val_auc: 0.7559 - val_f1_macro: 0.4362 - val_loss: 3.9402\n\nEpoch 106: Learning Rate is 2.11e-06\nEpoch 106/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 174ms/step - accuracy: 0.4669 - auc: 0.7404 - f1_macro: 0.3788 - loss: 4.1219 - val_accuracy: 0.5424 - val_auc: 0.7551 - val_f1_macro: 0.4344 - val_loss: 3.9413\n\nEpoch 107: Learning Rate is 2.04e-06\nEpoch 107/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 177ms/step - accuracy: 0.4627 - auc: 0.7412 - f1_macro: 0.3690 - loss: 4.0673 - val_accuracy: 0.5417 - val_auc: 0.7558 - val_f1_macro: 0.4329 - val_loss: 3.9480\n\nEpoch 108: Learning Rate is 1.97e-06\nEpoch 108/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 176ms/step - accuracy: 0.4422 - auc: 0.7296 - f1_macro: 0.3599 - loss: 4.1488 - val_accuracy: 0.5432 - val_auc: 0.7561 - val_f1_macro: 0.4315 - val_loss: 3.9554\n\nEpoch 109: Learning Rate is 1.90e-06\nEpoch 109/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.4350 - auc: 0.7238 - f1_macro: 0.3529 - loss: 4.1697 - val_accuracy: 0.5435 - val_auc: 0.7573 - val_f1_macro: 0.4320 - val_loss: 3.9588\n\nEpoch 110: Learning Rate is 1.83e-06\nEpoch 110/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 183ms/step - accuracy: 0.4958 - auc: 0.7514 - f1_macro: 0.3900 - loss: 3.8937 - val_accuracy: 0.5435 - val_auc: 0.7607 - val_f1_macro: 0.4255 - val_loss: 3.9632\n\nEpoch 111: Learning Rate is 1.76e-06\nEpoch 111/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 169ms/step - accuracy: 0.4941 - auc: 0.7483 - f1_macro: 0.3811 - loss: 3.8510 - val_accuracy: 0.5443 - val_auc: 0.7617 - val_f1_macro: 0.4250 - val_loss: 3.9685\n\nEpoch 112: Learning Rate is 1.69e-06\nEpoch 112/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 159ms/step - accuracy: 0.4767 - auc: 0.7536 - f1_macro: 0.3706 - loss: 3.7815 - val_accuracy: 0.5428 - val_auc: 0.7609 - val_f1_macro: 0.4233 - val_loss: 3.9631\n\nEpoch 113: Learning Rate is 1.62e-06\nEpoch 113/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 167ms/step - accuracy: 0.4428 - auc: 0.7354 - f1_macro: 0.3478 - loss: 4.0176 - val_accuracy: 0.5446 - val_auc: 0.7599 - val_f1_macro: 0.4265 - val_loss: 3.9609\n\nEpoch 114: Learning Rate is 1.55e-06\nEpoch 114/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 180ms/step - accuracy: 0.4431 - auc: 0.7289 - f1_macro: 0.3494 - loss: 3.9811 - val_accuracy: 0.5443 - val_auc: 0.7615 - val_f1_macro: 0.4211 - val_loss: 3.9667\n\nEpoch 115: Learning Rate is 1.49e-06\nEpoch 115/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.4848 - auc: 0.7450 - f1_macro: 0.3625 - loss: 3.9157 - val_accuracy: 0.5450 - val_auc: 0.7594 - val_f1_macro: 0.4245 - val_loss: 3.9694\n\nEpoch 116: Learning Rate is 1.42e-06\nEpoch 116/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 167ms/step - accuracy: 0.4329 - auc: 0.7216 - f1_macro: 0.3327 - loss: 4.0770 - val_accuracy: 0.5454 - val_auc: 0.7642 - val_f1_macro: 0.4197 - val_loss: 3.9706\n\nEpoch 117: Learning Rate is 1.36e-06\nEpoch 117/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 179ms/step - accuracy: 0.4522 - auc: 0.7323 - f1_macro: 0.3321 - loss: 3.9684 - val_accuracy: 0.5459 - val_auc: 0.7637 - val_f1_macro: 0.4228 - val_loss: 3.9759\n\nEpoch 118: Learning Rate is 1.29e-06\nEpoch 118/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 176ms/step - accuracy: 0.5112 - auc: 0.7577 - f1_macro: 0.3580 - loss: 3.6986 - val_accuracy: 0.5432 - val_auc: 0.7603 - val_f1_macro: 0.4192 - val_loss: 3.9889\n\nEpoch 119: Learning Rate is 1.23e-06\nEpoch 119/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.4912 - auc: 0.7413 - f1_macro: 0.3518 - loss: 3.8263 - val_accuracy: 0.5441 - val_auc: 0.7599 - val_f1_macro: 0.4212 - val_loss: 3.9826\n\nEpoch 120: Learning Rate is 1.17e-06\nEpoch 120/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 161ms/step - accuracy: 0.4699 - auc: 0.7345 - f1_macro: 0.3414 - loss: 3.8525 - val_accuracy: 0.5448 - val_auc: 0.7637 - val_f1_macro: 0.4185 - val_loss: 3.9752\n\nEpoch 121: Learning Rate is 1.10e-06\nEpoch 121/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 181ms/step - accuracy: 0.4466 - auc: 0.7275 - f1_macro: 0.3274 - loss: 3.9064 - val_accuracy: 0.5472 - val_auc: 0.7666 - val_f1_macro: 0.4216 - val_loss: 3.9678\n\nEpoch 122: Learning Rate is 1.04e-06\nEpoch 122/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 174ms/step - accuracy: 0.4436 - auc: 0.7236 - f1_macro: 0.3200 - loss: 3.9323 - val_accuracy: 0.5463 - val_auc: 0.7656 - val_f1_macro: 0.4171 - val_loss: 3.9724\n\nEpoch 123: Learning Rate is 9.85e-07\nEpoch 123/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 171ms/step - accuracy: 0.4877 - auc: 0.7542 - f1_macro: 0.4960 - loss: 3.6295 - val_accuracy: 0.5458 - val_auc: 0.7655 - val_f1_macro: 0.4185 - val_loss: 3.9623\n\nEpoch 124: Learning Rate is 9.27e-07\nEpoch 124/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 184ms/step - accuracy: 0.5375 - auc: 0.7779 - f1_macro: 0.5150 - loss: 3.4531 - val_accuracy: 0.5446 - val_auc: 0.7654 - val_f1_macro: 0.4199 - val_loss: 3.9545\n\nEpoch 125: Learning Rate is 8.71e-07\nEpoch 125/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.5384 - auc: 0.7753 - f1_macro: 0.5084 - loss: 3.4299 - val_accuracy: 0.5454 - val_auc: 0.7650 - val_f1_macro: 0.4225 - val_loss: 3.9409\n\nEpoch 126: Learning Rate is 8.16e-07\nEpoch 126/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 167ms/step - accuracy: 0.5225 - auc: 0.7826 - f1_macro: 0.5016 - loss: 3.4066 - val_accuracy: 0.5437 - val_auc: 0.7645 - val_f1_macro: 0.4203 - val_loss: 3.9483\n\nEpoch 127: Learning Rate is 7.62e-07\nEpoch 127/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.5482 - auc: 0.7923 - f1_macro: 0.5090 - loss: 3.2610 - val_accuracy: 0.5456 - val_auc: 0.7636 - val_f1_macro: 0.4270 - val_loss: 3.9374\n\nEpoch 128: Learning Rate is 7.10e-07\nEpoch 128/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 185ms/step - accuracy: 0.5806 - auc: 0.8069 - f1_macro: 0.5030 - loss: 3.1079 - val_accuracy: 0.5433 - val_auc: 0.7650 - val_f1_macro: 0.4242 - val_loss: 3.9203\n\nEpoch 129: Learning Rate is 6.59e-07\nEpoch 129/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 166ms/step - accuracy: 0.5661 - auc: 0.8041 - f1_macro: 0.5115 - loss: 3.0944 - val_accuracy: 0.5417 - val_auc: 0.7606 - val_f1_macro: 0.4264 - val_loss: 3.9270\n\nEpoch 130: Learning Rate is 6.10e-07\nEpoch 130/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 170ms/step - accuracy: 0.5785 - auc: 0.8095 - f1_macro: 0.5077 - loss: 3.1095 - val_accuracy: 0.5420 - val_auc: 0.7593 - val_f1_macro: 0.4310 - val_loss: 3.9214\n\nEpoch 131: Learning Rate is 5.62e-07\nEpoch 131/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 188ms/step - accuracy: 0.6056 - auc: 0.8213 - f1_macro: 0.5233 - loss: 2.9137 - val_accuracy: 0.5428 - val_auc: 0.7598 - val_f1_macro: 0.4330 - val_loss: 3.9004\n\nEpoch 132: Learning Rate is 5.16e-07\nEpoch 132/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.6372 - auc: 0.8401 - f1_macro: 0.5323 - loss: 2.7241 - val_accuracy: 0.5406 - val_auc: 0.7597 - val_f1_macro: 0.4310 - val_loss: 3.9061\n\nEpoch 133: Learning Rate is 4.72e-07\nEpoch 133/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.6423 - auc: 0.8439 - f1_macro: 0.5335 - loss: 2.7038 - val_accuracy: 0.5383 - val_auc: 0.7563 - val_f1_macro: 0.4326 - val_loss: 3.9159\n\nEpoch 134: Learning Rate is 4.30e-07\nEpoch 134/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.6472 - auc: 0.8448 - f1_macro: 0.5311 - loss: 2.6079 - val_accuracy: 0.5376 - val_auc: 0.7553 - val_f1_macro: 0.4324 - val_loss: 3.9165\n\nEpoch 135: Learning Rate is 3.89e-07\nEpoch 135/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 178ms/step - accuracy: 0.6521 - auc: 0.8483 - f1_macro: 0.5315 - loss: 2.6445 - val_accuracy: 0.5365 - val_auc: 0.7540 - val_f1_macro: 0.4320 - val_loss: 3.9249\n\nEpoch 136: Learning Rate is 3.50e-07\nEpoch 136/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.6610 - auc: 0.8566 - f1_macro: 0.5273 - loss: 2.5428 - val_accuracy: 0.5363 - val_auc: 0.7531 - val_f1_macro: 0.4334 - val_loss: 3.9173\n\nEpoch 137: Learning Rate is 3.13e-07\nEpoch 137/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 167ms/step - accuracy: 0.6488 - auc: 0.8576 - f1_macro: 0.5286 - loss: 2.4946 - val_accuracy: 0.5335 - val_auc: 0.7517 - val_f1_macro: 0.4321 - val_loss: 3.9270\n\nEpoch 138: Learning Rate is 2.78e-07\nEpoch 138/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 178ms/step - accuracy: 0.6680 - auc: 0.8550 - f1_macro: 0.5237 - loss: 2.4925 - val_accuracy: 0.5327 - val_auc: 0.7509 - val_f1_macro: 0.4344 - val_loss: 3.9185\n\nEpoch 139: Learning Rate is 2.45e-07\nEpoch 139/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 167ms/step - accuracy: 0.6767 - auc: 0.8661 - f1_macro: 0.5220 - loss: 2.3928 - val_accuracy: 0.5333 - val_auc: 0.7475 - val_f1_macro: 0.4372 - val_loss: 3.9224\n\nEpoch 140: Learning Rate is 2.14e-07\nEpoch 140/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - accuracy: 0.6892 - auc: 0.8704 - f1_macro: 0.5247 - loss: 2.3395 - val_accuracy: 0.5331 - val_auc: 0.7476 - val_f1_macro: 0.4388 - val_loss: 3.9260\n\nEpoch 141: Learning Rate is 1.85e-07\nEpoch 141/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 184ms/step - accuracy: 0.7004 - auc: 0.8749 - f1_macro: 0.5506 - loss: 2.2591 - val_accuracy: 0.5314 - val_auc: 0.7470 - val_f1_macro: 0.4387 - val_loss: 3.9233\n\nEpoch 142: Learning Rate is 1.58e-07\nEpoch 142/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 167ms/step - accuracy: 0.6992 - auc: 0.8763 - f1_macro: 0.5316 - loss: 2.3022 - val_accuracy: 0.5283 - val_auc: 0.7409 - val_f1_macro: 0.4430 - val_loss: 3.9423\n\nEpoch 143: Learning Rate is 1.33e-07\nEpoch 143/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 171ms/step - accuracy: 0.6985 - auc: 0.8800 - f1_macro: 0.5275 - loss: 2.1965 - val_accuracy: 0.5283 - val_auc: 0.7406 - val_f1_macro: 0.4427 - val_loss: 3.9483\n\nEpoch 144: Learning Rate is 1.10e-07\nEpoch 144/200\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 168ms/step - accuracy: 0.7172 - auc: 0.8871 - f1_macro: 0.5373 - loss: 2.2212 - val_accuracy: 0.5260 - val_auc: 0.7368 - val_f1_macro: 0.4461 - val_loss: 3.9644\nEpoch 144: early stopping\nRestoring model weights from the end of the best epoch: 69.\nĐã lưu mô hình cho Fold 5 tại: /kaggle/working/output_results/EfficienetB0_CV_TPU_fold_5.keras\nĐang tạo và lưu biểu đồ huấn luyện...\nĐã lưu biểu đồ cho Fold 5 tại: /kaggle/working/output_results/fold_5_metrics.png\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756912907.510011      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:17842064229865454063\nI0000 00:00:1756912908.801210     913 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(10503501604871670389), session_name()\nI0000 00:00:1756912921.816997     913 tpu_compile_op_common.cc:245] Compilation of 10503501604871670389 with session name  took 13.015713702s and succeeded\nI0000 00:00:1756912921.836606     913 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(10503501604871670389), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_17842064229865454063\", property.function_library_fingerprint = 14351898955023293915, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"32,224,224,3,;32,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756912921.836661     913 tpu_compilation_cache_interface.cc:542] After adding entry for key 10503501604871670389 with session_name  cache is 38 entries (2377017669 bytes),  marked for eviction 0 entries (0 bytes).\nI0000 00:00:1756912923.973982     904 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(16810127803233789873), session_name()\nI0000 00:00:1756912936.934350     904 tpu_compile_op_common.cc:245] Compilation of 16810127803233789873 with session name  took 12.960328141s and succeeded\nI0000 00:00:1756912936.952723     904 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(16810127803233789873), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_17842064229865454063\", property.function_library_fingerprint = 14351898955023293915, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"28,224,224,3,;28,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1756912936.952767     904 tpu_compilation_cache_interface.cc:542] After adding entry for key 16810127803233789873 with session_name  cache is 39 entries (2404548808 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"Fold 5 - Validation Loss: 3.9951, Validation Accuracy: 0.5266, Validation AUC: 0.7365, Validation F1-Macro: 0.4803\n==================================================\nKết quả Cross-Validation:\nValidation Accuracy trung bình: 0.5269 +/- 0.0076\nValidation Loss trung bình: 4.1705 +/- 0.1343\nValidation AUC trung bình: 0.7454 +/- 0.0099\nValidation F1-Macro trung bình: 0.4726 +/- 0.0149\n==================================================\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nfrom tqdm.notebook import tqdm\n\nprint(\"Bắt đầu quy trình Knowledge Distillation...\")\n\n# --- BƯỚC 1: TẠO KIẾN THỨC TỪ ENSEMBLE 5 MÔ HÌNH THẦY GIÁO ---\n\nprint(\"Bước 1: Tạo các nhãn mềm từ 5 mô hình thầy giáo...\")\n\n# 1.1. Tạo lại dataset huấn luyện (không shuffle, không repeat) để lấy dự đoán\ntrain_val_ds_for_preds = tf.data.TFRecordDataset(TRAIN_VAL_TFREC)\ntrain_val_ds_for_preds = train_val_ds_for_preds.map(parse_tfrecord_fn, num_parallel_calls=AUTOTUNE)\ntrain_val_ds_for_preds = train_val_ds_for_preds.batch(GLOBAL_BATCH_SIZE)\ntrain_val_ds_for_preds = train_val_ds_for_preds.prefetch(buffer_size=AUTOTUNE)\n\n# 1.2. Lặp qua 5 folds để lấy và cộng dồn logits\nall_fold_logits = []\nfor fold_number in range(1, N_SPLITS + 1):\n    print(f\"  - Đang lấy dự đoán từ Fold {fold_number}...\")\n    model_path = os.path.join(KAGGLE_OUTPUT_PATH, f'{MODEL_ID}_fold_{fold_number}.keras')\n    with strategy.scope():\n        teacher_model = tf.keras.models.load_model(model_path)\n    \n    fold_logits = teacher_model.predict(train_val_ds_for_preds, verbose=0)\n    all_fold_logits.append(fold_logits)\n\n# 1.3. Tính trung bình logits và tạo nhãn mềm (xác suất)\nmean_logits = np.mean(np.stack(all_fold_logits, axis=0), axis=0)\nsoft_labels = tf.nn.softmax(mean_logits).numpy()\n\n# 1.4. Lấy nhãn thật (hard labels)\nhard_labels_onehot = np.concatenate([label.numpy() for _, label in train_val_ds_for_preds], axis=0)\n\n# 1.5. Lấy dữ liệu ảnh (X_train)\nX_train_spectrograms = np.concatenate([img.numpy() for img, _ in train_val_ds_for_preds], axis=0)\n\nprint(f\"-> Đã tạo xong {len(soft_labels)} nhãn mềm.\")\n\n\n# --- BƯỚC 2: XÂY DỰNG MÔ HÌNH HỌC SINH ---\n\ndef create_student_model(input_shape, num_classes):\n    inputs = keras.Input(shape=input_shape)\n    # Sử dụng mixed precision cho mô hình học sinh để tăng tốc\n    x = layers.Rescaling(1./255)(inputs)\n    x = layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    x = layers.Conv2D(64, (3, 3), padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    x = layers.MaxPooling2D(3, strides=2)(x)\n\n    x = layers.Conv2D(128, (3, 3), padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    x = layers.MaxPooling2D(3, strides=2)(x)\n    \n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.4)(x)\n    outputs = layers.Dense(num_classes, dtype='float32')(x) # Output logits\n    return keras.Model(inputs, outputs)\n\nprint(\"Bước 2: Đã định nghĩa kiến trúc mô hình học sinh.\")\n\n\n# --- BƯỚC 3: TẠO LỚP DISTILLER ---\n\nclass Distiller(keras.Model):\n    def __init__(self, student):\n        super().__init__()\n        self.student = student\n\n    def compile(self, optimizer, metrics, student_loss_fn, distillation_loss_fn, alpha=0.1, temperature=3):\n        super().compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n\n    def train_step(self, data):\n        # Unpack data\n        x, y = data\n        teacher_predictions = y[\"soft\"]\n        true_labels = y[\"hard\"]\n\n        with tf.GradientTape() as tape:\n            # Lấy dự đoán từ mô hình học sinh (logits)\n            student_predictions = self.student(x, training=True)\n\n            # Tính student loss với nhãn thật\n            student_loss = self.student_loss_fn(true_labels, student_predictions)\n\n            # Tính distillation loss với nhãn mềm của thầy\n            # Dùng temperature để làm \"mềm\" hơn nữa distribition\n            distillation_loss = self.distillation_loss_fn(\n                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n            )\n\n            # Kết hợp 2 loss lại\n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\n        # Cập nhật trọng số\n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # Cập nhật metrics\n        self.compiled_metrics.update_state(true_labels, student_predictions)\n        \n        # Trả về kết quả\n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss, \"distillation_loss\": distillation_loss})\n        return results\n\nprint(\"Bước 3: Đã định nghĩa lớp Distiller.\")\n\n\n# --- BƯỚC 4: HUẤN LUYỆN MÔ HÌNH HỌC SINH ---\nprint(\"Bước 4: Bắt đầu huấn luyện mô hình học sinh...\")\n\n# Tạo dataset cuối cùng cho việc huấn luyện\n# Dữ liệu X là spectrogram, dữ liệu y là một dictionary chứa cả 2 loại nhãn\ndistillation_ds = tf.data.Dataset.from_tensor_slices(\n    (X_train_spectrograms, {\"soft\": soft_labels, \"hard\": hard_labels_onehot})\n).batch(GLOBAL_BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n\n\nwith strategy.scope():\n    # Khởi tạo mô hình học sinh\n    student = create_student_model(input_shape=INPUT_SHAPE, num_classes=len(ALL_CLASSES))\n    \n    # Khởi tạo Distiller\n    distiller = Distiller(student=student)\n    \n    # Compile Distiller\n    distiller.compile(\n        optimizer=keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-4),\n        metrics=[keras.metrics.CategoricalAccuracy(name=\"accuracy\")],\n        student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n        distillation_loss_fn=keras.losses.KLDivergence(),\n        alpha=0.2, # Trọng số cho hard loss, 80% học từ thầy\n        temperature=5, # Làm mềm distribition\n    )\n\n# Huấn luyện\ndistiller.fit(distillation_ds, epochs=50) # Tăng epochs nếu cần\n\n# Lưu lại mô hình học sinh sau khi huấn luyện\nstudent_model_path = os.path.join(KAGGLE_OUTPUT_PATH, \"student_model.keras\")\ndistiller.student.save(student_model_path)\nprint(f\"Đã huấn luyện và lưu mô hình học sinh tại: {student_model_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ĐÁNH GIÁ SO SÁNH 5-FOLD, ENSEMBLE VÀ MÔ HÌNH HỌC SINH\n\nprint(\"Bắt đầu quy trình đánh giá toàn diện...\")\n\n# --- BƯỚC 1: CHUẨN BỊ DỮ LIỆU TEST (CHỈ LÀM MỘT LẦN) ---\nTEST_TFREC = os.path.join(TFRECORD_OUTPUT_PATH, 'test.tfrec')\nprint(f\"Sử dụng dữ liệu test từ: {TEST_TFREC}\")\n\ntest_ds = tf.data.TFRecordDataset(TEST_TFREC)\ntest_ds = test_ds.map(parse_tfrecord_fn, num_parallel_calls=AUTOTUNE)\ntest_ds = test_ds.batch(GLOBAL_BATCH_SIZE)\ntest_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n\ny_test_onehot = np.concatenate([label.numpy() for _, label in test_ds], axis=0)\ny_test_encoded = np.argmax(y_test_onehot, axis=1)\ntarget_names = le.classes_\nn_classes = len(target_names)\nrun_timestamp = datetime.datetime.now(pytz.timezone('Asia/Ho_Chi_Minh')).strftime(\"%Y%m%d_%H%M\")\nreport_figs_path = os.path.join(KAGGLE_OUTPUT_PATH, \"final_report_figures\")\nos.makedirs(report_figs_path, exist_ok=True)\n\n# ==============================================================================\n# PHẦN 1: ĐÁNH GIÁ ENSEMBLE TỪ 5 FOLDS\n# ==============================================================================\nprint(\"\\n\" + \"=\"*30 + \"\\nPHẦN 1: ĐÁNH GIÁ ENSEMBLE\\n\" + \"=\"*30)\n\nall_fold_preds_logits = []\nfor fold_number in range(1, N_SPLITS + 1):\n    print(f\"  - Lấy dự đoán từ Fold {fold_number}...\")\n    model_path = os.path.join(KAGGLE_OUTPUT_PATH, f'{MODEL_ID}_fold_{fold_number}.keras')\n    with strategy.scope():\n        teacher_model = tf.keras.models.load_model(model_path)\n    all_fold_preds_logits.append(teacher_model.predict(test_ds, verbose=0))\n\nmean_logits = np.mean(np.stack(all_fold_preds_logits, axis=0), axis=0)\ny_pred_probs_ensemble = tf.nn.softmax(mean_logits).numpy()\ny_pred_encoded_ensemble = np.argmax(y_pred_probs_ensemble, axis=1)\n\nprint(\"\\nBáo cáo chi tiết của mô hình Ensemble:\")\nprint(classification_report(y_test_encoded, y_pred_encoded_ensemble, target_names=target_names))\n\n# Vẽ Ma trận nhầm lẫn cho Ensemble\ncm_ensemble = confusion_matrix(y_test_encoded, y_pred_encoded_ensemble)\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm_ensemble, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\nplt.title(f'Ma trận nhầm lẫn (Ensemble trên {N_SPLITS} Folds)', fontsize=16)\nplt.ylabel('Nhãn thật'); plt.xlabel('Nhãn dự đoán')\ncm_ensemble_path = os.path.join(report_figs_path, f'ensemble_confusion_matrix_{run_timestamp}.png')\nplt.savefig(cm_ensemble_path)\nplt.show(); plt.close()\n\n# (Các biểu đồ so sánh 5-fold và ROC chi tiết có thể giữ lại hoặc bỏ đi tuỳ bạn)\n\n\n# ==============================================================================\n# PHẦN 2: ĐÁNH GIÁ MÔ HÌNH HỌC SINH\n# ==============================================================================\nprint(\"\\n\" + \"=\"*30 + \"\\nPHẦN 2: ĐÁNH GIÁ MÔ HÌNH HỌC SINH\\n\" + \"=\"*30)\n\nstudent_model_path = os.path.join(KAGGLE_OUTPUT_PATH, \"student_model.keras\")\n\nif not os.path.exists(student_model_path):\n    print(f\"Không tìm thấy mô hình học sinh tại '{student_model_path}'. Bỏ qua bước đánh giá này.\")\nelse:\n    print(\"Đang tải mô hình học sinh...\")\n    with strategy.scope():\n        student_model = tf.keras.models.load_model(student_model_path)\n        # Biên dịch lại model để có thể tính toán đầy đủ các metrics\n        student_model.compile(\n            loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n            metrics=['accuracy', tf.keras.metrics.AUC(name='auc'), MacroF1Score(num_classes=n_classes)]\n        )\n\n    print(\"Đang đánh giá mô hình học sinh trên tập test...\")\n    student_loss, student_acc, student_auc, student_f1 = student_model.evaluate(test_ds, verbose=1)\n    \n    print(\"Đang dự đoán với mô hình học sinh...\")\n    y_pred_probs_student = tf.nn.softmax(student_model.predict(test_ds, verbose=0)).numpy()\n    y_pred_encoded_student = np.argmax(y_pred_probs_student, axis=1)\n\n    # --- Lưu kết quả ra file text ---\n    student_report_text = classification_report(y_test_encoded, y_pred_encoded_student, target_names=target_names)\n    results_filepath = os.path.join(KAGGLE_OUTPUT_PATH, \"student_model_evaluation_results.txt\")\n    with open(results_filepath, \"w\") as f:\n        f.write(\"--- KET QUA DANH GIA MO HINH HOC SINH ---\\n\\n\")\n        f.write(f\"Timestamp: {run_timestamp}\\n\\n\")\n        f.write(f\"Test Loss: {student_loss:.4f}\\n\")\n        f.write(f\"Test Accuracy: {student_acc:.4f}\\n\")\n        f.write(f\"Test AUC: {student_auc:.4f}\\n\")\n        f.write(f\"Test F1-Macro: {student_f1:.4f}\\n\")\n        f.write(\"\\n--- BAO CAO PHAN LOAI CHI TIET ---\\n\")\n        f.write(student_report_text)\n    print(f\"\\nĐã lưu kết quả đánh giá của mô hình học sinh vào file: {results_filepath}\")\n    print(student_report_text)\n\n    # --- Vẽ biểu đồ riêng cho mô hình học sinh ---\n    # 1. Ma trận nhầm lẫn (Học sinh)\n    cm_student = confusion_matrix(y_test_encoded, y_pred_encoded_student)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm_student, annot=True, fmt='d', cmap='Greens', xticklabels=target_names, yticklabels=target_names)\n    plt.title('Ma trận nhầm lẫn (Mô hình Học sinh)', fontsize=16)\n    plt.ylabel('Nhãn thật'); plt.xlabel('Nhãn dự đoán')\n    cm_student_path = os.path.join(report_figs_path, f'student_confusion_matrix_{run_timestamp}.png')\n    plt.savefig(cm_student_path)\n    plt.show(); plt.close()\n\n    # 2. Đường cong ROC (Học sinh)\n    y_test_binarized = label_binarize(y_test_encoded, classes=range(n_classes))\n    plt.figure(figsize=(12, 10))\n    for i in range(n_classes):\n        fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred_probs_student[:, i])\n        auc_score = sklearn_auc(fpr, tpr)\n        plt.plot(fpr, tpr, lw=2, label=f'Lớp: {target_names[i]} (AUC = {auc_score:.3f})')\n    \n    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Chance')\n    plt.title('Đường cong ROC (Mô hình Học sinh)', fontsize=16)\n    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n    plt.legend(loc='lower right'); plt.grid(True)\n    roc_student_path = os.path.join(report_figs_path, f'student_roc_curves_{run_timestamp}.png')\n    plt.savefig(roc_student_path)\n    plt.show(); plt.close()\n\n    print(f\"\\nĐã hoàn tất đánh giá mô hình học sinh! Các biểu đồ được lưu tại: {report_figs_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T15:23:26.673122Z","iopub.execute_input":"2025-09-03T15:23:26.673496Z","iopub.status.idle":"2025-09-03T15:23:27.851109Z","shell.execute_reply.started":"2025-09-03T15:23:26.673468Z","shell.execute_reply":"2025-09-03T15:23:27.844117Z"}},"outputs":[{"name":"stdout","text":"\nBắt đầu đánh giá so sánh 5 mô hình từ Cross-Validation trên tập Test...\nSử dụng dữ liệu test từ: /kaggle/working/tfrecords/test.tfrec\n--------------------------------------------------\nĐang xử lý Fold 1/5\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(KAGGLE_OUTPUT_PATH, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m---> 30\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfocal_loss_fixed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfocal_loss_from_logits_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_weights_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGAMMA\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMacroF1Score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mMacroF1Score\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m loss, accuracy, auc, f1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_ds, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     36\u001b[0m all_fold_metrics\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m: auc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m: f1})\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/saving/saving_api.py:189\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    186\u001b[0m         is_keras_zip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_keras_zip \u001b[38;5;129;01mor\u001b[39;00m is_keras_dir \u001b[38;5;129;01mor\u001b[39;00m is_hf:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[1;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[1;32m    198\u001b[0m     )\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:370\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filename: expected a `.keras` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m     )\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_model_from_fileobj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:447\u001b[0m, in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zf\u001b[38;5;241m.\u001b[39mopen(_CONFIG_FILENAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    445\u001b[0m     config_json \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 447\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_model_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[1;32m    452\u001b[0m extract_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:436\u001b[0m, in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[0;32m--> 436\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:694\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_retrieve_class_or_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregistered_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfull_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mcls\u001b[39m, types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:810\u001b[0m, in \u001b[0;36m_retrieve_class_or_fn\u001b[0;34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[0m\n\u001b[1;32m    803\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m    804\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    805\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not deserialize \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m because \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    806\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mits parent module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be imported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    807\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    808\u001b[0m             )\n\u001b[0;32m--> 810\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure custom classes are decorated with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`@keras.saving.register_keras_serializable()`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m )\n","\u001b[0;31mTypeError\u001b[0m: Could not locate class 'FinalModel'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'FinalModel', 'config': {'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'mixed_bfloat16'}, 'registered_name': None}}, 'registered_name': 'FinalModel', 'build_config': {'input_shape': [32, 224, 224, 3]}, 'compile_config': {'optimizer': {'module': 'keras.optimizers', 'class_name': 'AdamW', 'config': {'name': 'adamw', 'learning_rate': {'module': 'keras.optimizers.schedules', 'class_name': 'CosineDecayRestarts', 'config': {'initial_learning_rate': 5e-06, 'first_decay_steps': 4350, 't_mul': 2.0, 'm_mul': 0.9, 'alpha': 0.0, 'name': 'SGDRDecay'}, 'registered_name': None}, 'weight_decay': 0.003, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': {'module': 'builtins', 'class_name': 'function', 'config': 'focal_loss_fixed', 'registered_name': 'function'}, 'loss_weights': None, 'metrics': ['accuracy', {'module': 'keras.metrics', 'class_name': 'AUC', 'config': {'name': 'auc', 'dtype': 'float32', 'num_thresholds': 200, 'curve': 'ROC', 'summation_method': 'interpolation', 'multi_label': False, 'num_labels': None, 'label_weights': None, 'from_logits': False}, 'registered_name': None}, {'module': None, 'class_name': 'MacroF1Score', 'config': {'name': 'f1_macro', 'dtype': 'float32'}, 'registered_name': 'MacroF1Score'}], 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': False}}"],"ename":"TypeError","evalue":"Could not locate class 'FinalModel'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'FinalModel', 'config': {'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'mixed_bfloat16'}, 'registered_name': None}}, 'registered_name': 'FinalModel', 'build_config': {'input_shape': [32, 224, 224, 3]}, 'compile_config': {'optimizer': {'module': 'keras.optimizers', 'class_name': 'AdamW', 'config': {'name': 'adamw', 'learning_rate': {'module': 'keras.optimizers.schedules', 'class_name': 'CosineDecayRestarts', 'config': {'initial_learning_rate': 5e-06, 'first_decay_steps': 4350, 't_mul': 2.0, 'm_mul': 0.9, 'alpha': 0.0, 'name': 'SGDRDecay'}, 'registered_name': None}, 'weight_decay': 0.003, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': {'module': 'builtins', 'class_name': 'function', 'config': 'focal_loss_fixed', 'registered_name': 'function'}, 'loss_weights': None, 'metrics': ['accuracy', {'module': 'keras.metrics', 'class_name': 'AUC', 'config': {'name': 'auc', 'dtype': 'float32', 'num_thresholds': 200, 'curve': 'ROC', 'summation_method': 'interpolation', 'multi_label': False, 'num_labels': None, 'label_weights': None, 'from_logits': False}, 'registered_name': None}, {'module': None, 'class_name': 'MacroF1Score', 'config': {'name': 'f1_macro', 'dtype': 'float32'}, 'registered_name': 'MacroF1Score'}], 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': False}}","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"# VẼ GRAD-CAM CHO MÔ HÌNH ENSEMBLE (PHIÊN BẢN SỬA LỖI HOÀN CHỈNH)\n\n# --- BƯỚC 0: TẢI 5 MODELS VÀ LẤY DỰ ĐOÁN ENSEMBLE ---\n# (Giả định rằng các biến từ ô đánh giá trước đó vẫn tồn tại, ví dụ: test_ds, y_test_encoded)\n\nall_fold_preds_logits = []\nfor fold_number in range(1, N_SPLITS + 1):\n    print(f\"Đang tải dự đoán từ Fold {fold_number}...\")\n    model_path = os.path.join(KAGGLE_OUTPUT_PATH, f'{MODEL_ID}_fold_{fold_number}.keras')\n    with strategy.scope():\n        model = tf.keras.models.load_model(model_path) # Không cần custom_objects vì đã đăng ký\n    preds_logits = model.predict(test_ds, verbose=0)\n    all_fold_preds_logits.append(preds_logits)\n\n# Lấy dự đoán cuối cùng bằng cách trung bình hóa logits\nmean_logits = np.mean(np.stack(all_fold_preds_logits, axis=0), axis=0)\ny_pred_probs_ensemble = tf.nn.softmax(mean_logits).numpy()\ny_pred_encoded_ensemble = np.argmax(y_pred_probs_ensemble, axis=1)\n\nprint(\"Đã có dự đoán từ mô hình ensemble.\")\n\n\n# --- BƯỚC 1: LẤY DỮ LIỆU TEST TỪ PIPELINE `test_ds` (NẾU CẦN CHẠY LẠI) ---\nprint(\"Đang trích xuất dữ liệu ảnh từ test_ds để vẽ Grad-CAM...\")\nX_test_list = [images.numpy() for images, _ in test_ds]\nX_test = np.concatenate(X_test_list, axis=0)\nprint(f\"Đã trích xuất thành công {len(X_test)} mẫu ảnh.\")\n\n\n# --- BƯỚC 2: CHỈ ĐỊNH LỚP CONVOLUTION CUỐI CÙNG ---\n# Gán trực tiếp tên lớp, không cần tìm kiếm tự động\nlast_conv_layer_name = \"top_conv\" \nprint(f\"Sử dụng lớp Grad-CAM được chỉ định: {last_conv_layer_name}\")\n\n\n# --- BƯỚC 3: TẠO HÌNH ẢNH GRAD-CAM CHO CÁC MẪU TIÊU BIỂU (DÙNG KẾT QUẢ ENSEMBLE) ---\n# Tải lại một mô hình bất kỳ để làm sườn cho việc tính Grad-CAM\n# Vì kiến trúc giống nhau, ta có thể dùng bất kỳ model nào từ 5 folds\nprint(\"Tải một mô hình mẫu để tính Grad-CAM...\")\nwith strategy.scope():\n    sample_model_for_gradcam = tf.keras.models.load_model(os.path.join(KAGGLE_OUTPUT_PATH, f'{MODEL_ID}_fold_1.keras'))\n\nrun_timestamp = datetime.datetime.now(pytz.timezone('Asia/Ho_Chi_Minh')).strftime(\"%Y%m%d_%H%M\")\ngradcam_path = os.path.join(KAGGLE_OUTPUT_PATH, \"grad_cam_figures\")\nos.makedirs(gradcam_path, exist_ok=True)\nprint(\"Tạo hình ảnh Grad-CAM cho các mẫu tiêu biểu...\")\n\nresults_list = []\nfor i in range(len(y_test_encoded)):\n    is_correct = (y_test_encoded[i] == y_pred_encoded_ensemble[i])\n    results_list.append({\n        'index': i, \n        'true_label': y_test_encoded[i], \n        'pred_label': y_pred_encoded_ensemble[i], \n        'confidence': y_pred_probs_ensemble[i][y_pred_encoded_ensemble[i]], \n        'is_correct': is_correct\n    })\nresults_df = pd.DataFrame(results_list)\n\n# Lấy tên các lớp\ntarget_names = le.classes_\ntrained_class_indices = range(len(target_names))\n\nfor class_index, class_name in enumerate(target_names):\n    correct_samples = results_df[(results_df['is_correct'] == True) & (results_df['true_label'] == class_index)].nlargest(3, 'confidence')\n    incorrect_samples = results_df[(results_df['is_correct'] == False) & (results_df['true_label'] == class_index)].nlargest(3, 'confidence')\n    \n    for _, row in correct_samples.iterrows():\n        idx = int(row['index'])\n        img_array, spec = X_test[idx][np.newaxis, ...], X_test[idx, :, :, 0]\n        heatmap = get_grad_cam_final(sample_model_for_gradcam, img_array, last_conv_layer_name, pred_index=class_index)\n        overlay = overlay_grad_cam(spec, heatmap)\n        plt.imshow(overlay)\n        plt.title(f\"Dự đoán Đúng: {class_name}\\n(Tin cậy: {row['confidence']:.2f})\")\n        plt.axis('off')\n        plt.savefig(os.path.join(gradcam_path, f\"correct_{class_name}_{idx}_{run_timestamp}.png\"))\n        plt.close()\n        \n    for _, row in incorrect_samples.iterrows():\n        idx = int(row['index'])\n        pred_class_name = target_names[int(row['pred_label'])]\n        img_array, spec = X_test[idx][np.newaxis, ...], X_test[idx, :, :, 0]\n        # Tính heatmap theo nhãn thật để xem tại sao mô hình nhầm\n        heatmap = get_grad_cam_final(sample_model_for_gradcam, img_array, last_conv_layer_name, pred_index=int(row['true_label']))\n        overlay = overlay_grad_cam(spec, heatmap)\n        plt.imshow(overlay)\n        plt.title(f\"Thật: {class_name}, Sai -> {pred_class_name}\\n(Tin cậy: {row['confidence']:.2f})\")\n        plt.axis('off')\n        plt.savefig(os.path.join(gradcam_path, f\"incorrect_{class_name}_as_{pred_class_name}_{idx}_{run_timestamp}.png\"))\n        plt.close()\n\nprint(\"Hoàn tất việc tạo Grad-CAM.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T15:22:28.864492Z","iopub.status.idle":"2025-09-03T15:22:28.865091Z","shell.execute_reply.started":"2025-09-03T15:22:28.864712Z","shell.execute_reply":"2025-09-03T15:22:28.864728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TẠO BÁO CÁO PDF TỔNG HỢP 5-FOLD VÀ ENSEMBLE\n\nprint(\"Bắt đầu tạo báo cáo PDF tổng hợp...\")\n\n# --- BƯỚC 1: TRÍCH XUẤT CÁC KẾT QUẢ CẦN THIẾT ---\n\n# Lấy kết quả trung bình từ validation set (từ ô huấn luyện)\nval_acc_mean = np.mean(fold_accuracies)\nval_acc_std = np.std(fold_accuracies)\nval_loss_mean = np.mean(fold_losses)\nval_loss_std = np.std(fold_losses)\nval_f1_mean = np.mean(fold_f1s)\nval_f1_std = np.std(fold_f1s)\n\n# Lấy kết quả trung bình từ test set (từ ô đánh giá)\ntest_metrics_mean = metrics_df.mean()\n\n# Lấy báo cáo chi tiết của mô hình ensemble\nensemble_report_text = classification_report(y_test_encoded, y_pred_encoded_ensemble, target_names=target_names)\nensemble_report_dict = classification_report(y_test_encoded, y_pred_encoded_ensemble, target_names=target_names, output_dict=True)\nensemble_accuracy = ensemble_report_dict['accuracy']\nensemble_f1_macro = ensemble_report_dict['macro avg']['f1-score']\n\n\n# --- BƯỚC 2: TẠO FILE PDF ---\npdf = PDFReport()\npdf.add_page()\n\n# --- Phần 1: Tóm tắt cấu hình và kết quả ---\npdf.chapter_title(\"1. Tom tat cau hinh va Ket qua\")\n# Lấy timestamp từ ô đánh giá để thống nhất\nrun_timestamp = datetime.datetime.now(pytz.timezone('Asia/Ho_Chi_Minh')).strftime(\"%Y-%m-%d %H:%M:%S\")\n\nconfig_summary = f\"\"\"\n- Model ID: {MODEL_ID}\n- Thoi gian chay: {run_timestamp}\n- Cac lop huan luyen: {', '.join(CLASSES_TO_TRAIN)}\n- K-Fold Cross-Validation: {N_SPLITS} folds\n\n--- KET QUA CROSS-VALIDATION (TRUNG BINH TREN TAP VALIDATION) ---\n- Validation Accuracy: {val_acc_mean:.4f} +/- {val_acc_std:.4f}\n- Validation F1-Macro: {val_f1_mean:.4f} +/- {val_f1_std:.4f}\n- Validation Loss: {val_loss_mean:.4f} +/- {val_loss_std:.4f}\n\n--- KET QUA ENSEMBLE (TREN TAP TEST CUOI CUNG) ---\n- Ensemble Test Accuracy: {ensemble_accuracy:.4f}\n- Ensemble Test F1-Macro: {ensemble_f1_macro:.4f}\n\n--- CAU HINH CHI TIET ---\n- SEED: {SEED}\n- Tong so Epochs toi da: {TOTAL_EPOCHS} (Patience: {PATIENCE_EPOCHS})\n- Batch Size: {BATCH_SIZE} (Global: {GLOBAL_BATCH_SIZE})\n- Learning Rate ban dau: {LEARNING_RATE}\n- Ham Loss: {'Focal Loss' if USE_FOCAL_LOSS else 'Categorical Crossentropy'}\n- Tang cuong du lieu: {'Co' if USE_DATA_AUGMENTATION else 'Khong'}\n- Kich thuoc Input: {INPUT_SHAPE}\n\"\"\"\npdf.chapter_body(config_summary)\n\n\n# --- Phần 2: Đánh giá chi tiết trên tập Test ---\npdf.add_page()\npdf.chapter_title(\"2. Danh gia chi tiet tren tap Test\")\npdf.chapter_body(\"Bao cao phan loai chi tiet cua mo hinh Ensemble:\")\npdf.set_font('Courier', '', 8)\npdf.chapter_body(ensemble_report_text)\npdf.set_font('Arial', '', 10) # Reset font\n\n# Thêm các biểu đồ so sánh đã tạo\npdf.add_image_section(\"So sanh Ma tran nham lan cua 5 Folds:\", cm_plot_path)\npdf.add_image_section(\"So sanh Duong cong ROC cua 5 Folds (theo tung lop):\", roc_plot_path)\n\n\n# --- Phần 3: Phân tích Grad-CAM ---\npdf.add_page()\npdf.chapter_title(\"3. Phan tich Grad-CAM (dua tren du doan Ensemble)\")\n\n# Lấy timestamp của lần chạy Grad-CAM gần nhất để tìm đúng file ảnh\ngradcam_run_timestamp = os.path.basename(cm_plot_path).replace('comparison_confusion_matrix_', '').replace('.png', '')\n\nfor class_name in target_names:\n    pdf.chapter_body(f\"Lop: {class_name}\")\n    \n    # Tìm các ảnh Grad-CAM tương ứng\n    correct_imgs = sorted(glob.glob(os.path.join(gradcam_path, f\"correct_{class_name}_*_{gradcam_run_timestamp}.png\")))\n    incorrect_imgs = sorted(glob.glob(os.path.join(gradcam_path, f\"incorrect_{class_name}_as_*_{gradcam_run_timestamp}.png\")))\n    \n    x_pos, y_pos = pdf.get_x(), pdf.get_y()\n    \n    if correct_imgs:\n        pdf.chapter_body(\"Vi du du doan dung:\")\n        y_pos = pdf.get_y()\n        for i, img_path in enumerate(correct_imgs[:3]):\n            if os.path.exists(img_path): pdf.image(img_path, x=x_pos + i * 60, y=y_pos, w=55)\n        pdf.ln(45) # Xuống dòng\n    \n    if incorrect_imgs:\n        pdf.chapter_body(\"Vi du du doan sai:\")\n        y_pos = pdf.get_y()\n        for i, img_path in enumerate(incorrect_imgs[:3]):\n            if os.path.exists(img_path): pdf.image(img_path, x=x_pos + i * 60, y=y_pos, w=55)\n        pdf.ln(45) # Xuống dòng\n\n# --- BƯỚC 3: LƯU FILE PDF VÀ TẢI LÊN DRIVE ---\nreport_filename = f\"report_5fold_{MODEL_ID}_{run_timestamp}.pdf\"\nreport_filepath = os.path.join(KAGGLE_OUTPUT_PATH, report_filename)\npdf.output(report_filepath)\nprint(f\"Da tao bao cao PDF tai: {report_filepath}\")\n\n# Phần tải lên Google Drive (giữ nguyên)\n# print(\"\\nBắt đầu quá trình tải kết quả lên Google Drive...\")\n# drive = authenticate_gdrive()\n# upload_folder_to_drive(drive, KAGGLE_OUTPUT_PATH, 'YOUR_DRIVE_FOLDER_ID') # Thay ID folder của bạn vào đây\n# print(\"Hoàn tất! Toàn bộ kết quả đã được lưu về Google Drive.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T15:22:28.866743Z","iopub.status.idle":"2025-09-03T15:22:28.867310Z","shell.execute_reply.started":"2025-09-03T15:22:28.866911Z","shell.execute_reply":"2025-09-03T15:22:28.866927Z"}},"outputs":[],"execution_count":null}]}